# Factor Models {#Ch14}

Factor analysis is the second family of statistical analyses within the structural equation modeling (SEM) framework. Factor analysis is used to describe the dependencies between a set of observed variables, by using a limited number of so-called underlying or common factors. These common factors are not directly observed (i.e., they are latent factors) and represent everything that the associated observed variables have in common. In this chapter we first introduce an empirical example that will be used to explain factor models in this and subsequent chapters, then we give a general explanation of the factor model—both conceptually and through symbols, and explain how to fit the factor model using `lavaan`.
 
## Empirical example of a factor model
The School Attitudes Questionnaire (SAQ) of Smits & Vorst (1982) is a Dutch questionnaire that measures three components of educational attitudes:  motivation for school tasks, satisfaction with school life, and self-confidence about ones scholastic capabilities. Each component is measured with three scales:

-__Motivation__ (for school tasks) is measured by: learning orientation, concentration on school work, and homework attitude;

-__Satisfaction__ (with school life) is measured by: fun at school, acceptance by classmates, and relationship with teacher;

-__Self-confidence__ (about ones scholastic capabilities) is measured by: self-expression, self-efficacy, and social skills.

## Conceptual explanation of a factor model
Figure \@ref(fig:fig14-1) is a graphical display of the factor model of the SAQ. The nine squares represent the nine observed scale scores of the questionnaire: learning orientation (LO), concentration on school work (CO), homework attitude (HM), fun at school (FU), acceptance by classmates (AC), relationship with teacher (TE), self-expression (SE), self-efficacy (SF), and social skills (SK). In the context of factor models the observed variables are often referred to as indicator variables. The circles at the top are the common factors Motivation, Satisfaction and Self-confidence. The common factors are represented by circles to reflect the fact that they are latent factors, i.e., they are not directly observed. The relationships between the latent factors and the observed variables are represented by one-sided arrows (→) and are called factor loadings. In our example, the common factors Motivation, Satisfaction, and Self-confidence are each measured by three observed indicators. For example, Motivation is measured by the observed indicators LO, CO, and HM, as represented by the direct effects of the common factor on these three variables. There are no arrows pointing from the common factor Motivation to the other observed indicator variables. The common factor Motivation therefore represent everything that the associated observed indicator variables LO, CO, and HM have in common. Likewise, the common factor Satisfaction represents everything that the variables FU, AC, and TE have in common. The model that is represented in Figure 1 defines the measurement structure of the model (i.e., the relationships between the common factors and indicator variables) and is therefore called a measurement model. The underlying common factors in the measurement model are allowed to correlate, which is reflected by the double headed arrows (↔) between the common factors. The factor model thus provides a description of the relationships between the observed indicator variables using a limited number of underlying common factors.

```{r label="fig14-1", echo = FALSE, out.width = "80%", fig.align = "center", fig.cap="Factor model of the School Attitudes Questionnaire" }

knitr::include_graphics("images/Ch14_factor_model.png")
```

The circles $ε_{LO}$ through $ε_{SK}$ at the bottom of Figure \@ref(fig:fig14-1) are so-called residual factors. Just as with path analysis, residual factors in factor analysis are unobserved, latent factors that represent all factors that fall outside the model but may influence the states of the corresponding observed indicator variables. In the context of factor analysis, these residual factors can be considered ‘container variables’ that represent everything that is specific to the associated observed indicator variable (as opposed to everything that the observed indicator variables have in common, as represented by the common factors). The residual factors also reflect measurement error. The variance of a residual factor therefore partly consists of variance due to random error fluctuations (i.e., measurement error; unsystematic variance) and partly of specific variable variance (i.e., due to unmeasured causes; systematic variance). The directional effects from the residual factors to the corresponding observed indicator variables are accompanied by the scaling constant ‘1’ for reasons of identification (which is explained in more detail in section 14.4).

## Symbolic explanation of a factor model
Figure 2 is again a graphical representation of the factor model of the SAQ, but now also includes the Greek symbols that are associated with the common factors (‘$ξ$’, or ‘ksi’) and the directional effects between the common factors and observed indicator variables (‘$λ$’, or ‘lambda’). Using these symbols, the relationships between the observed indicator variables and the underlying common factor as presented in Figure 2 can be given by the following equations:

\begin{align}
\mathrm{x}_1 = \lambda_{11}\xi_1+\varepsilon_1, (\#eq:14-1) \\
\mathrm{x}_2 = \lambda_{21}\xi_1+\varepsilon_2, (\#eq:14-2) \\
\mathrm{x}_3 = \lambda_{31}\xi_1+\varepsilon_3, (\#eq:14-3) \\
\mathrm{x}_4 = \lambda_{42}\xi_2+\varepsilon_4, (\#eq:14-4) \\
\mathrm{x}_5 = \lambda_{52}\xi_2+\varepsilon_5, (\#eq:14-5) \\
\mathrm{x}_6 = \lambda_{62}\xi_2+\varepsilon_6, (\#eq:14-6) \\
\mathrm{x}_7 = \lambda_{73}\xi_3+\varepsilon_7, (\#eq:14-7) \\
\mathrm{x}_8 = \lambda_{83}\xi_3+\varepsilon_8, (\#eq:14-8) \\
\mathrm{x}_9 = \lambda_{93}\xi_3+\varepsilon_9, (\#eq:14-9) 
\end{align}

where $\mathrm{x}_1$ through $\mathrm{x}_9$ are the nine observed indicator variables, $\xi_1$, $\xi_2$ and $\xi_3$ are the common factors Motivation, Satisfaction, and Self-Confidence respectively, and $\varepsilon_1$ through $\varepsilon_9$ are the residual factors of the associated $\mathrm{x}_1$ through $\mathrm{x}_9$. Parameters $\lambda_{11}$ through $\lambda_{93}$ are factor loadings. Using the equations, we can again see that each of the  observed variables is only affected by one of the common factors. Note also that each of the observed variables is affected by its associated residual factor. 

```{r label="fig14-2", echo = FALSE, out.width = "80%", fig.align = "center", fig.cap="Factor Model of the School Attitudes Quetsionnaire" }

knitr::include_graphics("images/Ch14_factor_model_lambdas.png")
```

Instead of writing down the equation for each observed variable, the equations for the observed variables x1 through x9 can also be written in matrix form:

```{=tex}
\begin{equation}
\mathbf{x} = \mathbf\Lambda \mathbf\xi + \mathbf{\varepsilon},
(\#eq:14-10)
\end{equation}
```

where $\mathbf{x}$ is a vector of all observed variables, $\mathbf\xi$ is a vector of all common factors, $\mathbf\varepsilon$ is a vector of all residual factors, and $\mathbf\Lambda$ is a matrix of factor loadings.
$$
\mathbf{x}=
\begin{bmatrix}
x_1\\x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\\x_9
\end{bmatrix},
\mathbf\xi =
\begin{bmatrix}
\xi_1\\\xi_2\\\xi_3
\end{bmatrix}
\mathbf\varepsilon =
\begin{bmatrix}
\varepsilon_1\\\varepsilon_2\\\varepsilon_3\\\varepsilon_4\\\varepsilon_5\\\varepsilon_6\\\varepsilon_7\\\varepsilon_8\\\varepsilon_9
\end{bmatrix}
\mathbf\Lambda =
\begin{bmatrix}
\lambda_{11}&0&0\\\lambda_{21}&0&0\\\lambda_{31}&0&0\\0&\lambda_{42}&0\\0&\lambda_{52}&0\\0&\lambda_{62}&0\\0&0&\lambda_{73}\\0&0&\lambda_{83}\\0&0&\lambda_{93}
\end{bmatrix}.
$$

Matrix $\mathbf\Lambda$ contains nine nonzero elements: $\lambda_{11}$ through $\lambda_{93}$. Like the regression slopes in the Beta matrix ($\mathbf{B}$), rows represent the outcomes (here, indicators) and columns represent the predictors (here, common factors).  So the factor loading $\lambda_{ij}$ represents the regression of variable $i$ on common factor $j$. When the factor model is specified such that variable $i$ does not load on common factor $j$, then that specific element $\lambda_{ij}$ is fixed to zero. Specifically, the pattern of matrix $\mathbf\Lambda$ specifies the measurement structure of the factor model. In our example, $\mathbf\Lambda$ contains many elements that are fixed to zero because each indicator variable loads only on one of three common factors. Such a pattern of factor loadings is sometimes referred to as ‘simple structure’.

Substituting these matrices into Equation \@ref(eq:14-10) gives:

```{=tex}
\begin{equation}

(\#eq:14-11)
\end{equation}
```

and evaluation gives:

```{=tex}
\begin{equation}
\begin{bmatrix}
x_1\\x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\\x_9
\end{bmatrix} =
\begin{bmatrix}
\lambda_{11}\xi_1+\varepsilon_1\\
\lambda_{21}\xi_1+\varepsilon_2\\
\lambda_{31}\xi_1+\varepsilon_3\\
\lambda_{42}\xi_2+\varepsilon_4\\
\lambda_{52}\xi_2+\varepsilon_5\\
\lambda_{62}\xi_2+\varepsilon_6\\
\lambda_{73}\xi_3+\varepsilon_7\\
\lambda_{83}\xi_3+\varepsilon_8\\
\lambda_{93}\xi_3+\varepsilon_9
\end{bmatrix}.
(\#eq:14-12)
\end{equation}
```

Here, we can see that the equations for variables $\mathrm{x}_1$ through $\mathrm{x}_9$ are the same as separate Equations \@ref(eq:14-01) through \@ref(eq:14-09). 

We now have an expression for the scores on the observed indicator variables. However, in standard SEM we do not model the observed scores directly, but rather the variances and covariances of the observed scores. Therefore, we need to find an expression for the model-implied covariance matrix. Using Equation \@ref(eq:14-10) and some covariance algebra, we obtain the expression for the variances and covariances of $\mathbf{x}$, called $\mathbf\Sigma_{\text{model}} = \text{COV}(\mathbf{x},\mathbf{x})$:

```{=tex}
\begin{equation}
\mathbf\Sigma_{\text{model}} = \mathbf\Lambda \mathbf\Phi \mathbf\Lambda^\text{T} + \mathbf\Theta,
(\#eq:14-13)
\end{equation}
```

where the variances and covariances of the common factors are represented by $\text{COV}(\mathbf\xi,\mathbf\xi) = \mathbf\Phi$, and the variances and covariances of the residual factors represented by $\text{COV}(\mathbf\varepsilon,\mathbf\varepsilon) = \mathbf\Theta$. The matrix $\mathbf\Lambda$ contains the common factor loadings, and $\mathbf\Lambda^\text{T}$ denotes the transpose of the matrix $\mathbf\Lambda$. Derivations of Equation \@ref(eq:14-13) are given in [Appendix 1](#appendix14) at the end of this Chapter. 

Matrix $\mathbf\Phi$ is a symmetric matrix and contains the variances and covariances of the common factors $\mathbfξ$. For the model given in Equation \@ref(eq:14-12), matrix $\mathbf\Phi$ is:

```{=tex}
\begin{equation}
\mathbf\Phi = 
\begin{bmatrix}
\phi_{11}\\\phi_{21}&\phi_{22}\\\phi_{31}&\phi_{32}&\phi_{33}\\
\end{bmatrix},
(\#eq:14-14)
\end{equation}
```

where the diagonal elements $φ_{11}$, $φ_{22}$, $φ_{33}$ represent the variances of common factors $ξ_1$, $ξ_2$, and $ξ_3$, respectively, and the off-diagonal elements represent the covariances between the common factors, e.g., the element $φ_{21}$ represents the covariance between common factors $ξ_1$ and $ξ_2$. 

Matrix $\mathbf\Theta$ is a symmetric matrix and contains the variances and covariances of the residual factors. It is often assumed that the residual factors do not covary with each other, so that matrix $\mathbf\Theta$ is a diagonal matrix with residual variances only. However, in some applications covariances between (some) residual factors are allowed, which are then specified by nonzero off-diagonal elements of matrix $\mathbf\Theta$. The $\mathbf\Theta$ matrix for our illustrative example from the Figure \@ref(fig:fig14-2) model is:

```{=tex}
\begin{equation}
\mathbf\Theta = 
\begin{bmatrix}
\theta_{11}\\
0&\theta_{22}\\
0&0&\theta_{33}\\
0&0&0&\theta_{44}\\
0&0&0&0&\theta_{55}\\
0&0&0&0&0&\theta_{66}\\
0&0&0&0&0&0&\theta_{77}\\
0&0&0&0&0&0&0&\theta_{88}\\
0&0&0&0&0&0&0&0&\theta_{99}
\end{bmatrix},
(\#eq:14-15)
\end{equation}
```

where $θ_{11}$ through $θ_{99}$ represent the variances of the residual factors of $\mathrm{x}_1$ through $\mathrm{x}_9$. These residual variances represent the variance of the associated indicator variable that is not represented by the underlying common factors. They can be interpreted as variance that is unexplained by the model. 

In Figure 3, the $\mathbf\Phi$ and $\mathbf\Theta$ parameters representing common factor variances and covariances and residual factor variances and covariances are added to the graphical display. 

```{r label="fig14-3", echo = FALSE, out.width = "80%", fig.align = "center", fig.cap="Factor model of the School Attitudes Questionnaire including common factor and residual factor (co)variance parameters" }

knitr::include_graphics("images/Ch14_SAQ_3.png")
```

Now that we have found the general expression for the variances and covariances as a function of model parameters of a factor model, we can evaluate Equation \@ref(eq:14-13) of our illustrative example. Substitution of the , , and  of our example yields:

















## Appendix. Derivations of Equation 14.13 {#appendix14}

$$
\mathbf\Sigma = \text{COV}(\mathbf{x},\mathbf{x}) \Leftrightarrow
$$
(substitute Equation \@ref(eq:14-13)

$$
\mathbf\Sigma = \text{COV}(\mathbf\Lambda \mathbf\xi + \mathbf\varepsilon, \mathbf\Lambda \mathbf\xi + \mathbf\varepsilon) \Leftrightarrow
$$

(decompose)

$$
\mathbf\Sigma = \text{COV}(\mathbf\Lambda \mathbf\xi,\mathbf\Lambda \mathbf\xi) +
\text{COV}(\mathbf\Lambda \mathbf\xi,\mathbf\varepsilon)+
\text{COV}(\mathbf\varepsilon,\mathbf\Lambda \mathbf\xi)+
\text{COV}(\mathbf\varepsilon,\mathbf\varepsilon) \Leftrightarrow
$$

(residual factors are assumed not to covary with the common factors, $COV(\mathbf\xi,\mathbf\varepsilon) = 0.$)

$$
\mathbf\Sigma = \text{COV}(\mathbf\Lambda \mathbf\xi,\mathbf\Lambda \mathbf\xi) + 0 + 0 + \text{COV}(\mathbf\varepsilon,\mathbf\varepsilon) \Leftrightarrow
$$
(place constants outside brackets).

$$
\mathbf\Sigma = \mathbf\Lambda \text{COV}(\mathbf\xi,\mathbf\xi)\mathbf\Lambda^{\text{T}} +\text{COV}(\mathbf\varepsilon,\mathbf\varepsilon)\Leftrightarrow
$$

 (variances and covariances of $\mathbf\xi$ and $\mathbf\varepsilon$ are denoted $\mathbf\Phi$ and $\mathbf\Theta$)

$$
\mathbf\Sigma = \mathbf\Lambda \mathbf\Phi \mathbf\Lambda^\text{T} + \mathbf\Theta
$$
