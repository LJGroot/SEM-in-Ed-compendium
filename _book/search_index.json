[["index.html", "Compendium Structural Equation Modeling in Educational Research Preface", " Compendium Structural Equation Modeling in Educational Research Suzanne Jak and Terrence D. Jorgensen Preface "],["regression-as-mean--and-covariance-structure-analysis.html", "1 Regression as Mean- and Covariance-Structure Analysis 1.1 Installing lavaan 1.2 Types of Data Used in SEM 1.3 Regression Review 1.4 Summary Statistics 1.5 Covariance Matrix 1.6 References", " 1 Regression as Mean- and Covariance-Structure Analysis This chapter prepares the reader to learn about structural equation modeling (SEM) by reviewing the fundamentals of ordinary least-squares (OLS) regression. Different types of data are discussed, and some review of matrix algebra will be incorporated into the regression review. 1.1 Installing lavaan The SEM software that we will use is an open-source R package called . To install the lavaan package you can type the following command in the R console: install.packages(&quot;lavaan&quot;, dependencies = TRUE) Depending on your () settings, this might open a window where you have to select a mirror (select [your country], [closest city]) prior to installing the package, including all additional packages that it depends on. You only need to install the package once (on a specific computer), but you must also load the library to access its functionality (similar to first installing an app, then opening the app to use it): library(lavaan) ## Warning: package &#39;lavaan&#39; was built under R version 4.1.3 Every time you start R you need to use this command to activate the lavaan package into the current R workspace. Therefore, it is advisable to start every script with this command. Sometimes, it will be necessary to install the development version of lavaan before the next version is made available on CRAN. For example, if the developer has fixed a bug or introduced a new feature, the development version can be installed using this syntax: install.packages(&quot;lavaan&quot;, repos = &quot;https://www.da.ugent.be&quot;, type = &quot;source&quot;) 1.2 Types of Data Used in SEM SEM can be seen as a generalization of the general(ized) linear model (GLM) for multivariate data. But rather than merely allowing multiple predictors and multiple outcomes, any variable can operate both as a predictor and an outcome in the same model. This allows us to model complex relationships among variables, including chains of causation (i.e., mediation). Estimation of a GLM involves minimizing discrepancies (differences) between observed (\\(y\\)) and expected (\\(\\hat{y}\\)) casewise values (i.e., an individual subjects score on an outcome variable \\(y\\)). In contrast, estimation of a SEM involves minimizing discrepancies between observed and expected summary statistics (i.e., the modeled variables means and covariance matrix). This is why another (older) name for an SEM is covariance structure analysis (when only trying to explain why variables are related to each other), or mean and covariance structure (MACS) analysis when means are also modeled. Summary statistics can be calculated from observed raw data, so raw data can be provided directly to SEM software for analysis. But if data are complete and come from a multivariate normal distribution, summary statistics can also be passed directly to the SEM software, which has advantages. Unlike raw data, summary statistics do not compromise the privacy of the research participants, so summary statistics can be reported in published empirical research. That allows readers (and students like you!) to have access to the same summary statistics to replicate the results or to fit a different model (e.g., that represents a competing theory). Thus, this chapter will show how to import both raw and summary data, as well as how to calculate summary statistics from raw data so that they can be reported along with results. 1.2.1 Raw Data Typically, data are already stored in an external file with a predefined format, such as SPSS (*.sav) or Excel (*.xlsx), and there are some pre-specified functions in R that can read in data from such type of formats, although they require the installation of specific packages. As an alternative, you can store the data in a plain text (with .txt or .csv extensions), where each line represents the observed responses of one individual. Spaces, commas, semicolons or tabs can be used to separate the individual responses. Some programs (like SPSS) can also export data to these types of file formats. read.table(&quot;data.txt&quot;, header = TRUE) read.spss(&quot;data.sav&quot;, to.data.frame = TRUE) # requires the package foreign read.xls(&quot;data.xls&quot;) # requires the package gdata as.data.frame(read_excel(&quot;data.xlsx&quot;)) # requires the package readxl This chapter uses data from a tutorial published on the . dat &lt;- foreign::read.spss(&quot;MODMED.sav&quot;, to.data.frame = TRUE)[c(2, 3, 5, 7)] head(dat) ## MOOD NFC POS ATT ## 1 -1 3.07579 6.2500 18.5455 ## 2 -1 2.59489 -5.0671 -16.9684 ## 3 -1 -1.00952 0.9346 -5.9578 ## 4 -1 -0.43824 2.8668 -7.2256 ## 5 -1 0.21788 -16.5601 -26.7000 ## 6 -1 0.43842 -13.1365 -24.1241 MOOD indicates experimental conditions \\(+1\\) = positive mood induction \\(-1\\) = neutral mood (control condition) NFC = need for cognition, POS = positive thoughts, and ATT = attitude 1.2.2 Summary Statistics from Raw Data To report summary statistics of your modeled variables, they can be calculated using the colMeans() function for means, the cov() function for a covariance matrix, and the nrow() function for the sample size Note: Counting the number of rows assumes you have complete data (M &lt;- colMeans(dat)) # mean vector ## MOOD NFC POS ATT ## 0.0000000 -0.0000002 0.0000040 1.9807230 (S &lt;- cov(dat)) # covariance matrix ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.03243879 4.3546465 6.841187 ## NFC -0.03243879 1.97289018 0.7953901 2.598788 ## POS 4.35464646 0.79539011 69.2629723 87.914121 ## ATT 6.84118687 2.59878834 87.9141208 282.059756 (N &lt;- nrow(dat)) # complete-data sample size ## [1] 100 Covariances are explained in more detail in a later section, but it is sufficient here to understand that the covariance between variables \\(x\\) and \\(y\\) (i.e., \\(\\sigma_{xy}\\)) is their unstandardized correlation coefficient (\\(r_{xy}\\)). That is, a correlation is a covariance between \\(z\\) scores. Because correlations are easier to interpret than covariances, it is common to report the means, SDs, and correlations. Using the SDs, readers can rescale correlations to covariances in the orignal units of measurement (i.e., \\(\\sigma_{xy} = \\sigma_x \\times \\sigma_y \\times r_{xy}\\)). The cov2cor() function can standardize a covariance matrix, or you can use the cor() function directly. (SD &lt;- sapply(dat, sd)) ## MOOD NFC POS ATT ## 1.005038 1.404596 8.322438 16.794635 sqrt(diag(S)) # SDs == square-roots of variances ## MOOD NFC POS ATT ## 1.005038 1.404596 8.322438 16.794635 cov2cor(S) # standardize the covariance matrix, or use cor() ## MOOD NFC POS ATT ## MOOD 1.00000000 -0.02297898 0.52061891 0.4053018 ## NFC -0.02297898 1.00000000 0.06804217 0.1101663 ## POS 0.52061891 0.06804217 1.00000000 0.6289810 ## ATT 0.40530176 0.11016633 0.62898098 1.0000000 (R &lt;- cor(dat)) ## MOOD NFC POS ATT ## MOOD 1.00000000 -0.02297898 0.52061891 0.4053018 ## NFC -0.02297898 1.00000000 0.06804217 0.1101663 ## POS 0.52061891 0.06804217 1.00000000 0.6289810 ## ATT 0.40530176 0.11016633 0.62898098 1.0000000 Because covariance and correlation matrices are symmetric (and diagonal elements of a correlation matrix are 1 by definition), both could be reported in a single table. For example, the upper triangle could be correlations, and the lower triangle (including the diagonal) could be (co)variances. printS &lt;- S printS[upper.tri(R)] &lt;- R[upper.tri(R)] printS # not symmetric, correlations in upper triangle ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.02297898 0.52061891 0.4053018 ## NFC -0.03243879 1.97289018 0.06804217 0.1101663 ## POS 4.35464646 0.79539011 69.26297231 0.6289810 ## ATT 6.84118687 2.59878834 87.91412077 282.0597561 1.2.2.1 Summary Statistics for Multiple Groups In SEM, it is common to estimate model parameters simultaneously in multiple groups (i.e., multigroup SEM or MG-SEM). In our example data, the MOOD variable is a 2-group variable, so we could create a factor from the numeric codes: dat$mood.f &lt;- factor(dat$MOOD, levels = c(-1, 1), labels = c(&quot;neutral&quot;,&quot;positive&quot;)) Then we can calculate the summary statistics separately within each group. modVars &lt;- c(&quot;ATT&quot;,&quot;NFC&quot;,&quot;POS&quot;) # modeled variables gM &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) colMeans(dat[dat$mood.f == g, modVars]) ) gS &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) cov(dat[dat$mood.f == g, modVars]) ) gN &lt;- table(dat$mood.f) To fit a MG-SEM in lavaan, the summary statistics must be a list with one (mean vector or covariance matrix) per group, as well as a vector of group sample sizes. gM ## $neutral ## ATT NFC POS ## -4.7920520 0.0321142 -4.3110960 ## ## $positive ## ATT NFC POS ## 8.7534980 -0.0321146 4.3111040 gS ## $neutral ## ATT NFC POS ## ATT 245.020884 4.812796 61.799307 ## NFC 4.812796 2.456274 -1.201971 ## POS 61.799307 -1.201971 47.663230 ## ## $positive ## ATT NFC POS ## ATT 231.2417228 0.8817017 56.235120 ## NFC 0.8817017 1.5276643 3.091531 ## POS 56.2351197 3.0915313 54.346483 gN ## ## neutral positive ## 50 50 1.2.3 Importing Summary Data 1.2.3.1 Full Covariance Matrix If we are importing summary data (e.g., from an article that reported them), we can type our data directly to our R script. Suppose the values above from S were rounded to 3 decimal places: covVals &lt;- c( 1.010, -0.032, 4.355, 6.841, -0.032, 1.973, 0.795, 2.599, 4.355, 0.795, 69.263, 87.914, 6.841, 2.599, 87.914, 282.060) covVals ## [1] 1.010 -0.032 4.355 6.841 -0.032 1.973 0.795 2.599 4.355 ## [10] 0.795 69.263 87.914 6.841 2.599 87.914 282.060 If these values were saved as plain text, we could also read the data from a file using the scan() function, which returns a vector that contains the stored values in the file values.txt. covVals &lt;- scan(&quot;values.txt&quot;) Either way, we can place these values from the full covariance matrix into a matrix using the matrix() function, specifying how many rows and columns there are: (newS &lt;- matrix(data = covVals, nrow = 4, ncol = 4)) ## [,1] [,2] [,3] [,4] ## [1,] 1.010 -0.032 4.355 6.841 ## [2,] -0.032 1.973 0.795 2.599 ## [3,] 4.355 0.795 69.263 87.914 ## [4,] 6.841 2.599 87.914 282.060 We can give names to the variables in the covariance matrix by using the dimnames= argument of the matrix() function, or by using the dimnames() function after creating the matrix. Lets save the variable names in an object obsnames, then use them to assign names. obsnames &lt;- c(&quot;MOOD&quot;, &quot;NFC&quot;, &quot;POS&quot;, &quot;ATT&quot;) ## providing names when creating matrix newS &lt;- matrix(covVals, nrow = 4, ncol = 4, dimnames = list(obsnames, obsnames)) ## or assign afterward, all at once dimnames(newS) &lt;- list(obsnames, obsnames) ## or one dimension at a time (useful for asymmetric matrix) rownames(newS) &lt;- obsnames colnames(newS) &lt;- obsnames 1.2.3.1.1 Rescaling a Correlation Matrix If the imported matrix is instead a full correlation matrix (i.e., assuming all variables have \\(SD=1\\)), then it can be transformed back into a covariance matrix using the \\(SD\\)s, so variables will be in their original units. This is important because an SEM fitted to a correlation matrix will have biased \\(SE\\)s and inflated Type I error rates unless complex constraints are imposed. ## store values from correlation matrix corVals &lt;- c( 1, -0.023, 0.521, 0.405, -0.023, 1, 0.068, 0.110, 0.521, 0.068, 1, 0.629, 0.405, 0.110, 0.629, 1) newR &lt;- matrix(data = corVals, nrow = 4, ncol = 4, dimnames = list(obsnames, obsnames)) newR ## MOOD NFC POS ATT ## MOOD 1.000 -0.023 0.521 0.405 ## NFC -0.023 1.000 0.068 0.110 ## POS 0.521 0.068 1.000 0.629 ## ATT 0.405 0.110 0.629 1.000 ## store SDs as a diagonal matrix (SDs &lt;- diag(SD)) ## [,1] [,2] [,3] [,4] ## [1,] 1.005038 0.000000 0.000000 0.00000 ## [2,] 0.000000 1.404596 0.000000 0.00000 ## [3,] 0.000000 0.000000 8.322438 0.00000 ## [4,] 0.000000 0.000000 0.000000 16.79463 ## transform correlations to covariances scaled.S &lt;- SDs %*% newR %*% SDs round(scaled.S, 3) ## [,1] [,2] [,3] [,4] ## [1,] 1.010 -0.032 4.358 6.836 ## [2,] -0.032 1.973 0.795 2.595 ## [3,] 4.358 0.795 69.263 87.917 ## [4,] 6.836 2.595 87.917 282.060 ## matches covariance matrix (within rounding error) newS ## MOOD NFC POS ATT ## MOOD 1.010 -0.032 4.355 6.841 ## NFC -0.032 1.973 0.795 2.599 ## POS 4.355 0.795 69.263 87.914 ## ATT 6.841 2.599 87.914 282.060 Note that rescaling the correlation matrix to be a covariance matrix required pre- and postmultipication of the (diagnal matrix of) \\(SD\\)s. The operator for matrix multiplication is %*% (the normal scalar multiplication operator * would simply multiply each cell of one matrix with the corresponding cell of another matrix). The table below gives an overview of commonly used functions in matrix algebra in R. Symbol Function Example in R \\(A+B\\) Addition A + B \\(A-B\\) Subtraction A - B (none) Elementwise Multiplication (in R) A * B (none) Elementwise Division (in R) A / B \\(AB\\) Matrix Multiplication A %*% B \\(A^{-1}\\) Inverse (enables division) solve(A) \\(A^{&#39;}\\) Transpose (rows become columns) t(A) \\(|A|\\) Determinant (generalized variance) det(A) To review basic matrix algebra, consult a linear algebra text or an appendix of some SEM textbooks (e.g., Bollen, 1989; Schumacker &amp; Lomax, 2016). 1.2.3.2 Upper/Lower Triangle of a Covariance Matrix As a covariance matrix is a symmetric matrix, we do not need to provide the complete matrix. Instead, we can import only the nonredundant information from only the lower (or upper) triangle of the covariance matrix, then use the lavaan function getCov() to create the full matrix. By default, getCov() expects the values from the lower triangle, and it will read values row-by-row (i.e., left-to-right, top-to-bottom), including the diagonal elements (diagonal = TRUE is the default argument). \\[ \\begin{bmatrix} 1.010 &amp; &amp; &amp; \\\\ -0.032 &amp; 1.973 &amp; &amp; \\\\ 4.355 &amp; 0.795 &amp; 69.263 &amp; \\\\ 6.841 &amp; 2.599 &amp; 87.914 &amp; 282.060 \\end{bmatrix} \\] This is equivalent to reading the upper triangle column-by-column (i.e., top-to-bottom, left-to-right). One can also add a vector with the names= of the observed variables. It returns the complete covariance matrix, including row and column names. The following code can be used to create a full matrix that is equal to S and newS above, using only the values from the lower triangle of the matrix. lowerS &lt;- c( 1.010, -0.032, 1.973, 4.355, 0.795, 69.263, 6.841, 2.599, 87.914, 282.060) getCov(x = lowerS, names = obsnames) ## MOOD NFC POS ATT ## MOOD 1.010 -0.032 4.355 6.841 ## NFC -0.032 1.973 0.795 2.599 ## POS 4.355 0.795 69.263 87.914 ## ATT 6.841 2.599 87.914 282.060 Sometimes you will find the upper triangle reported in a published paper instead of the lower triangle. Because getCov() only reads values row-by-row (i.e., left-to-right, top-to-bottom), reading an upper-triangle requires changing the argument to lower = FALSE. \\[ \\begin{bmatrix} 1.010 &amp; -0.032 &amp; 4.355 &amp; 6.841 \\\\ &amp; 1.973 &amp; 0.795 &amp; 2.599 \\\\ &amp; &amp; 69.263 &amp; 87.914 \\\\ &amp; &amp; &amp; 282.060 \\end{bmatrix} \\] upperS &lt;- c(1.010, -0.032, 4.355, 6.841, 1.973, 0.795, 2.599, 69.263, 87.914, 282.060) getCov(x = upperS, names = obsnames, lower = FALSE) ## MOOD NFC POS ATT ## MOOD 1.010 -0.032 4.355 6.841 ## NFC -0.032 1.973 0.795 2.599 ## POS 4.355 0.795 69.263 87.914 ## ATT 6.841 2.599 87.914 282.060 1.2.3.2.1 Upper/Lower Triangle of a Correlation Matrix Quite frequently, in published papers the covariance matrix is not provided, but instead a correlation matrix and \\(SD\\)s are given separately. The getCov() argument sds= can be used to automatically rescale the correlation matrix. Because the diagonal of a correlation matrix is always 1, it is not necessary to include the diagonal values. \\[ \\begin{bmatrix} 1 &amp; &amp; &amp; \\\\ -0.023 &amp; 1 &amp; &amp; \\\\ 0.521 &amp; 0.068 &amp; 1 &amp; \\\\ 0.405 &amp; 0.110 &amp; 0.629 &amp; 1 \\end{bmatrix} \\] In this case, tell lavaan that the diagonal entries are omitted using the diagonal=FALSE argument. The following syntax creates the complete covariance matrix that was used in the previous examples from the correlations and \\(SD\\)s. getCov(x = c(-0.023, 0.521, 0.068, 0.405, 0.110, 0.629), diagonal = FALSE, sds = SD, names = obsnames) ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.03246846 4.3578341 6.836093 ## NFC -0.03246846 1.97289018 0.7948971 2.594865 ## POS 4.35783405 0.79489713 69.2629723 87.916779 ## ATT 6.83609342 2.59486462 87.9167795 282.059756 If you are wondering whether the correlations appear in the correct order in the matrix, you can first leave the \\(SD\\)s out and check that the cells of the correlation matrix are in the correct order. If everything looks correct, then you can add the \\(SD\\)s. Covariance/correlation matrices are also symmetric, so it is also important to check that the lower triangle is a reflection of the upper triangle (i.e., Row-\\(x\\) Column-\\(y\\) of the matrix should contain the same value as Row-\\(y\\) Column-\\(x\\)). The R function isSymmetric() can perform this check for you: isSymmetric(S) ## [1] TRUE 1.3 Regression Review 1.3.1 Linear Regression Models An outcome \\(y\\) is a linear combination (weighted sum) of predictors Like a recipe for the outcome: How much of \\(x_1\\) and \\(x_2\\) do you need to recreate \\(y\\)? \\[\\begin{align*} y_i &amp;= \\textcolor{blue}{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}} &amp;+&amp; \\textcolor{darkred}{\\varepsilon_i} &amp; \\\\ &amp;= \\textcolor{blue}{\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij}} &amp;+&amp; \\textcolor{darkred}{\\varepsilon_i} &amp;\\sim \\textcolor{darkred}{\\mathcal{N}(0, \\sigma)} \\end{align*}\\] The of \\(y\\) can be predicted or explained by \\(X\\) The of \\(y\\) includes all other unmodeled effects (omitted variables) and sources of error 1.3.2 Matrix Notation For subjects \\(i = 1, \\dots, N\\) in rows and predictors \\(j = 1, \\dots, p\\) in columns: \\[\\begin{equation*} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\dots &amp; x_{1,p} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\dots &amp; x_{2,p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{N,1} &amp; x_{N,2} &amp; \\dots &amp; x_{N,p} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_N \\end{bmatrix} \\end{equation*}\\] Shorthand: \\(\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon}\\) The generalized linear model (GLM) extends basic linear models by incorporating different functions that link the outcome to its linear predictor distributional assumptions about the outcome(s errors) 1.3.3 Interpreting and Estimating Coefficients Regression models provide predictions of the outcome for any level(s) of the predictor(s) Only the intercept (\\(\\beta_0\\)) is interpreted as a mean: The expected value (\\(\\hat{y}\\)) when each predictor \\(x=0\\) Each slope (\\(\\beta_j\\)) is interpreted as the change in \\(\\hat{y}\\) as \\(x_j\\) increases i.e., How does \\(y\\) covary with \\(x_j\\), given other \\(x\\)s? Ordinary Least Squares (OLS) estimator of coefficients minimizes overall discrepancies between observations and predictions (\\(y - \\hat{y}\\)): \\[\\beta = (\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} \\mathbf{X}^{&#39;} \\mathbf{y}\\] Minimizing residual variance = maximizing \\(R^2\\) \\(R\\) = Cor(\\(y, \\hat{y}\\)) 1.3.4 Syntax Examples with Raw Data Using matrix algebra to estimate the coefficients: y &lt;- dat$ATT X &lt;- cbind(`(Intercept)` = 1, # first column is constant as.matrix(dat[c(&quot;MOOD&quot;,&quot;NFC&quot;,&quot;POS&quot;)])) solve(t(X) %*% X) %*% t(X) %*% y # beta ## [,1] ## (Intercept) 1.9807186 ## MOOD 1.8839147 ## NFC 0.8883671 ## POS 1.1406346 Matches results from linear-modeling function: mod &lt;- lm(ATT ~ MOOD + NFC + POS, data = dat) coef(mod) ## (Intercept) MOOD NFC POS ## 1.9807186 1.8839147 0.8883671 1.1406346 1.3.5 Categorical Predictors Groups \\(g = 1, \\ldots, G\\) can be represented using numeric codes dummy code for Group \\(g\\) indicates whether someone belongs to Group \\(g\\) (1) or not (0) effects codes and orthogonal contrast codes are alternatives MOOD is an effects-coded to indicate experimental conditions \\(+1\\) = positive mood induction \\(-1\\) = neutral mood (control condition) table(dat$MOOD) ## ## -1 1 ## 50 50 1.3.5.1 Regression with a Baseline Group When we are interested in comparing scores on a variable across groups, we are typically interested in the differences between the group means. Recall that an intercept-only model estimates a single mean for everyone mod0 &lt;- lm(POS ~ 1, data = dat) When we add a predictor to the model, the intercept becomes a conditional mean (i.e., the expected value when each predictor = 0), and the slope represents how the means differ between groups. By default, lm() will create dummy codes for all levels of a factor variable, except for the first group (the baseline for comparison) dat$mood.f &lt;- factor(dat$MOOD, levels = c(-1, 1), labels = c(&quot;neutral&quot;,&quot;positive&quot;)) coef(mod1 &lt;- lm(POS ~ mood.f, data = dat)) ## (Intercept) mood.fpositive ## -4.311096 8.622200 Practice interpreting the intercept and slope: aggregate(POS ~ mood.f, data = dat, FUN = mean) ## mood.f POS ## 1 neutral -4.311096 ## 2 positive 4.311104 diff(aggregate(POS ~ mood.f, dat, mean)$POS) ## [1] 8.6222 1.3.5.2 Regression with Group-Specific Intercepts Rather than choosing a baseline group, we can omit the intercept from the model, in which case lm() will include all \\(G\\) dummy codes (not \\(G-1\\)) coef(mod2 &lt;- lm(POS ~ -1 + mood.f, data = dat)) ## mood.fneutral mood.fpositive ## -4.311096 4.311104 Essentially, this model gives each group its own intercept, which is also the group-mean when there are no other predictors in the model aggregate(POS ~ mood.f, data = dat, FUN = mean) ## mood.f POS ## 1 neutral -4.311096 ## 2 positive 4.311104 Later in this course, you will learn about multigroup SEM, which is when the same SEM is fitted separately (but simultaneously) to 2 or more groups. Multigroup SEM is analogously advantageous, but (unlike in the GLM) variances can also differ across groups, so we do not need to assume homoskedasticity when using multigroup SEM. 1.3.5.3 Baseline Comparison vs. Group-Specific Intercepts To test the \\(H_0\\) that the group means are equivalent, we can compare the model that represents \\(H_0\\) (i.e., the intercept-only model, which estimates a single mean for all groups) to the model that represents the alternative hypothesis (\\(H_A\\)) that group means may differ. Because the models mod1 (intercept + 1 slope) and mod2 (2 slopes that are group-specific intercepts) are statistically equivalent (i.e., they make identical predictions), the ANOVA results are identical: anova(mod0, mod1) ## Analysis of Variance Table ## ## Model 1: POS ~ 1 ## Model 2: POS ~ mood.f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 6857.0 ## 2 98 4998.5 1 1858.6 36.439 2.819e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod0, mod2) ## Analysis of Variance Table ## ## Model 1: POS ~ 1 ## Model 2: POS ~ -1 + mood.f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 6857.0 ## 2 98 4998.5 1 1858.6 36.439 2.819e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 That is, both mod1 and mod2 say that we need 2 parameters to make good predictors (i.e., separate means for 2 groups), whereas the intercept-only model says that estimating 1 parameter provides similar predictions. 1.3.5.4 \\(t\\) vs. \\(F\\) Tests The group mean difference can also be tested if it is an estimated parameter, as it is in the baseline-group approach ## ## Call: ## lm(formula = POS ~ mood.f, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.7416 -5.6706 -0.4135 4.4780 17.4323 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.311 1.010 -4.268 4.55e-05 *** ## mood.fpositive 8.622 1.428 6.036 2.82e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.142 on 98 degrees of freedom ## Multiple R-squared: 0.271, Adjusted R-squared: 0.2636 ## F-statistic: 36.44 on 1 and 98 DF, p-value: 2.819e-08 Same \\(p\\) value, \\(t^2 = F\\) from ANOVA But group-specific intercepts (and slopes) can also be advantageous Each groups intercept and slopes are model parameters, rather than functions of parameters when using a baseline group 1.3.6 Constant vs. Varying Slopes Suppose we are investigating the possibility that this grouping variable moderates the effect of another (focal) predictor: Need for cognition (NFC). See this primer for a review. To represent the \\(H_0\\) that NFC has a constant effect on positive thoughts across groups, we can add it as a predictor to the model without an intercept. In this case, rather than group-specific means, the dummy codes are group-specific intercepts. slope0 &lt;- lm(POS ~ -1 + mood.f + NFC, data = dat) summary(slope0) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.1713 -5.4855 -0.2585 4.5490 16.9279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.3263 1.0109 -4.280 4.39e-05 *** ## mood.fpositive 4.3263 1.0109 4.280 4.39e-05 *** ## NFC 0.4743 0.5115 0.927 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 97 degrees of freedom ## Multiple R-squared: 0.2774, Adjusted R-squared: 0.2551 ## F-statistic: 12.42 on 3 and 97 DF, p-value: 6.115e-07 An interaction implies that slopes differ across groups With a baseline group, the reference-groups slope is estimated, along with how much each of \\(G-1\\) groups slopes differ from it ## or POS ~ mood.f * NFC slope1 &lt;- lm(POS ~ mood.f + NFC + mood.f:NFC, data = dat) summary(slope1) ## ## Call: ## lm(formula = POS ~ mood.f + NFC + mood.f:NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.3300 -5.3633 0.3697 4.7769 15.2801 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.2954 0.9858 -4.357 3.3e-05 *** ## mood.fpositive 8.6715 1.3943 6.219 1.3e-08 *** ## NFC -0.4893 0.6353 -0.770 0.4430 ## mood.fpositive:NFC 2.5130 1.0259 2.450 0.0161 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.969 on 96 degrees of freedom ## Multiple R-squared: 0.32, Adjusted R-squared: 0.2987 ## F-statistic: 15.06 on 3 and 96 DF, p-value: 4.168e-08 With group-specific parameters, each groups own intercept and slope are estimated, but multiplying each groups dummy code against the moderator ## no &quot;main effect&quot; of NFC without a reference group slope2 &lt;- lm(POS ~ -1 + mood.f + mood.f:NFC, data = dat) summary(slope2) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + mood.f:NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.3300 -5.3633 0.3697 4.7769 15.2801 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.2954 0.9858 -4.357 3.30e-05 *** ## mood.fpositive 4.3761 0.9860 4.438 2.42e-05 *** ## mood.fneutral:NFC -0.4893 0.6353 -0.770 0.4430 ## mood.fpositive:NFC 2.0237 0.8055 2.512 0.0137 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.969 on 96 degrees of freedom ## Multiple R-squared: 0.32, Adjusted R-squared: 0.2916 ## F-statistic: 11.29 on 4 and 96 DF, p-value: 1.498e-07 1.4 Summary Statistics 1.4.1 Reimport Data The next section is easier if all the variables in the data.frame are numeric, so lets just use the given effects-coded MOOD dat &lt;- foreign::read.spss(&quot;MODMED.sav&quot;, to.data.frame = TRUE)[c(2, 3, 5, 7)] 1.4.2 Raw Data vs. Summary Statistics Regression models include both mean-structure parameters (intercepts) and covariance-structure parameters (slopes, residual variance). Much of SEM involves the analysis of mean and covariance structures (MACS). OLS regression minimizes discrepancies between observed and predicted casewise observations of \\(y\\) \\(y - \\hat{y}\\) LS and ML estimators in SEM minimize discrepancies between observed and predicted summary statistics mean vector: \\(\\bar{y} - \\widehat{\\mu}\\) \\(\\bar{y}\\) = observed sample means \\(\\widehat{\\mu}\\) = expected/predicted/model-implied population means covariance matrix: \\(\\mathbf{S} - \\widehat{\\Sigma}\\) \\(\\mathbf{S}\\) = observed sample (co)variances \\(\\widehat{\\Sigma}\\) = expected/predicted/model-implied population (co)variances Rather than raw data in \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), we can obtain the same estimates using summary statistics alone 1.4.3 What Is Covariance? Variance (\\(\\sigma^2_y\\)) quantifies how individual scores vary the mean of squared deviations from \\(\\bar{y}\\) \\[\\text{Var}(y) = \\frac{\\sum_{i=1}^N (y_i - \\bar{y})^2}{N-1} = \\frac{\\sum_{i=1}^N (y_i - \\bar{y})(y_i - \\bar{y})}{N-1}\\] Covariance (\\(\\sigma_{xy}\\)) quantifies how \\(x\\) and \\(y\\) are linearly related covary = vary together \\[\\text{Cov}(x,y) = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{N-1}\\] Difficult to interpret its absolute value bound by \\(\\pm \\sigma_x \\sigma_y\\) so, dividing \\(\\sigma_{xy}\\) by \\(\\sigma_x \\sigma_y\\) limits the range to \\(\\pm 1\\) For \\(&gt;2\\) variables in a data set \\(\\mathbf{Y}\\), we can simultaneously calculate all (co)variances using matrix algebra: Mean-center the data-matrix \\(\\mathbf{Y} - \\bar{\\mathbf{Y}}\\) Square it by (pre)multiplying it by its transpose \\[\\Sigma = (\\mathbf{Y} - \\bar{\\mathbf{Y}})^{&#39;} (\\mathbf{Y} - \\bar{\\mathbf{Y}})\\] Variances are on the diagonal of \\(\\Sigma\\), and each off-diagonal cell contains the covariance between the row-variable and the column-variable. 1.4.4 Interpreting Covariance We can scale the covariance to provide familiar interpretations. Slopes are change in \\(y\\) (or \\(x\\)) per unit \\(x\\) (or \\(y\\)) divide by variance: \\(\\beta_{yx} = \\frac{\\sigma_{xy}}{\\sigma^2_x} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_x}\\) (or \\(\\frac{\\sigma_{xy}}{\\sigma^2_y} = \\frac{\\sigma_{xy}}{\\sigma_y \\sigma_y}\\)) Correlations are standardized covariances (bound by \\(\\pm 1\\)) divide by both \\(SD\\)s: \\(\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\) This is how slopes can be standardized (\\(\\beta^*\\)), which is a correlation when there is only 1 predictor \\[\\begin{align*} \\beta^*_{yx} &amp;= \\beta_{yx} \\times \\frac{\\sigma_x}{\\sigma_y} \\\\ &amp;= \\frac{\\sigma_{xy}}{\\mathcolorbox{orange}{\\sigma_x} \\sigma_x} \\times \\frac{\\mathcolorbox{orange}{\\sigma_x}}{\\sigma_y} \\text{ (\\colorbox{orange}{these} cancel out)} \\\\ &amp;= \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y} (= \\rho_{xy}) \\end{align*}\\] 1.5 Covariance Matrix With mean-centered predictors, \\((\\mathbf{X}^{&#39;} \\mathbf{X})^{-1}\\) is their covariance matrix and \\(\\mathbf{X}^{&#39;} \\mathbf{y}\\) captures their covariances with the outcome n &lt;- nrow(dat) - 1L Xc &lt;- scale(dat[c(&quot;MOOD&quot;,&quot;NFC&quot;,&quot;POS&quot;)], center = TRUE, scale = FALSE) cbind(t(Xc) %*% Xc , t(Xc) %*% y) / n ## MOOD NFC POS ## MOOD 1.01010101 -0.03243879 4.3546465 6.841187 ## NFC -0.03243879 1.97289018 0.7953901 2.598788 ## POS 4.35464646 0.79539011 69.2629723 87.914121 cov(dat) ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.03243879 4.3546465 6.841187 ## NFC -0.03243879 1.97289018 0.7953901 2.598788 ## POS 4.35464646 0.79539011 69.2629723 87.914121 ## ATT 6.84118687 2.59878834 87.9141208 282.059756 1.5.1 OLS with Summary Statistics: \\(\\beta = (\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} \\mathbf{X}^{&#39;} \\mathbf{y}\\) In simple regression (1 predictor), the slope is simply the covariance scaled by the predictors variance: \\(\\beta_1 = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}\\) \\(\\mathbf{X}^{&#39;} \\mathbf{X} = \\text{Var}(x)\\), so \\((\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} = \\frac{1}{\\text{Var}(x)}\\) matrix-multiplication by inverse \\(\\approx\\) division \\(\\mathbf{X}^{&#39;} \\mathbf{y} = \\text{Cov}(x,y)\\) In multiple regression, the same formulas additionally account for covariances of \\(x\\) and \\(y\\) with covariates cov(dat)[&quot;POS&quot;,&quot;ATT&quot;] / var(dat$POS) ## [1] 1.26928 coef(lm(ATT ~ POS, data = dat))[[&quot;POS&quot;]] ## [1] 1.26928 1.5.2 Intercepts and Means With uncentered data, \\(X\\) can include a constant (all 1s) in the first column, which effectively partials out the mean. With no other predictors (an intercept-only model): \\[\\begin{align*} y_i &amp;= \\beta_0 (1) + \\varepsilon_i \\\\ &amp;= \\bar{y} + (y_i - \\bar{y}) \\end{align*}\\] the intercept is the mean residuals are mean-centered \\(y\\) (same variance) OLS simplifies to the arithmetic mean formula: \\(\\bar{y} = \\frac{\\Sigma y}{N}\\) \\((\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} = \\frac{1}{N}\\) \\(\\mathbf{X}^{&#39;} \\mathbf{y} = \\sum y\\) 1.5.3 Intercepts and Means ## using matrix algebra ONE &lt;- X[ , &quot;(Intercept)&quot;] solve(t(ONE) %*% ONE) %*% t(ONE) %*% y ## [,1] ## [1,] 1.980723 ## fitted linear model with only an intercept coef(lm(y ~ 1)) ## (Intercept) ## 1.980723 ## calculate the mean mean(y) ## [1] 1.980723 1.5.4 What Are Mean and Covariance Structures? Regression models already illustrate how means are structured A mean (\\(\\bar{y}\\)) is an expected value under a particular condition (\\(\\beta_0\\) when \\(X=0\\)) plus predictors means times their slopes: \\(\\bar{y} = \\beta_0 + \\sum \\beta_j \\bar{x}_j\\) Recall how often statistical procedures involve partitioning variance into components Regression: explained (\\(R^2\\)) vs. unexplained (residual) variance ANOVA: variance between vs. within groups Multilevel modeling: Level-1 v. -2 variance and (un)explained at each level Between vs. within multiple imputations of missing data Covariance can also be partitioned into components Variance is merely a variable covarying with itself We can account for multiple reasons why 2 variables are related The decomposition of covariances will be explained after introducing path analysis. 1.6 References Bollen, K. A. (1989). Structural equations with latent variables. Wiley. http://dx.doi.org/10.1002/9781118619179 Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 136. http://dx.doi.org/10.18637/jss.v048.i02 Schumacker, R. E., &amp; Lomax, R. G. (2016). A beginners guide to structural equation modeling (4th ed.). Lawrence Erlbaum. "],["using-lavaan-to-run-regression-and-anova-as-a-sem.html", "2 Using lavaan to Run Regression and ANOVA as a SEM 2.1 Prepare Data and Workspace 2.2 Multiple Regression in SEM 2.3 Caveats with SEM 2.4 Multigroup SEM", " 2 Using lavaan to Run Regression and ANOVA as a SEM 2.1 Prepare Data and Workspace 2.1.1 Import Example Data We will use data from a tutorial published on the . dat &lt;- foreign::read.spss(&quot;MODMED.sav&quot;, to.data.frame = TRUE)[c(2, 3, 5, 7)] ## Save summary statistics M &lt;- colMeans(dat) S &lt;- cov(dat) N &lt;- nrow(dat) Recall from Chapter 1 that: SEM can equivalently be fitted to summary statistics rather than raw data It is possible to fit a multigroup SEM to obtain parameter estimates separately per group This chapter will show how to fit a multigroup SEM to raw data as well as to summary statistics. For the latter, we require group-specific summary statistics, which Chapter 1 discussed how to obtain: ## Create factor from effects-coded conditions dat$mood.f &lt;- factor(dat$MOOD, levels = c(-1, 1), labels = c(&quot;neutral&quot;,&quot;positive&quot;)) ## Save lists of group-specific summary statistics CC &lt;- c(&quot;ATT&quot;,&quot;NFC&quot;,&quot;POS&quot;) # when group = &quot;mood.f&quot; gM &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) colMeans(dat[dat$mood.f == g, CC]) ) gS &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) cov(dat[dat$mood.f == g, CC]) ) gN &lt;- table(dat$mood.f) 2.1.2 Load lavaan into Workspace library(lavaan) 2.2 Multiple Regression in SEM 2.2.1 lavaan Syntax The standard syntax for regression is a formula object. To regress ATTitude on MOOD condition, Need For Cognition, and POSitive thoughts: ATT ~ MOOD + NFC + POS SEM is a multivariate modeling framework There can be many outcome variables, each with its own formula Outcomes can also predict other outcomes There can be unobserved (latent) variables So lavaan requires collecting these simultaneous equations in a character vector, even when there is only one outcome &#39; ATT ~ MOOD + NFC + POS &#39; 2.2.1.1 OLS Regression Using the familiar lm() function to obtain OLS estimates of a regression model: ols &lt;- lm(ATT ~ MOOD + NFC + POS, data = dat) summary(ols) ## ## Call: ## lm(formula = ATT ~ MOOD + NFC + POS, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.061 -8.440 -0.417 8.718 28.010 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9807 1.3107 1.511 0.134 ## MOOD 1.8839 1.5388 1.224 0.224 ## NFC 0.8884 0.9422 0.943 0.348 ## POS 1.1406 0.1862 6.126 1.98e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.11 on 96 degrees of freedom ## Multiple R-squared: 0.4094, Adjusted R-squared: 0.3909 ## F-statistic: 22.18 on 3 and 96 DF, p-value: 5.382e-11 2.2.1.2 MLE in lavaan with Raw or Summary Data When data are complete, maximum likelihood estimation (MLE) provides equivalent results using either raw or summary data. There are multiple model-fitting functions in lavaan: lavaan() is the main engine, but expects that models are fully specified in complete detail sem() is a wrapper that calls lavaan() with some sensible defaults that apply to most SEMs (e.g., automatically estimating residual variances, automatically estimating covariances among predictors) cfa() is meant for fitting confirmatory factor analysis (CFA) models, discussed in a later chapter. cfa() and sem() actually behave identically (i.e., they call lavaan() with the same defaults) growth() is meant for very simple latent growth curve models, also discussed in a later chapter For now, we will use the sem() function similar to the way we use the lm() function, including how we obtain results from summary(). ## Raw Data mle &lt;- sem(&#39;ATT ~ MOOD + NFC + POS&#39;, data = dat, meanstructure = TRUE) # explicitly request intercept ## Summary Data mle &lt;- sem(&#39;ATT ~ MOOD + NFC + POS&#39;, sample.nobs = N, sample.cov = S, sample.mean = M) Now inspect the output. Identify each parameter estimate, and compare it to the corresponding OLS estimate in the lm() results. summary(mle, rsquare = TRUE, header = FALSE, nd = 4) ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## ATT ~ ## MOOD 1.8839 1.5077 1.2495 0.2115 ## NFC 0.8884 0.9232 0.9623 0.3359 ## POS 1.1406 0.1824 6.2519 0.0000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .ATT 1.9807 1.2842 1.5424 0.1230 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ATT 164.9191 23.3231 7.0711 0.0000 ## ## R-Square: ## Estimate ## ATT 0.4094 sigma(ols)^2 # compare to residual variance from OLS ## [1] 171.7908 2.2.1.3 Compare/Contrast Estimates lavaans point estimates match OLS, except residual variance (\\(\\sigma^2_\\varepsilon\\)) OLS estimates \\(\\sigma^2_\\varepsilon\\) by dividing the residual \\(SS\\) by the residual \\(df\\) SEM estimators choose estimates that reproduce \\(\\bar{y}\\) and \\(S\\) If we calculate variance of the residuals (\\(y_i - \\hat{y}\\)) using the population formula (dividing by \\(N\\)), we obtain lavaans result sigma(ols)^2 # residual variance from OLS ## [1] 171.7908 sum(resid(ols)^2) / N # manually calculated using POPULATION formula ## [1] 164.9191 coef(mle)[[&quot;ATT~~ATT&quot;]] # matches lavaan ## [1] 164.9191 Note: The double-tilde (~~) operator signifies a two-headed arrow, in contrast to the one-headed arrow (~) of a directed effect 2.2.1.4 Compare/Contrast \\(SE\\)s The estimated \\(SE\\)s are smaller from lavaan because SEM is an asymptotic method estimator assumes \\(N\\) is sufficiently large that \\(S \\approx \\Sigma\\) Wald \\(z\\) statistics replace OLSs \\(t\\) statistics Model comparison results in a \\(\\chi^2\\) rather than \\(F\\) statistic The asymptotic assumption yields more power, but inflated Type I error rates when sample size is low (relative to number of estimated parameters). Note that lavaan simply calculates sample.cov= and sample.mean= from data=, but they can be passed directly. Also note that raw data= are required to analyze incomplete data (using full information MLE rather than listwise deletion), obtain robust statistics (e.g., to account for nonnormality of continuous outcomes), or model discrete (binary or ordinal) outcomes (which require a threshold model, explained later in a chapter about categorical data) 2.3 Caveats with SEM 2.3.1 Model Comparison with OLS To test the \\(H_0\\) that the experimental manipulation (MOOD) explains \\(R^2=0\\)% of variance in POSitive thoughts, compare models that represent the \\(H_0\\) and \\(H_A\\) ols0 &lt;- lm(POS ~ 1, data = dat) ols1 &lt;- lm(POS ~ MOOD, data = dat) anova(ols0, ols1) ## Analysis of Variance Table ## ## Model 1: POS ~ 1 ## Model 2: POS ~ MOOD ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 6857.0 ## 2 98 4998.5 1 1858.6 36.439 2.819e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This result matches the \\(F\\) test at the bottom of summary(ols1) 2.3.2 Model Comparison with lavaan Omitting variables from an SEM means they are removed from \\(\\bar{y}\\) and \\(S\\), returning a warning mle0 &lt;- sem(&#39;POS ~ 1&#39;, data = dat) mle1 &lt;- sem(&#39;POS ~ 1 + MOOD&#39;, data = dat) anova(mle0, mle1) ## ... models are based on a different set of observed variables Instead, keep predictor(s) in the model, but fix the slope(s) to zero mle0 &lt;- sem(&#39;POS ~ 1 + 0*MOOD&#39;, data = dat) anova(mle0, mle1) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mle1 0 680.96 688.78 0.000 ## mle0 1 710.57 715.78 31.614 31.614 1 1.88e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.3.3 Moderation Caveats When evaluating moderation using a product term, that product term is a new variable in \\(\\bar{y}\\) and \\(S\\) e.g., we want to test MOODs effect, controlling for NFC before comparing adjusted group means, we should evaluate the homogeneity-of-slopes assumption As in formula objects, lavaan syntax recognizes the colon (:) operator but the asterisk (*) is reserved for assigning labels or values to parameters POS ~ 1 + MOOD + NFC + MOOD:NFC MOOD:NFC is now an additional variable in \\(\\bar{y}\\) and \\(S\\), so comparison to any model without that interaction term requires including the product term but fixing its effect to zero 2.3.3.1 Estimate and Test an Interaction with OLS ols.hom &lt;- lm(POS ~ MOOD + NFC, data = dat) ols.het &lt;- lm(POS ~ MOOD + NFC + MOOD:NFC, data = dat) anova(ols.hom, ols.het) ## Analysis of Variance Table ## ## Model 1: POS ~ MOOD + NFC ## Model 2: POS ~ MOOD + NFC + MOOD:NFC ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 4954.6 ## 2 96 4663.1 1 291.47 6.0005 0.01611 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using ML estimation, model comparison would analogously yield an analysis of deviance a.k.a. likelihood ratio test (LRT) 2.3.3.2 Estimate and Test an Interaction with lavaan mle.hom &lt;- sem(&#39;POS ~ 1 + MOOD + NFC + 0*MOOD:NFC&#39;, data = dat) mle.het &lt;- sem(&#39;POS ~ 1 + MOOD + NFC + MOOD:NFC&#39;, data = dat) anova(mle.hom, mle.het) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mle.het 0 678.01 691.04 0.0000 ## mle.hom 1 682.08 692.50 6.0629 6.0629 1 0.0138 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can reject \\(H_0\\) of homogeneous NFC slopes across MOOD groups 2.3.3.3 Compare Adjusted Means Suppose we had failed to reject the \\(H_0\\) of homogeneity of slopes, and we wanted a single comparison of (adjusted) group means, controlling for NFC. MOOD is effect coded, so its slope (which is identical using OLS regression or SEM) is interpreted as the difference between a groups mean and the grand mean half the group mean difference The \\(t\\) test (OLS) or Wald \\(z\\) test (SEM) is sufficient to test the \\(H_0: \\widehat{y}_\\text{Treatment}|x = \\widehat{y}_\\text{Control}|x\\) summary(mle.hom) ## lavaan 0.6-11 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 4 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 6.063 ## Degrees of freedom 1 ## P-value (Chi-square) 0.014 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## POS ~ ## MOOD 4.326 0.704 6.145 0.000 ## NFC 0.474 0.504 0.941 0.346 ## MOOD:NFC 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 0.000 0.704 0.000 1.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 49.546 7.007 7.071 0.000 But the MOOD slope could also be fixed to zero (or dropped) to obtain a LRT from SEM (or \\(F\\) test from ANOVA). 2.3.4 Summary of Caveats SEMs are only comparable when they are fitted to the same data same observations (rows) and same variables (columns) Instead of removing predictors, fix slopes to zero Interaction terms are additional variables, added to \\(\\bar{y}\\) and \\(S\\) only compare models with same variables products of variables can be saved in the data.frame or specified using the colon (:) operator (e.g., x1:x2), not the asterisk (*) standardized solution is incorrect because it treats product terms as separate variables (e.g., if they were independently transformed to \\(z\\) scores rather than products of \\(z\\) scores) 2.4 Multigroup SEM 2.4.1 Group-Specific Intercepts in OLS Recall that omitting the intercepts allows each group to have a dummy code in the model slope0 &lt;- lm(POS ~ -1 + mood.f + NFC, data = dat) summary(slope0) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.1713 -5.4855 -0.2585 4.5490 16.9279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.3263 1.0109 -4.280 4.39e-05 *** ## mood.fpositive 4.3263 1.0109 4.280 4.39e-05 *** ## NFC 0.4743 0.5115 0.927 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 97 degrees of freedom ## Multiple R-squared: 0.2774, Adjusted R-squared: 0.2551 ## F-statistic: 12.42 on 3 and 97 DF, p-value: 6.115e-07 This still assumes homoskedasticity of residuals, and comparing intercepts (or adjusted means) requires the , which is a way to estimate the SE of a function of parameters (e.g., difference between 2 slopes) from the SEs of the original parameter estimates. car::deltaMethod(slope0, &quot;b2 - b1&quot;, rhs = 0, parameterNames = paste0(&quot;b&quot;, 1:3)) ## Estimate SE 2.5 % 97.5 % Hypothesis z value Pr(&gt;|z|) ## b2 - b1 8.6527 1.4298 5.8504 11.4549 0.0000 6.0519 1.432e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.4.2 Group-Specific Intercepts in SEM? lavaan automatically uses the delta method when parameters are labeled in the model syntax (before * operator) model syntax includes user-defined parameters (:= operator) However, SEM cannot fit group-specific intercepts because they are linearly dependent (recall (multi)collinearity among predictors from a previously read regression text). That is, the positive-mood dummy code is simply 1 minus the neutral-mood dummy code, so they are perfectly (negatively) correlated. Therefore, \\(S\\) would not be , which is a term that indicates some redundancy among variables. This means that fitting a single-group SEM with group-specific intercepts is not possible: dat$treat &lt;- dat$mood.f == &quot;positive&quot; dat$control &lt;- 1 - dat$treat # linear dependency ## label slopes &quot;b1&quot; for neutral group, &quot;b2&quot; for positive group mod1 &lt;- &#39; POS ~ b1*mood.fneutral + b2*mood.fpositive + NFC ## user-defined parameter adj_mean_diff := b2 - b1 &#39; fit1 &lt;- sem(mod1, data = dat) # lavaan ERROR: ## sample covariance matrix is not positive-definite 2.4.3 Multiple Groups in SEM Rather than including MOOD as variable(s) in the model, we can fit the remainder of the model (POS ~ 1 + NFC) separately in each group each group gets unique parameter estimates constraints across groups can be added homoskedasticity of residuals homogeneous slopes MOODs effect is how group-specific intercepts differ each group gets unique parameter labels, in a vector below: POS ~ c(b1, b2)*1 use the same label for homogeneous slopes below: POS ~ c(b3, b3)*NFC mod2 &lt;- &#39; POS ~ c(b1, b2)*1 + c(b3, b3)*NFC ## user-defined parameter adj_M_diff := b2 - b1 &#39; fit2 &lt;- sem(mod2, data = dat, group = &quot;mood.f&quot;) summary(fit2) ## lavaan 0.6-11 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## Number of equality constraints 1 ## ## Number of observations per group: ## neutral 50 ## positive 50 ## ## Model Test User Model: ## ## Test statistic 6.037 ## Degrees of freedom 1 ## P-value (Chi-square) 0.014 ## Test statistic for each group: ## neutral 2.217 ## positive 3.820 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [neutral]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## POS ~ ## NFC (b3) 0.443 0.502 0.882 0.378 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS (b1) -4.325 0.982 -4.404 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 48.225 9.645 5.000 0.000 ## ## ## Group 2 [positive]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## POS ~ ## NFC (b3) 0.443 0.502 0.882 0.378 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS (b2) 4.325 1.009 4.288 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 50.870 10.174 5.000 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## adj_M_diff 8.651 1.408 6.143 0.000 ## compare to OLS, which assumes homoskedasticity summary(slope0) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.1713 -5.4855 -0.2585 4.5490 16.9279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.3263 1.0109 -4.280 4.39e-05 *** ## mood.fpositive 4.3263 1.0109 4.280 4.39e-05 *** ## NFC 0.4743 0.5115 0.927 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 97 degrees of freedom ## Multiple R-squared: 0.2774, Adjusted R-squared: 0.2551 ## F-statistic: 12.42 on 3 and 97 DF, p-value: 6.115e-07 ## delta method to test adj_M_diff car::deltaMethod(slope0, &quot;b2 - b1&quot;, rhs = 0, parameterNames = paste0(&quot;b&quot;, 1:3)) ## Estimate SE 2.5 % 97.5 % Hypothesis z value Pr(&gt;|z|) ## b2 - b1 8.6527 1.4298 5.8504 11.4549 0.0000 6.0519 1.432e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the point and SE estimates from this delta-method result (using OLS) can differ from the corresponding results using SEM. Because SEM does not assume residual homoskedasticity, it may be more robust, but there is a trade-off because OLS is not biased by small samples the way that SEM is. 2.4.3.1 Summary Statistics for Multigroup SEM lavaan calculates summary statistics from the raw data= separately in each group, but lists can be passed directly gN # vector of sample sizes ## ## neutral positive ## 50 50 gM # mean vectors ## $neutral ## ATT NFC POS ## -4.7920520 0.0321142 -4.3110960 ## ## $positive ## ATT NFC POS ## 8.7534980 -0.0321146 4.3111040 gS # list of covariance matrices ## $neutral ## ATT NFC POS ## ATT 245.020884 4.812796 61.799307 ## NFC 4.812796 2.456274 -1.201971 ## POS 61.799307 -1.201971 47.663230 ## ## $positive ## ATT NFC POS ## ATT 231.2417228 0.8817017 56.235120 ## NFC 0.8817017 1.5276643 3.091531 ## POS 56.2351197 3.0915313 54.346483 fit2 &lt;- sem(mod2, sample.nobs = gN, # &quot;group=&quot; is implied by lists sample.mean = gM, sample.cov = gS) 2.4.3.2 Advantages of Multigroup SEM Less restrictive assumptions parameters differ by default More interpretable parameters each group has their own intercept and slopes Intuitive to specify \\(H_0\\) as user-defined parameter e.g., the difference between the intercepts in the treatment group (b2) and control group (b1) is zero Intuitive to represent assumptions about equality constraints by using the same labels across groups e.g., the same NFC effect (b3) testable by comparing models with(out) constraints Can easily test \\(H_0\\) about parameters other than means Are variances or correlations equal across groups? Disadvantages: asymptotic assumption (large \\(N\\)) applies to each group single-group regression better in small samples "],["path-analysis-with-categorical-outcomes.html", "3 Path Analysis with Categorical Outcomes 3.1 Prepare Data and Workspace 3.2 Regression Models for Binary Variables 3.3 Latent Response Variable 3.4 Mediation Model with Categorical Outcomes 3.5 References", " 3 Path Analysis with Categorical Outcomes Structural equation modeling is a technique designed for continuous variables. In practice, variables are often not continuous but categorical, such as variables scored on discrete Likert scales (i.e., ordinal data) or correct/incorrect responses on test items (i.e., binary data). If endogenous variables in a path model (or in any SEM) are categorical, the SEM-method to deal with the categorical variables is to assume that there exists a latent (unobserved) continuous variable that underlies the observed categorical variable. The underlying variable is called a latent response variable (LRV). Categorical SEM fits the model to the LRVs instead of to the categorical variables. The observed categorical scores can be linked to the LRV using so-called thresholds that represent the value on the LRV beyond which individuals who score higher would get an observed categorical score in the higher category. For example, for a test item that asks How many degrees of freedom does this path model have? the students could give the incorrect (scored 0) or the correct (scored 1) answer. The LRV here represents the ability (LRV) to calculate \\(df\\) for this path model on a continuous scale. Students whose ability is high enough would give the correct response; otherwise, the response would be incorrect. The threshold would reflect the minimum required ability to indicate the correct response. Fitting path models to categorical data thus involves linking the LRV to the observed categorical variables. In this chapter we first import example data, then we show how one can use probit and logit link functions in a generalized linear model (GLM). Next, we discuss the concept of the LRV for one ordinal variable, for multiple ordinal variables, and then we illustrate fitting a path model to ordinal variables. 3.1 Prepare Data and Workspace library(lavaan) 3.1.1 Import Example Data We will use example data provided on the for a technical report (Muthén, 2011, p. 24). The example data can be downloaded onto your computer from the reports , or you can provide the URL directly to read.table(file=), as shown below. Note that the data are in summary format (i.e., a table with frequencies for each combination of categories for the 3 variables). The read.table() output is therefore transformed into a standard data.frame with 1 row per subject. dd &lt;- read.table(file = &quot;http://www.statmodel.com/examples/shortform/4cat%20m.dat&quot;, col.names = c(&quot;intention&quot;, &quot;intervention&quot;, &quot;ciguse&quot;, &quot;w&quot;)) dd$intention &lt;- dd$intention - 1L # make the lowest category 0, not 1 ## transform frequency table to casewise data myData &lt;- do.call(rbind, lapply(1:nrow(dd), function(RR) { data.frame(rep(1, dd$w[RR]) %*% as.matrix(dd[RR, 1:3])) })) Predictor: intervention (1 = treatment, 0 = control) Mediator: intention to smoke in the following 2 months (measured 6 months after intervention) 0 = No, 1 = I dont think so, 2 = Probably, 3 = Yes Outcome: ciguse in previous month (1 = smoked, 0 = not), measured a 1-year follow-up 3.1.2 Summarize Data The resulting data.frame looks like this: ## intention intervention ciguse ## 790 0 0 0 ## 653 0 0 0 ## 167 1 1 0 ## 63 2 1 1 ## 626 0 0 1 The marginal frequencies for each variable are: ## table for each variable lapply(myData, table) ## $intention ## ## 0 1 2 3 ## 644 103 60 57 ## ## $intervention ## ## 0 1 ## 371 493 ## ## $ciguse ## ## 0 1 ## 708 156 Lets begin with a frequency table to estimate the proportion of subjects who smoke cigarettes (i.e., the estimated probability of smoking: \\(\\widehat{\\pi}\\)) in each combination of categories for the predictors. tab3way &lt;- table(myData) probs &lt;- prop.table(tab3way, 1:2)[,,2] addmargins(probs, FUN = mean) ## Margins computed over dimensions ## in the following order: ## 1: intention ## 2: intervention ## intervention ## intention 0 1 mean ## 0 0.11583012 0.08311688 0.09947350 ## 1 0.26530612 0.20370370 0.23450491 ## 2 0.58823529 0.42307692 0.50565611 ## 3 0.68965517 0.67857143 0.68411330 ## mean 0.41475668 0.34711723 0.38093696 The treatment group (intervention == 1) smokes less, both on average and within each intention group. Smoking also increases with intent to smoke (both on average and within treatment/control groups). Treatment appears least effective among those who were certain they would continue smoking (row 4). 3.2 Regression Models for Binary Variables Suppose we used a linear model to predict the probability of smoking, treating intention as a continuous predictor to estimate only its linear effect. mod.lin &lt;- lm(ciguse ~ intervention + intention, data = myData) coef(mod.lin) ## (Intercept) intervention intention ## 0.1177588 -0.0439528 0.1927033 The estimated intercept indicates the predicted probability of smoking when predictors are zero (i.e., those in the control group with no intent to smoke). The estimated slopes reflect what we saw in the table: treatment reduces the probability of smoking (given the intent to smoke), and intent to smoke is associated with more smoking (given treatment). Note: No interaction is included here, but you can test for yourself that it was not significant. It would be problematic if the model predicted any probabilities outside the natural 01 range. We do not observed that problem in this specific sample: summary(fitted(mod.lin)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.07381 0.07381 0.11776 0.18056 0.26651 0.69587 This limitation of linear probability models is resolved with a link function in a GLM that allows for predicted values along the entire real-number line. Two such transformations are in frequent use: The probit (probability unit) transformation dates back to Bliss (1934). A probability can be transformed to a corresponding quantile in a cumulative distribution function (CDF), such as the standard-normal distribution. In other words, any probability p between 01 has a corresponding z score, such as can be requested from qnorm(p). Likewise, any predicted value on the probit scale can be transformed back into a probability using pnorm(probit). The GLM takes the form below, where \\(\\Phi()\\) is the standard-normal CDF, \\(\\Phi^{-1}()\\) is its inverse, and \\(\\mathbf{XB}\\) is the linear predictor (\\(\\widehat{y}\\)): \\[\\text{Probit}(y=1) = \\Phi(\\pi) = z = \\mathbf{XB}\\] \\[\\text{Pr}(y=1) = \\pi = \\Phi^{-1}(z)\\] The logit (logistic unit) transformation is often favored because its coefficients can be interpreted on the (natural-)log-odds scale. Recall the odds of an outcome is a ratio of its probability (\\(\\pi\\)) to its complement (\\(1-\\pi\\)), so its range is 0 to \\(+\\infty\\) (i.e., it is merely bound to nonnegative numbers). The natural (i.e., base-\\(e\\)) log of a nonnegative number can take any real number. The formulas below show how probabilities, odds, and logits are related. \\[\\text{Logit}(\\pi) = \\ln(\\text{odds}) = \\ln \\Bigg(\\frac{\\pi}{1 - \\pi} \\Bigg) = \\mathbf{XB}\\] \\[\\text{odds} = \\frac{\\pi}{1 - \\pi} = e^{\\text{Logit}(\\pi)} \\text{ , where } \\; \\; e = 2.7182818\\] \\[\\pi = \\frac{\\text{odds}}{1 + \\text{odds}} = \\frac{e^\\text{Logit}}{1 + e^\\text{Logit}}\\] There is a logit() function in the psych package, but it is simple enough to calculate the transformation (and its inverse): logit &lt;- log( p / (1-p) ) # odds &lt;- p / (1-p) p &lt;- exp(logit) / (1 + exp(logit)) # odds &lt;- exp(logit) Both probit and logistic regression models can be estimated using the glm() function, demonstrated below. 3.2.1 Fit a Logistic Regression Model In order to compare the results of fitting the logistic and the probit regression models, we first fit the GLM with the logit link. We continue to treat the ordinal mediator intention as continuous for now. mod.logit &lt;- glm(ciguse ~ intervention + intention, data = myData, family = binomial(&quot;logit&quot;)) coef(mod.logit) ## (Intercept) intervention intention ## -2.0177349 -0.3771387 1.0379733 3.2.2 Fit a Probit Regression Model mod.probit &lt;- glm(ciguse ~ intervention + intention, data = myData, family = binomial(&quot;probit&quot;)) coef(mod.probit) ## (Intercept) intervention intention ## -1.1894765 -0.2030216 0.6081512 3.2.3 Compare Models The GLMs make very similar predictions. We can compare all 3 models (linear, logit and probit) predictions to the observed probabilities in each group. ## intention intervention p.obs p.linear p.logit p.probit ## 1 3 1 0.67857143 0.65191604 0.67239696 0.66711311 ## 3 3 0 0.68965517 0.69586883 0.74954459 0.73727830 ## 5 2 1 0.42307692 0.45921270 0.42093726 0.43007009 ## 7 2 0 0.58823529 0.50316549 0.51454880 0.51070069 ## 9 1 1 0.20370370 0.26650936 0.20474455 0.21641830 ## 11 1 0 0.26530612 0.31046215 0.27293908 0.28051061 ## 13 0 1 0.08311688 0.07380601 0.08356445 0.08188581 ## 15 0 0 0.11583012 0.11775881 0.11735341 0.11712611 Notice how similar the predicted probabilities are across models, and how close they are to the observed proportions in each group. Each of these models estimates the same number of parameters, but their functional form differs (identity, logit, and probit link functions, with different models for error). So we can only compare the fit of each model to the data descriptively. Here are two examples: (1) sum the squared differences between each models predicted probabilities and the observed probabilities, summarized by Pearsons \\(\\chi^2\\) statistic (lower fits better): ## p.linear p.logit p.probit ## 0.04094642 0.01564608 0.01676192 Or compare their Tjurs pseudo-\\(R^2\\) values (higher fits better): ## linear logit probit ## 0.2042811 0.2074272 0.2063500 In both cases, the logit model performs slightly better than probit, both of which perform better than the linear model. But the discrepancies are small. 3.3 Latent Response Variable In SEM, the probit model is often preferred because predicted values are z scores that can be transformed into probabilities. This lends itself easily to a latent-response interpretation, which can be stated as follows for the cigarette-use example. Each subject has a latent propensity to smoke. This propensity is a continuous, normally distributed trait. Subjects whose propensity exceeds a threshold succumb to smoking, whereas those whose propensity does not exceed the threshold refrain from smoking. The probit model above can therefore be reframed in terms of this LRV, \\(y^*\\). \\[y^* = \\mathbf{XB} + \\varepsilon ,\\] where \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma)\\) and the observed binary response \\(y\\) is linked to the LRV by a threshold model: \\[y = I(y^* &gt; \\tau),\\] where the indicator function \\(I()\\) assigns 1 when its argument is TRUE and 0 when it is FALSE. Because the LRV is unobserved (i.e., latent), its distributional parameters cannot be estimated from the data. GLM software like the glm() function typically identify the model by fixing the residual variance \\(\\sigma=1\\) and the threshold \\(\\tau=0\\). For simplicity, consider an intercept-only model for cigarette use. mod0 &lt;- glm(ciguse ~ 1, data = myData, family = binomial(&quot;probit&quot;)) coef(mod0) ## (Intercept) ## -0.9132499 In lavaan the default is instead to fix the intercept to 0 and estimate the threshold. The model syntax below shows how thresholds are specified with the pipe (vertical bar: |) followed by t1 indicating the first threshold. mod0.ciguse &lt;- &#39;ciguse | t1&#39; # specify first (and only) threshold fit0.ciguse &lt;- sem(mod0.ciguse, data = myData) summary(fit0.ciguse, header = FALSE, nd = 7) ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse 0.0000000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse|t1 0.9132499 0.0498031 18.3372246 0.0000000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse 1.0000000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse 1.0000000 Binary variables have only 1 threshold that splits a normal distribution into 2 categories. Notice that it is arbitrary whether the intercept (mean) or threshold is fixed to 0. In the output of glm() the threshold was fixed at zero and intercept was estimated to be \\(-0.913\\). In the output of lavaan the intercept is fixed at zero and the threshold was estimated to be \\(+0.913\\). In either case, the threshold is the same distance from the LRVs mean, as depicted in the figure below. Notice that the distributions are identical. Only the \\(x\\)-axis differs, reflecting the change in (arbitrary) identification constraint. 3.3.1 Ordinal Outcomes The LRV interpretation is easily extended to polytomous ordinal variables, such as intent to smoke. This is called the cumulative probit model (and link function). Any ordinal variable with categories \\(c=0, \\ldots, C\\) can be interpreted as a crude discretization of an underlying LRV, whose \\(C\\) thresholds divide the latent distribution into \\(C+1\\) categories. \\[y=c \\ \\ \\ \\text{ if } \\ \\ \\ \\tau_c &lt; y^* \\le \\tau_{c+1} \\] Because the normal distribution is unbounded, the first category starts at \\(\\tau_0 = -\\infty\\), and the last category ends at \\(\\tau_{C+1} = +\\infty\\). For example, intent to smoke has 4 categories, so there are \\(C=3\\) thresholds. We do not have to specify thresholds in lavaan model syntax if we tell lavaan which variables are ordinal using the argument ordered=. When all modeled variables are ordinal, one can use TRUE as a shortcut. fit2 &lt;- sem(&#39;ciguse ~~ intention&#39;, data = myData, ordered = TRUE) summary(fit2, header = FALSE, standardized = TRUE) ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse ~~ ## intention 0.637 0.041 15.496 0.000 0.637 0.637 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 0.000 0.000 0.000 ## intention 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse|t1 0.913 0.050 18.337 0.000 0.913 0.913 ## intention|t1 0.660 0.046 14.280 0.000 0.660 0.660 ## intention|t2 1.101 0.054 20.570 0.000 1.101 1.101 ## intention|t3 1.506 0.066 22.867 0.000 1.506 1.506 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 1.000 1.000 1.000 ## intention 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 1.000 1.000 1.000 ## intention 1.000 1.000 1.000 Notice that intent has 3 thresholds, labeled with sequential integers following the letter t. This is how they would be specified in lavaan model syntax, e.g., &#39; intention | NA*t1 + NA*t2 + NA*t3 &#39; Alternatively, set the argument auto.th=TRUE to automatically estimate all thresholds. The LRV for intention can be interpreted as the degree to which someone intends to smokesimilar to the observed variable, but the degree increases continuously rather than in discrete increments. Subjects in Category 0 do not have a latent intent to smoke that exceeds the first threshold (\\(\\hat\\tau_1\\)), so they do not indicate that they intend to smoke. Subjects will only respond that they dont know when their latent intent exceeds \\(\\hat\\tau_1=\\) 0.66 \\(SD\\)s above the mean of that distribution. Subjects must exceed \\(\\hat\\tau_2=\\) 1.101 \\(SD\\) above the mean before they begin indicating probably. Only those whose latent intent exceeds \\(\\hat\\tau_3=\\) 1.506 \\(SD\\)s above the mean do they respond firmly yes. The plot below visualizes the classification rules above. 3.3.2 Estimating Thresholds Thresholds are univariate statistics (like means). Threshold estimates are based on the frequency distribution of a categorical variable. Recall that the probit function merely transforms a (predicted) probability (\\(\\pi\\)) into a corresponding z score. The (standardized) thresholds are these z scores. We can estimate them by plugging into qnorm() the cumulative proportions (\\(\\widehat\\pi_c\\)) up to each category \\(c\\). For example, here are the (cumulative) proportions in each category of intention. p.intent &lt;- prop.table(table(myData$intention)) # proportions per category cumsum(p.intent) # cumulative proportions (up to and including each category) ## 0 1 2 3 ## 0.7453704 0.8645833 0.9340278 1.0000000 By definition, 100% of the sample was observed in any of the categories up to (and including) the highest category, so the final cumulative proportion is 1. Likewise, no one in the sample was observed in a category lower than the lowest category (by definition), so we can append a zero as the first number in this vector of cumulative proportions. Then we can find the corresponding z scores in a normal distribution, which are the thresholds between categories: qnorm(c(0, cumsum(p.intent))) ## 0 1 2 3 ## -Inf 0.6599915 1.1011455 1.5064783 Inf The upper and lower thresholds are \\(+/-\\infty\\) by definition, corresponding to 0 and 100%. Those never need to be estimated because they are fixed by design. The remaining 3 thresholds are actual borders between observed categories, showing how far above/below the mean a subject must be in order to indicate a particular response category. 3.3.3 Estimating Polychoric Correlations With multiple ordinal variables, we can estimate the correlations between their associated LRVs by relying on the assumption that the LRVs are normally distributed (Olsson, 1979, 1982). These correlations are called polychoric correlations (Olsson, 1979), which are bivariate statistics (like the covariance matrix). If the model also includes a continuous variable, its correlation with an ordinal variables LRV is called the polyserial correlation (Olsson, 1982). Special names for 2-category variables are tetrachoric correlation (between 2 binary variables) or biserial correlation (between a binary and continuous variable). For example, the estimated covariance between cigarette use and intent to smoke in the model above is a polychoric correlation because each LRVs \\(SD\\) was fixed to 1 in that unrestricted (saturated) model. The reason that the probit model is so popular in SEM is that the standard SEM matrices and interpretations still apply to normal data, except those normal variables are latent. All we need to do is append the SEM with a threshold model in order to link these normal LRVs to their observed discrete counterparts. The default saturated model in lavaan estimates all thresholds and polychoric (and polyserial, if applicable) correlations, along with means and (co)variances among any continuous variables. This is used as input data when fitting any hypothesized model (even when fitting a regression model with \\(df=0\\)). We can see the estimated sample statistics for the model above: lavInspect(fit2, &quot;sampstat&quot;) ## $cov ## ciguse intntn ## ciguse 1.000 ## intention 0.637 1.000 ## ## $mean ## ciguse intention ## 0 0 ## ## $th ## ciguse|t1 intention|t1 intention|t2 intention|t3 ## 0.913 0.660 1.101 1.506 Notice that the means are 0 and variances are 1, consistent with the correlation metric and the assumption that the underlying LRV is a z score. These values are actually fixed to identify the saturated model. We will discuss scaling constraints and identification of latent variables more when we introduce the common-factor model. 3.3.4 Estimating an SEM with Estimated Input The problem with treating thresholds and polychoric correlations as observed data is that they were not observed. They are estimates of population parameters, based on the data. They each have an associated SE and 95% CI. In order to trust the SEs, 95% CIs, and test statistics in our hypothesized model, we need to take the uncertainty of the data into account. This is often accomplished using weighted least-squares estimation, where the weight matrix \\(\\mathbf{W}\\) is the sampling covariance matrix of the estimated thresholds and polychoric correlations. Note: Do not confuse the sampling covariance matrix of estimated parameters with the covariance matrix of variables. The latter is used as input data, and it is what we want our SEM to explain. The sampling covariance of parameter estimates is how we quantify their uncertainty: its diagonal contains sampling variances, the square-roots of which are the \\(SE\\)s that we use to calculate Wald z tests and CIs. The more variables (and categories) there are, the larger \\(\\mathbf{W}\\) becomes: there is one row/column for every estimated threshold and correlation. Even in samples as large as 1000, this makes estimation unstable. An alternative is to simply ignore the sampling covariances during estimation, so \\(\\text{diag}(\\mathbf{W})\\) is the weight matrix to obtain point estimates of parameters. This is called diagonally weighted least squares (estimator = \"DWLS\"), which is the default in lavaan. Even with DWLS, the full weight matrix is still needed to calculate SEs and the \\(\\chi^2\\) statistic. The \\(\\chi^2\\) statistic needs to be robust against the uncertainty of the input data, so it is adjusted by scaling it (a mean-adjusted statistic) and optionally shifting it (a mean- and variance-adjusted statistic). The default is both: estimator = \"WLSMV\" is a shortcut that implies lavaan(..., estimator = \"DWLS\", se = \"robust.sem\", test = \"scaled.shifted\") Another alternative is unweighted/ordinary least squares (ULS), where \\(\\mathbf{W}\\) is an identity matrix. This often yields good point estimates, but Type I error rates can differ from the \\(\\alpha\\) level, so it is still recommended to request a scaled/shifted \\(\\chi^2\\) statistic (e.g., estimator = \"ULSMV\"). There are also likelihood-based estimators. One option is marginal maximum likelihood (estimator = \"MML\"), which is currently disabled in lavaan because it was too computationally intensive and the developer is exploring better routines. A less computationally intensive alternative is pairwise maximum likelihood (estimator = \"PML\"), which maximizes only the (sum of) uni- and bivariate (log-)likelihoods. This is a quite robust procedure even in small samples, and because it is likelihood-based, information criteria (AIC and BIC) are available. Note: The robust statistics are only corrected for the two-stage estimation procedure (thresholds+pollychorics, followed by SEM parameters using DWLS or ULS) or for combining pairwise log-likelihoods (using PML). The LRV interpretation for categorical outcomes assumes a normal distribution for latent responses. Because LRVs are (by definition) unobserved, there is no way to estimate their degree of nonnormality, which would be necessary to correct for it. 3.3.4.1 Counting Degrees of Freedom For SEM with only continuous variables, the number of observed summary statistics \\(p^*\\) is a simply function of the number of variables \\(p\\): \\[p^* = \\frac{p(p+3)}{2} \\text{ (with mean structure) or } \\frac{p(p+1)}{2} \\text{ (without means)}\\] \\(p\\) means \\(p\\) variances \\(\\frac{p(p-1)}{2}\\) covariances The number of observed covariances does not differ between continuous and categorical variables, but some covariances are replaced by: polychoric correlations among LRVs (scaled) polyserial correlations between LRVs and observed variables But the number of univariate statistics can differ. Whereas continuous variables each contribute an observed mean and variance, categorical variables do not contribute either (i.e., those values are fixed to 0 and 1, respectively, to estimate thresholds and polychoric correlations). Instead, each categorical variable contributes \\(C\\) observed standardized thresholds for its \\(C+1\\) categories. So each binary variable contributes \\(C=1\\) threshold, each ternary (3-category) variable contributes \\(C=2\\) thresholds, and so on. Thus, the number summary statistics is a function of the number of observed continuous and discrete variables. Suppose we denote: \\(p_c\\) the number of continuous variables \\(p_d\\) the number of discrete variables so the total number of observed variables remains \\(p = p_c + p_d\\) the number of thresholds for discrete variables \\(d = 1, \\dots, D\\) is \\(C_d\\) the total number of thresholds is \\(K = \\sum_{d=1}^D C_d\\) Then the number summary statistics is \\(\\frac{p(p-1)}{2} + 2p_c + K\\). That is, both continuous and discrete variables contribute covariances/correlations (first term), only continuous variables contribute a mean and variance (second term), and only discrete variables contribute thresholds (third term). This has implications for determining the expected \\(df\\), which is an important part of verifying that you fit the model you intended to fit. Just as mean structures are typically saturated (e.g., for every observed mean, an intercept is estimated), each observed standardized threshold has a corresponding estimated threshold (not necessarily standardized, but is by default in lavaan). Likewise, intercepts and (residual or marginal) variances typically remain fixed for LRVs in hypothesized models, as they do in the default saturated model. Thresholds might be constrained (contributing \\(df\\)) in models for multiple groups or occasionsas might LRV intercepts or (residual or marginal) varianceswhich we discuss in later chapters. 3.3.4.2 Missing Data When multivariate-normal data are incomplete, all available information can be used to estimate parameters by setting missing = \"FIML\" (full-information ML estimation) to override the default listwise deletion method. Actually, FIML applies to nonnormal continuous data as well, since a robust correction for nonnormality is available with FIML results (estimator = \"MLR\"). FIML is also available for categorical outcome(s), but only when using (marginal) ML estimation. Until estimator = \"MML\" becomes available in lavaan, the FIML option is unfortunately unavailable. When at least 1 endogenous variable is binary/ordinal, missing = \"pairwise\" deletion retains as much information as possible, so it is still preferred over the default missing = \"listwise\" deletion. However, both deletion methods make the restrictive assumption that data are missing completely at random (MCAR). The reason pairwise deletion assumes MCAR data is that for each pair of variables, it estimates their covariance (or polychoric/polyserial correlation) using all jointly observed complete data for that pair of variables. So no information from other observed variables can be used to estimate the covariance/correlation. In contrast, FIML estimates parameters using each persons entire vector of observed data, so missing data on Variable A can be compensated by observed data on Variable B. Thus, FIML makes the less restrictive assumption that data are missing at random (MAR)that is, the missingness is still random conditional on the observed data (see Little et al., 2014, for more detailed conceptual explanations). Pairwise deletion is available with any (diagonally or un)weighted least-squares estimators, as well as with (P)ML. But with PML for categorical outcomes, a missing = \"doubly.robust\" estimation method is available that only requires assuming data are MAR (Katsikatsou et al., in press). It is computationally intensive, but it may be worth it if the MAR assumption is not feasible. 3.4 Mediation Model with Categorical Outcomes Consider the mediation model depicted below. The dashed line is the direct effect of treatment on smoking behavior, given the intent to smoke. Because SEM treats an exogenous binary predictor as numeric (like dummy-coded predictors in standard OLS regression models), these \\(p=3\\) variables include \\(p_c=1\\) continuous predictor (intervention) and \\(p_d=2\\) discrete outcomes. These contribute \\(p^* = \\frac{p(p-1)}{2} = 3\\) observed covariances: The polyserial correlations of intervention with ciguse and intention The polychoric correlation between ciguse and intention The predictor also contributes \\(2p_c=2\\) observed values: a mean and a variance. The endogenous binary variable ciguse contributes 1 threshold, and the endogenous ordinal variable intention contributes 3 thresholds, so \\(K=4\\). Thus, there are 3 + 2 + 4 = 9 summary statistics in this 3-variable system. There are 3 possible conclusions: No mediation: the indirect effect \\(\\beta_{21} \\times \\beta_{32}=0\\) because either \\(\\beta_{21}=0\\) or \\(\\beta_{32}=0\\) Partial mediation: the indirect effect \\(\\beta_{21} \\times \\beta_{32} \\ne 0\\) and the direct effect \\(\\beta_{31} \\ne 0\\) Full mediation: the indirect effect \\(\\beta_{21} \\times \\beta_{32} \\ne 0\\) but the direct effect \\(\\beta_{31} = 0\\) Omitting the dashed line (by fixing \\(\\beta_{31}=0\\)$) would imply full mediation. A full-mediation model would estimate 8 parameters: 2 regression slopes (\\(\\beta_{21}\\) and \\(\\beta_{32}\\)) 1 mean and 1 variance for the exogenous dummy coded intervention 3 thresholds for the mediator intention 1 threshold for the outcome ciguse Thus, the full-mediation model will have \\(df=1\\). The partial-mediation model would estimate the dashed line, reducing \\(df=0\\). No mediation could be represented by a model that estimates the dashed line but fixes either \\(\\beta_{21}=0\\) (a multiple-regression model for ciguse) or \\(\\beta_{32}=0\\) (a multivariate regression model for intention and ciguse). We will only fit the full and partial mediation models in the next section. 3.4.1 Estimate Mediation Models Begin by fitting the partial-mediation model to the data, to test \\(H_0: \\beta_{21} \\times \\beta_{32}=0\\) (no mediation). We will only specify the regression slopes and user-defined parameters because the residual variances are not estimated. The identification constraints require more explanation (see the following section). mod.part &lt;- &#39; ## regression paths intention ~ b21*intervention ciguse ~ b31*intervention + b32*intention ## define indirect and total effects ind := b21*b32 tot := ind + b31 &#39; fit.part &lt;- sem(mod.part, data = myData, ordered = TRUE) summary(fit.part, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-11 ended normally after 13 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 864 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.000 0.000 ## Degrees of freedom 0 0 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## intention ~ ## intrvntn (b21) -0.246 0.089 -2.758 0.006 -0.246 -0.121 ## ciguse ~ ## intrvntn (b31) -0.130 0.093 -1.402 0.161 -0.130 -0.064 ## intentin (b32) 0.631 0.042 15.105 0.000 0.631 0.629 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .intention 0.000 0.000 0.000 ## .ciguse 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## intention|t1 0.525 0.067 7.844 0.000 0.525 0.521 ## intention|t2 0.970 0.071 13.572 0.000 0.970 0.963 ## intention|t3 1.378 0.082 16.710 0.000 1.378 1.368 ## ciguse|t1 0.760 0.072 10.491 0.000 0.760 0.752 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .intention 1.000 1.000 0.985 ## .ciguse 0.602 0.602 0.591 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## intention 1.000 1.000 1.000 ## ciguse 1.000 1.000 1.000 ## ## R-Square: ## Estimate ## intention 0.015 ## ciguse 0.409 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ind -0.155 0.057 -2.713 0.007 -0.155 -0.076 ## tot -0.285 0.100 -2.845 0.004 -0.285 -0.140 Using the standard criterion \\(\\alpha=5\\%\\), the Wald z tests allow us to reject the \\(H_0\\) of no mediation: indirect effect b = \\(-0.155\\), \\(z = -2.713\\), p = 0.007, \\(\\beta = -0.076\\). We fail to reject the \\(H_0\\) of full mediation: direct effect b = \\(-0.13\\), \\(z = -1.402\\), p = 0.161, \\(\\beta = -0.064\\). Because the scales of the LRVs are arbitrary, it is safer to interpret the standardized slopes test a \\(H_0\\) using a \\(\\Delta \\chi^2\\) test We can fit a full-mediation model to the data to obtain the LRT. mod.full &lt;- &#39; ## regression paths intention ~ b21*intervention ciguse ~ b32*intention ## define indirect and total effects ind := b21*b32 &#39; fit.full &lt;- sem(mod.full, data = myData, ordered = TRUE) lavTestLRT(fit.full, fit.part) # or anova(), which then calls lavTestLRT() ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.part 0 0.0000 ## fit.full 1 1.2648 1.9567 1 0.1619 Notice that lavTestLRT() automatically detects the robust estimator and test statistic requested when fitting the models, so it appropriately uses the same robust correction (mean- and variance-adjustment) for the \\(\\Delta \\chi^2\\) statistic. This is why the Chisq Diff column does not match the difference between (uncorrected) \\(\\chi^2\\) values in the Chisq column. The p value is very close to the Wald tests p value (also calculated using a robust SE), which is expected when N is large. 3.4.2 Interpreting Coefficients Notice that the glm() estimates from the probit regression for ciguse do not match those from the sem() function. There are noteworthy differences in parameterization: The GLM approach fixes the threshold to 0 and estimates the intercept. The SEM approach fixes the intercept to 0 and estimates the threshold. As shown in the figure above, this distinction is arbitrary because the LRV has no location that we can identify from the data. The intercept and threshold can only be said to be a certain distance apart, which is why they take the same absolute value. For example, we can free the ciguse intercept in lavaan syntax and fix its threshold instead: ## fixed intercept coef(fit.part, type = &quot;all&quot;)[c(&quot;ciguse|t1&quot;,&quot;ciguse~1&quot;)] ## ciguse|t1 ciguse~1 ## 0.7596909 0.0000000 ## fixed threshold mod.trade &lt;- &#39; ciguse | 0*t1 # fix threshold ciguse ~ NA*1 # estimate intercept &#39; coef(sem(c(mod.part, mod.trade), data = myData, ordered = TRUE), type = &quot;all&quot;)[c(&quot;ciguse|t1&quot;,&quot;ciguse~1&quot;)] ## ciguse|t1 ciguse~1 ## 0.0000000 -0.7596909 The GLM approach treats all predictors as fixed, and predictors are distinct from outcomes. Intent to smoke was treated as continuous. Although we could have used polynomial contrast codes to estimate linear, quadratic, and cubic effects, we still would have to take the observed category weights (03) as given. The SEM approach allows outcomes to predict other outcomes. So the LRV underlying intent to smoke was used as a predictor of smoking propensity (LRV underlying ciguse). This allows us to extrapolate from our results what we would expect if we had measured intent as a continuous variable (and it happened to be normally distributed). A final distinction has to do with the identification constraint on the LRV scale. 3.4.3 Delta vs. Theta Parameterizations The SEM approach in lavaan (by default) fixes the marginal (i.e., total) variance of the LRV to 1. This is called the delta parameterization in the SEM literature. The GLM approach fixes the residual/conditional variance to 1. This is called the theta parameterization in the SEM literature. Under the delta parameterization, the residual variance is fixed such that it equals 1 minus the explained variance (\\(1-R^2\\)). Under the theta parameterization, the LRVs total variance is the sum of explained and unexplained variance (1 + explained variance). The total variances is not an explicit SEM parameter; instead, the scaling factors (Scales y* in the output) are the reciprocal of the total \\(SD\\) (i.e., \\(\\frac{1}{SD}\\)). Use the argument parameterization = \"theta\" to override the default delta parameterization, if there is a reason to do so (i.e., if you want to model residual variances). This will be discussed further in the chapter about item factor analysis (i.e., common-factor models for categorical indicators). This arbitrary distinction can have important consequences for the interpretation and decomposition of mediated effects. For example, the indirect effects would not be comparable between full- and partial-mediation models when fixing residual variances to 1, because doing so would change the scale of the LRV across the two models. That is, the relative amount of residual variance would be different for the full- and partial mediation models. The partial-mediation model explains less variance (lower \\(R^2\\), so its residual variance should be larger) than the full-mediation model, but residual variances are fixed to 1 in both cases, making the total variance appear smaller in the partial-mediation model. This change of scales changes the meaning of one unit increase across the models. Luckily, when using the delta parameterization in single-group models, the outcomes LRV scale (and any categorical mediators LRV scale) is already held constant by fixing the marginal variance to 1 (Breene et al., 2013). However, this good news would not apply in more complex situations that affect LRV scales, such as multigroup models. So when using path analysis with categorical outcomes, it is best to exercise caution. Use the delta parameterization, and do not use a multigroup models to compare paths across groups (i.e., moderated mediation) unless you understand how to equate the LRV scales across groups. 3.4.4 Decomposing Total Effects Furthermore, a common method of quantifying the size of an indirect effect is the proportion of the total effect: \\(\\frac{\\beta_{21} \\times \\beta_{32}}{(\\beta_{21} \\times \\beta_{32}) + \\beta_{31}}\\). This is already problematic when the direct and indirect effects have opposite signs (called inconsistent mediation). But when the scale of a LRV varies between models with(out) a mediator, then the sum of estimated direct and indirect effects would no longer match the total effect. For example, fit separate probit models for the mediator and outcome, as would be necessary using the GLM approach. For simplicity, only treat ciguse as categorical. ## simple regression to obtain the total effect (total &lt;- coef(sem(&#39;ciguse ~ total*intervention&#39;, data = myData, ordered = &#39;ciguse&#39;, parameterization = &quot;theta&quot;))[&quot;total&quot;]) ## total ## -0.2850428 ## model for mediator (b21 &lt;- coef(sem(&#39;intention ~ b21*intervention&#39;, data = myData))[&quot;b21&quot;]) ## b21 ## -0.1644697 ## model for outcome (b3 &lt;- coef(sem(&#39;ciguse ~ b31*intervention + b32*intention&#39;, data = myData, ordered = &#39;ciguse&#39;, parameterization = &quot;theta&quot;))[c(&quot;b31&quot;,&quot;b32&quot;)]) ## b31 b32 ## -0.2030216 0.6081512 ## estimate total effect from decomposition b3[[1]] + b21*b3[[2]] ## b21 ## -0.303044 This does not match the total effect from the simple probit regression because the LRV scales differ. That is, the simple regression has more residual variance (less explained variance without intention in the model), yet the residual variance is still fixed to 1. Imai et al. (2010) proposed a general solution to this problem, using the causal-modeling framework, implemented in the (see their for examples). It is quite complicated, but here is how it would apply to our current example (see also Muthén, 2011, for corresponding Mplus syntax): mod.Imai &lt;- &#39; ciguse ~ c*intervention + b*intention intention ~ a*intervention # label threshold for ciguse ciguse | b0*t1 # biased SEs naive.indirect := a*b naive.direct := c # correct probit11 := (-b0+c+b*a)/sqrt(b^2+1) probit10 := (-b0+c )/sqrt(b^2+1) probit00 := (-b0 )/sqrt(b^2+1) indirect := pnorm(probit11) - pnorm(probit10) direct := pnorm(probit10) - pnorm(probit00) &#39; fit &lt;- sem(mod.Imai, data = myData, ordered = c(&quot;ciguse&quot;,&quot;intention&quot;)) summary(fit, std = TRUE) ## lavaan 0.6-11 ended normally after 13 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 864 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.000 0.000 ## Degrees of freedom 0 0 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse ~ ## interventn (c) -0.130 0.093 -1.402 0.161 -0.130 -0.064 ## intention (b) 0.631 0.042 15.105 0.000 0.631 0.629 ## intention ~ ## interventn (a) -0.246 0.089 -2.758 0.006 -0.246 -0.121 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ciguse 0.000 0.000 0.000 ## .intention 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse|t1 (b0) 0.760 0.072 10.491 0.000 0.760 0.752 ## intntn|t1 0.525 0.067 7.844 0.000 0.525 0.521 ## intntn|t2 0.970 0.071 13.572 0.000 0.970 0.963 ## intntn|t3 1.378 0.082 16.710 0.000 1.378 1.368 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ciguse 0.602 0.602 0.591 ## .intention 1.000 1.000 0.985 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 1.000 1.000 1.000 ## intention 1.000 1.000 1.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## naive.indirect -0.155 0.057 -2.713 0.007 -0.155 -0.076 ## naive.direct -0.130 0.093 -1.402 0.161 -0.130 -0.064 ## probit11 -0.884 0.062 -14.182 0.000 -0.884 -0.755 ## probit10 -0.752 0.070 -10.714 0.000 -0.752 -0.691 ## probit00 -0.643 0.063 -10.184 0.000 -0.643 -0.637 ## indirect -0.037 0.014 -2.647 0.008 -0.037 -0.020 ## direct -0.034 0.024 -1.403 0.161 -0.034 -0.017 However, Breen et al. (2013) proposed a simpler, less restrictive solution that holds the LRV scale consistent so that the decomposition holds. Their method is meant for researchers fitting separate regression models, and performed quite well in simulations (and similar to Imai et al.s method). Luckily, when using the delta parameterization in single-group models, their method is unnecessary when simultaneously estimating the full/partial-mediation model as a path analysis. This is because the outcomes LRV scale (and any categorical mediators LRV scale) is already held constant by fixing the marginal variance to 1. However, this good news would not apply in more complex situations that affect LRV scales, such as multigroup models. So when using path analysis with categorical outcomes, it is best to exercise caution. Use the delta parameterization, and do not use a multigroup models to compare paths across groups (i.e., moderated mediation) unless you understand how to equate the LRV scales across groups. 3.5 References Bliss, C. I. (1934). The method of probits. Science, 79(2037), 3839. https://doi.org/10.1126/science.79.2037.38 Breen, R., Karlson, K. B., &amp; Holm, A. (2013). Total, direct, and indirect effects in logit and probit models. Sociological Methods &amp; Research, 42(2), 164191. https://doi.org/10.1177/0049124113494572 Katsikatsou, M., Moustaki, I., &amp; Jamil, H. (in press). Pairwise likelihood estimation for confirmatory factor analysis models with categorical variables and data that are missing at random. British Journal of Mathematical and Statistical Psychology. https://doi.org/10.1111/bmsp.12243 Little, T. D., Jorgensen, T. D., Lang, K. M., &amp; Moore, E. W. G. (2014). On the joys of missing data. Journal of Pediatric Psychology, 39(2), 151162. https://doi.org/10.1093/jpepsy/jst048 Muthén, B. (2011). Applications of causally defined direct and indirect effects in mediation analysis using SEM in Mplus [Technical report]. Retrieved from http://www.statmodel.com/download/causalmediation.pdf Olsson, U. (1979). Maximum likelihood estimation of the polychoric correlation coefficient. Psychometrika, 44(4), 443460. https://doi.org/10.1007/BF02296207 Olsson, U., Drasgow, F., &amp; Dorans, N. J. (1982). The polyserial correlation coefficient. Psychometrika, 47(3), 337347. https://doi.org/10.1007/BF02294164 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
