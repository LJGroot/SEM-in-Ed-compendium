[["index.html", "Compendium Structural Equation Modeling in Educational Research Preface", " Compendium Structural Equation Modeling in Educational Research Suzanne Jak and Terrence D. Jorgensen Preface "],["regression-as-mean--and-covariance-structure-analysis.html", "1 Regression as Mean- and Covariance-Structure Analysis 1.1 Installing lavaan 1.2 Types of Data Used in SEM 1.3 Regression Review 1.4 Summary Statistics 1.5 Covariance Matrix 1.6 References", " 1 Regression as Mean- and Covariance-Structure Analysis This chapter prepares the reader to learn about structural equation modeling (SEM) by reviewing the fundamentals of ordinary least-squares (OLS) regression. Different types of data are discussed, and some review of matrix algebra will be incorporated into the regression review. 1.1 Installing lavaan The SEM software that we will use is an open-source R package called lavaan (Rosseel, 2012). To install the lavaan package you can type the following command in the R console: install.packages(&quot;lavaan&quot;, dependencies = TRUE) Depending on your R(Studio) settings, this might open a window where you have to select a CRAN mirror (select [your country], [closest city]) prior to installing the package, including all additional packages that it depends on. You only need to install the package once (on a specific computer), but you must also load the library to access its functionality (similar to first installing an app, then opening the app to use it): library(lavaan) ## Warning: package &#39;lavaan&#39; was built under R version 4.1.3 Every time you start R you need to use this command to activate the lavaan package into the current R workspace. Therefore, it is advisable to start every script with this command. Sometimes, it will be necessary to install the development version of lavaan before the next version is made available on CRAN. For example, if the developer has fixed a bug or introduced a new feature, the development version can be installed using this syntax: install.packages(&quot;lavaan&quot;, repos = &quot;https://www.da.ugent.be&quot;, type = &quot;source&quot;) 1.2 Types of Data Used in SEM SEM can be seen as a generalization of the general(ized) linear model (GLM) for multivariate data. But rather than merely allowing multiple predictors and multiple outcomes, any variable can operate both as a predictor and an outcome in the same model. This allows us to model complex relationships among variables, including chains of causation (i.e., mediation). Estimation of a GLM involves minimizing discrepancies (differences) between observed (\\(y\\)) and expected (\\(\\hat{y}\\)) casewise values (i.e., an individual subjects score on an outcome variable \\(y\\)). In contrast, estimation of a SEM involves minimizing discrepancies between observed and expected summary statistics (i.e., the modeled variables means and covariance matrix). This is why another (older) name for an SEM is covariance structure analysis (when only trying to explain why variables are related to each other), or mean and covariance structure (MACS) analysis when means are also modeled. Summary statistics can be calculated from observed raw data, so raw data can be provided directly to SEM software for analysis. But if data are complete and come from a multivariate normal distribution, summary statistics can also be passed directly to the SEM software, which has advantages. Unlike raw data, summary statistics do not compromise the privacy of the research participants, so summary statistics can be reported in published empirical research. That allows readers (and students like you!) to have access to the same summary statistics to replicate the results or to fit a different model (e.g., that represents a competing theory). Thus, this chapter will show how to import both raw and summary data, as well as how to calculate summary statistics from raw data so that they can be reported along with results. 1.2.1 Raw Data Typically, data are already stored in an external file with a predefined format, such as SPSS (*.sav) or Excel (*.xlsx), and there are some pre-specified functions in R that can read in data from such type of formats, although they require the installation of specific packages. As an alternative, you can store the data in a plain text (with .txt or .csv extensions), where each line represents the observed responses of one individual. Spaces, commas, semicolons or tabs can be used to separate the individual responses. Some programs (like SPSS) can also export data to these types of file formats. read.table(&quot;data.txt&quot;, header = TRUE) read.spss(&quot;data.sav&quot;, to.data.frame = TRUE) # requires the package foreign read.xls(&quot;data.xls&quot;) # requires the package gdata as.data.frame(read_excel(&quot;data.xlsx&quot;)) # requires the package readxl This chapter uses data from a tutorial published on the . dat &lt;- foreign::read.spss(&quot;MODMED.sav&quot;, to.data.frame = TRUE)[c(2, 3, 5, 7)] head(dat) ## MOOD NFC POS ATT ## 1 -1 3.07579 6.2500 18.5455 ## 2 -1 2.59489 -5.0671 -16.9684 ## 3 -1 -1.00952 0.9346 -5.9578 ## 4 -1 -0.43824 2.8668 -7.2256 ## 5 -1 0.21788 -16.5601 -26.7000 ## 6 -1 0.43842 -13.1365 -24.1241 MOOD indicates experimental conditions \\(+1\\) = positive mood induction \\(-1\\) = neutral mood (control condition) NFC = need for cognition, POS = positive thoughts, and ATT = attitude 1.2.2 Summary Statistics from Raw Data To report summary statistics of your modeled variables, they can be calculated using the colMeans() function for means, the cov() function for a covariance matrix, and the nrow() function for the sample size Note: Counting the number of rows assumes you have complete data (M &lt;- colMeans(dat)) # mean vector ## MOOD NFC POS ATT ## 0.0000000 -0.0000002 0.0000040 1.9807230 (S &lt;- cov(dat)) # covariance matrix ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.03243879 4.3546465 6.841187 ## NFC -0.03243879 1.97289018 0.7953901 2.598788 ## POS 4.35464646 0.79539011 69.2629723 87.914121 ## ATT 6.84118687 2.59878834 87.9141208 282.059756 (N &lt;- nrow(dat)) # complete-data sample size ## [1] 100 Covariances are explained in more detail in a later section, but it is sufficient here to understand that the covariance between variables \\(x\\) and \\(y\\) (i.e., \\(\\sigma_{xy}\\)) is their unstandardized correlation coefficient (\\(r_{xy}\\)). That is, a correlation is a covariance between \\(z\\) scores. Because correlations are easier to interpret than covariances, it is common to report the means, SDs, and correlations. Using the SDs, readers can rescale correlations to covariances in the orignal units of measurement (i.e., \\(\\sigma_{xy} = \\sigma_x \\times \\sigma_y \\times r_{xy}\\)). The cov2cor() function can standardize a covariance matrix, or you can use the cor() function directly. (SD &lt;- sapply(dat, sd)) ## MOOD NFC POS ATT ## 1.005038 1.404596 8.322438 16.794635 sqrt(diag(S)) # SDs == square-roots of variances ## MOOD NFC POS ATT ## 1.005038 1.404596 8.322438 16.794635 cov2cor(S) # standardize the covariance matrix, or use cor() ## MOOD NFC POS ATT ## MOOD 1.00000000 -0.02297898 0.52061891 0.4053018 ## NFC -0.02297898 1.00000000 0.06804217 0.1101663 ## POS 0.52061891 0.06804217 1.00000000 0.6289810 ## ATT 0.40530176 0.11016633 0.62898098 1.0000000 (R &lt;- cor(dat)) ## MOOD NFC POS ATT ## MOOD 1.00000000 -0.02297898 0.52061891 0.4053018 ## NFC -0.02297898 1.00000000 0.06804217 0.1101663 ## POS 0.52061891 0.06804217 1.00000000 0.6289810 ## ATT 0.40530176 0.11016633 0.62898098 1.0000000 Because covariance and correlation matrices are symmetric (and diagonal elements of a correlation matrix are 1 by definition), both could be reported in a single table. For example, the upper triangle could be correlations, and the lower triangle (including the diagonal) could be (co)variances. printS &lt;- S printS[upper.tri(R)] &lt;- R[upper.tri(R)] printS # not symmetric, correlations in upper triangle ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.02297898 0.52061891 0.4053018 ## NFC -0.03243879 1.97289018 0.06804217 0.1101663 ## POS 4.35464646 0.79539011 69.26297231 0.6289810 ## ATT 6.84118687 2.59878834 87.91412077 282.0597561 1.2.2.1 Summary Statistics for Multiple Groups In SEM, it is common to estimate model parameters simultaneously in multiple groups (i.e., multigroup SEM or MG-SEM). In our example data, the MOOD variable is a 2-group variable, so we could create a factor from the numeric codes: dat$mood.f &lt;- factor(dat$MOOD, levels = c(-1, 1), labels = c(&quot;neutral&quot;,&quot;positive&quot;)) Then we can calculate the summary statistics separately within each group. modVars &lt;- c(&quot;ATT&quot;,&quot;NFC&quot;,&quot;POS&quot;) # modeled variables gM &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) colMeans(dat[dat$mood.f == g, modVars]) ) gS &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) cov(dat[dat$mood.f == g, modVars]) ) gN &lt;- table(dat$mood.f) To fit a MG-SEM in lavaan, the summary statistics must be a list with one (mean vector or covariance matrix) per group, as well as a vector of group sample sizes. gM ## $neutral ## ATT NFC POS ## -4.7920520 0.0321142 -4.3110960 ## ## $positive ## ATT NFC POS ## 8.7534980 -0.0321146 4.3111040 gS ## $neutral ## ATT NFC POS ## ATT 245.020884 4.812796 61.799307 ## NFC 4.812796 2.456274 -1.201971 ## POS 61.799307 -1.201971 47.663230 ## ## $positive ## ATT NFC POS ## ATT 231.2417228 0.8817017 56.235120 ## NFC 0.8817017 1.5276643 3.091531 ## POS 56.2351197 3.0915313 54.346483 gN ## ## neutral positive ## 50 50 1.2.3 Importing Summary Data 1.2.3.1 Full Covariance Matrix If we are importing summary data (e.g., from an article that reported them), we can type our data directly to our R script. Suppose the values above from S were rounded to 3 decimal places: covVals &lt;- c( 1.010, -0.032, 4.355, 6.841, -0.032, 1.973, 0.795, 2.599, 4.355, 0.795, 69.263, 87.914, 6.841, 2.599, 87.914, 282.060) covVals ## [1] 1.010 -0.032 4.355 6.841 -0.032 1.973 0.795 2.599 4.355 ## [10] 0.795 69.263 87.914 6.841 2.599 87.914 282.060 If these values were saved as plain text, we could also read the data from a file using the scan() function, which returns a vector that contains the stored values in the file values.txt. covVals &lt;- scan(&quot;values.txt&quot;) Either way, we can place these values from the full covariance matrix into a matrix using the matrix() function, specifying how many rows and columns there are: (newS &lt;- matrix(data = covVals, nrow = 4, ncol = 4)) ## [,1] [,2] [,3] [,4] ## [1,] 1.010 -0.032 4.355 6.841 ## [2,] -0.032 1.973 0.795 2.599 ## [3,] 4.355 0.795 69.263 87.914 ## [4,] 6.841 2.599 87.914 282.060 We can give names to the variables in the covariance matrix by using the dimnames= argument of the matrix() function, or by using the dimnames() function after creating the matrix. Lets save the variable names in an object obsnames, then use them to assign names. obsnames &lt;- c(&quot;MOOD&quot;, &quot;NFC&quot;, &quot;POS&quot;, &quot;ATT&quot;) ## providing names when creating matrix newS &lt;- matrix(covVals, nrow = 4, ncol = 4, dimnames = list(obsnames, obsnames)) ## or assign afterward, all at once dimnames(newS) &lt;- list(obsnames, obsnames) ## or one dimension at a time (useful for asymmetric matrix) rownames(newS) &lt;- obsnames colnames(newS) &lt;- obsnames 1.2.3.1.1 Rescaling a Correlation Matrix If the imported matrix is instead a full correlation matrix (i.e., assuming all variables have \\(SD=1\\)), then it can be transformed back into a covariance matrix using the \\(SD\\)s, so variables will be in their original units. This is important because an SEM fitted to a correlation matrix will have biased \\(SE\\)s and inflated Type I error rates unless complex constraints are imposed. ## store values from correlation matrix corVals &lt;- c( 1, -0.023, 0.521, 0.405, -0.023, 1, 0.068, 0.110, 0.521, 0.068, 1, 0.629, 0.405, 0.110, 0.629, 1) newR &lt;- matrix(data = corVals, nrow = 4, ncol = 4, dimnames = list(obsnames, obsnames)) newR ## MOOD NFC POS ATT ## MOOD 1.000 -0.023 0.521 0.405 ## NFC -0.023 1.000 0.068 0.110 ## POS 0.521 0.068 1.000 0.629 ## ATT 0.405 0.110 0.629 1.000 ## store SDs as a diagonal matrix (SDs &lt;- diag(SD)) ## [,1] [,2] [,3] [,4] ## [1,] 1.005038 0.000000 0.000000 0.00000 ## [2,] 0.000000 1.404596 0.000000 0.00000 ## [3,] 0.000000 0.000000 8.322438 0.00000 ## [4,] 0.000000 0.000000 0.000000 16.79463 ## transform correlations to covariances scaled.S &lt;- SDs %*% newR %*% SDs round(scaled.S, 3) ## [,1] [,2] [,3] [,4] ## [1,] 1.010 -0.032 4.358 6.836 ## [2,] -0.032 1.973 0.795 2.595 ## [3,] 4.358 0.795 69.263 87.917 ## [4,] 6.836 2.595 87.917 282.060 ## matches covariance matrix (within rounding error) newS ## MOOD NFC POS ATT ## MOOD 1.010 -0.032 4.355 6.841 ## NFC -0.032 1.973 0.795 2.599 ## POS 4.355 0.795 69.263 87.914 ## ATT 6.841 2.599 87.914 282.060 Note that rescaling the correlation matrix to be a covariance matrix required pre- and postmultipication of the (diagnal matrix of) \\(SD\\)s. The operator for matrix multiplication is %*% (the normal scalar multiplication operator * would simply multiply each cell of one matrix with the corresponding cell of another matrix). The table below gives an overview of commonly used functions in matrix algebra in R. Symbol Function Example in R \\(A+B\\) Addition A + B \\(A-B\\) Subtraction A - B (none) Elementwise Multiplication (in R) A * B (none) Elementwise Division (in R) A / B \\(AB\\) Matrix Multiplication A %*% B \\(A^{-1}\\) Inverse (enables division) solve(A) \\(A^{&#39;}\\) Transpose (rows become columns) t(A) \\(|A|\\) Determinant (generalized variance) det(A) To review basic matrix algebra, consult a linear algebra text or an appendix of some SEM textbooks (e.g., Bollen, 1989; Schumacker &amp; Lomax, 2016). 1.2.3.2 Upper/Lower Triangle of a Covariance Matrix As a covariance matrix is a symmetric matrix, we do not need to provide the complete matrix. Instead, we can import only the nonredundant information from only the lower (or upper) triangle of the covariance matrix, then use the lavaan function getCov() to create the full matrix. By default, getCov() expects the values from the lower triangle, and it will read values row-by-row (i.e., left-to-right, top-to-bottom), including the diagonal elements (diagonal = TRUE is the default argument). \\[ \\begin{bmatrix} 1.010 &amp; &amp; &amp; \\\\ -0.032 &amp; 1.973 &amp; &amp; \\\\ 4.355 &amp; 0.795 &amp; 69.263 &amp; \\\\ 6.841 &amp; 2.599 &amp; 87.914 &amp; 282.060 \\end{bmatrix} \\] This is equivalent to reading the upper triangle column-by-column (i.e., top-to-bottom, left-to-right). One can also add a vector with the names= of the observed variables. It returns the complete covariance matrix, including row and column names. The following code can be used to create a full matrix that is equal to S and newS above, using only the values from the lower triangle of the matrix. lowerS &lt;- c( 1.010, -0.032, 1.973, 4.355, 0.795, 69.263, 6.841, 2.599, 87.914, 282.060) getCov(x = lowerS, names = obsnames) ## MOOD NFC POS ATT ## MOOD 1.010 -0.032 4.355 6.841 ## NFC -0.032 1.973 0.795 2.599 ## POS 4.355 0.795 69.263 87.914 ## ATT 6.841 2.599 87.914 282.060 Sometimes you will find the upper triangle reported in a published paper instead of the lower triangle. Because getCov() only reads values row-by-row (i.e., left-to-right, top-to-bottom), reading an upper-triangle requires changing the argument to lower = FALSE. \\[ \\begin{bmatrix} 1.010 &amp; -0.032 &amp; 4.355 &amp; 6.841 \\\\ &amp; 1.973 &amp; 0.795 &amp; 2.599 \\\\ &amp; &amp; 69.263 &amp; 87.914 \\\\ &amp; &amp; &amp; 282.060 \\end{bmatrix} \\] upperS &lt;- c(1.010, -0.032, 4.355, 6.841, 1.973, 0.795, 2.599, 69.263, 87.914, 282.060) getCov(x = upperS, names = obsnames, lower = FALSE) ## MOOD NFC POS ATT ## MOOD 1.010 -0.032 4.355 6.841 ## NFC -0.032 1.973 0.795 2.599 ## POS 4.355 0.795 69.263 87.914 ## ATT 6.841 2.599 87.914 282.060 1.2.3.2.1 Upper/Lower Triangle of a Correlation Matrix Quite frequently, in published papers the covariance matrix is not provided, but instead a correlation matrix and \\(SD\\)s are given separately. The getCov() argument sds= can be used to automatically rescale the correlation matrix. Because the diagonal of a correlation matrix is always 1, it is not necessary to include the diagonal values. \\[ \\begin{bmatrix} 1 &amp; &amp; &amp; \\\\ -0.023 &amp; 1 &amp; &amp; \\\\ 0.521 &amp; 0.068 &amp; 1 &amp; \\\\ 0.405 &amp; 0.110 &amp; 0.629 &amp; 1 \\end{bmatrix} \\] In this case, tell lavaan that the diagonal entries are omitted using the diagonal=FALSE argument. The following syntax creates the complete covariance matrix that was used in the previous examples from the correlations and \\(SD\\)s. getCov(x = c(-0.023, 0.521, 0.068, 0.405, 0.110, 0.629), diagonal = FALSE, sds = SD, names = obsnames) ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.03246846 4.3578341 6.836093 ## NFC -0.03246846 1.97289018 0.7948971 2.594865 ## POS 4.35783405 0.79489713 69.2629723 87.916779 ## ATT 6.83609342 2.59486462 87.9167795 282.059756 If you are wondering whether the correlations appear in the correct order in the matrix, you can first leave the \\(SD\\)s out and check that the cells of the correlation matrix are in the correct order. If everything looks correct, then you can add the \\(SD\\)s. Covariance/correlation matrices are also symmetric, so it is also important to check that the lower triangle is a reflection of the upper triangle (i.e., Row-\\(x\\) Column-\\(y\\) of the matrix should contain the same value as Row-\\(y\\) Column-\\(x\\)). The R function isSymmetric() can perform this check for you: isSymmetric(S) ## [1] TRUE 1.3 Regression Review 1.3.1 Linear Regression Models An outcome \\(y\\) is a linear combination (weighted sum) of predictors Like a recipe for the outcome: How much of \\(x_1\\) and \\(x_2\\) do you need to recreate \\(y\\)? \\[\\begin{align*} y_i &amp;= \\textcolor{blue}{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}} &amp;+&amp; \\textcolor{darkred}{\\varepsilon_i} &amp; \\\\ &amp;= \\textcolor{blue}{\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij}} &amp;+&amp; \\textcolor{darkred}{\\varepsilon_i} &amp;\\sim \\textcolor{darkred}{\\mathcal{N}(0, \\sigma)} \\end{align*}\\] The of \\(y\\) can be predicted or explained by \\(X\\) The of \\(y\\) includes all other unmodeled effects (omitted variables) and sources of error 1.3.2 Matrix Notation For subjects \\(i = 1, \\dots, N\\) in rows and predictors \\(j = 1, \\dots, p\\) in columns: \\[\\begin{equation*} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\dots &amp; x_{1,p} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\dots &amp; x_{2,p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{N,1} &amp; x_{N,2} &amp; \\dots &amp; x_{N,p} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_N \\end{bmatrix} \\end{equation*}\\] Shorthand: \\(\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon}\\) The generalized linear model (GLM) extends basic linear models by incorporating different functions that link the outcome to its linear predictor distributional assumptions about the outcome(s errors) 1.3.3 Interpreting and Estimating Coefficients Regression models provide predictions of the outcome for any level(s) of the predictor(s) Only the intercept (\\(\\beta_0\\)) is interpreted as a mean: The expected value (\\(\\hat{y}\\)) when each predictor \\(x=0\\) Each slope (\\(\\beta_j\\)) is interpreted as the change in \\(\\hat{y}\\) as \\(x_j\\) increases i.e., How does \\(y\\) covary with \\(x_j\\), given other \\(x\\)s? Ordinary Least Squares (OLS) estimator of coefficients minimizes overall discrepancies between observations and predictions (\\(y - \\hat{y}\\)): \\[\\beta = (\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} \\mathbf{X}^{&#39;} \\mathbf{y}\\] Minimizing residual variance = maximizing \\(R^2\\) \\(R\\) = Cor(\\(y, \\hat{y}\\)) 1.3.4 Syntax Examples with Raw Data Using matrix algebra to estimate the coefficients: y &lt;- dat$ATT X &lt;- cbind(`(Intercept)` = 1, # first column is constant as.matrix(dat[c(&quot;MOOD&quot;,&quot;NFC&quot;,&quot;POS&quot;)])) solve(t(X) %*% X) %*% t(X) %*% y # beta ## [,1] ## (Intercept) 1.9807186 ## MOOD 1.8839147 ## NFC 0.8883671 ## POS 1.1406346 Matches results from linear-modeling function: mod &lt;- lm(ATT ~ MOOD + NFC + POS, data = dat) coef(mod) ## (Intercept) MOOD NFC POS ## 1.9807186 1.8839147 0.8883671 1.1406346 1.3.5 Categorical Predictors Groups \\(g = 1, \\ldots, G\\) can be represented using numeric codes dummy code for Group \\(g\\) indicates whether someone belongs to Group \\(g\\) (1) or not (0) effects codes and orthogonal contrast codes are alternatives MOOD is an effects-coded to indicate experimental conditions \\(+1\\) = positive mood induction \\(-1\\) = neutral mood (control condition) table(dat$MOOD) ## ## -1 1 ## 50 50 1.3.5.1 Regression with a Baseline Group When we are interested in comparing scores on a variable across groups, we are typically interested in the differences between the group means. Recall that an intercept-only model estimates a single mean for everyone mod0 &lt;- lm(POS ~ 1, data = dat) When we add a predictor to the model, the intercept becomes a conditional mean (i.e., the expected value when each predictor = 0), and the slope represents how the means differ between groups. By default, lm() will create dummy codes for all levels of a factor variable, except for the first group (the baseline for comparison) dat$mood.f &lt;- factor(dat$MOOD, levels = c(-1, 1), labels = c(&quot;neutral&quot;,&quot;positive&quot;)) coef(mod1 &lt;- lm(POS ~ mood.f, data = dat)) ## (Intercept) mood.fpositive ## -4.311096 8.622200 Practice interpreting the intercept and slope: aggregate(POS ~ mood.f, data = dat, FUN = mean) ## mood.f POS ## 1 neutral -4.311096 ## 2 positive 4.311104 diff(aggregate(POS ~ mood.f, dat, mean)$POS) ## [1] 8.6222 1.3.5.2 Regression with Group-Specific Intercepts Rather than choosing a baseline group, we can omit the intercept from the model, in which case lm() will include all \\(G\\) dummy codes (not \\(G-1\\)) coef(mod2 &lt;- lm(POS ~ -1 + mood.f, data = dat)) ## mood.fneutral mood.fpositive ## -4.311096 4.311104 Essentially, this model gives each group its own intercept, which is also the group-mean when there are no other predictors in the model aggregate(POS ~ mood.f, data = dat, FUN = mean) ## mood.f POS ## 1 neutral -4.311096 ## 2 positive 4.311104 Later in this course, you will learn about multigroup SEM, which is when the same SEM is fitted separately (but simultaneously) to 2 or more groups. Multigroup SEM is analogously advantageous, but (unlike in the GLM) variances can also differ across groups, so we do not need to assume homoskedasticity when using multigroup SEM. 1.3.5.3 Baseline Comparison vs. Group-Specific Intercepts To test the \\(H_0\\) that the group means are equivalent, we can compare the model that represents \\(H_0\\) (i.e., the intercept-only model, which estimates a single mean for all groups) to the model that represents the alternative hypothesis (\\(H_A\\)) that group means may differ. Because the models mod1 (intercept + 1 slope) and mod2 (2 slopes that are group-specific intercepts) are statistically equivalent (i.e., they make identical predictions), the ANOVA results are identical: anova(mod0, mod1) ## Analysis of Variance Table ## ## Model 1: POS ~ 1 ## Model 2: POS ~ mood.f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 6857.0 ## 2 98 4998.5 1 1858.6 36.439 2.819e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod0, mod2) ## Analysis of Variance Table ## ## Model 1: POS ~ 1 ## Model 2: POS ~ -1 + mood.f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 6857.0 ## 2 98 4998.5 1 1858.6 36.439 2.819e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 That is, both mod1 and mod2 say that we need 2 parameters to make good predictors (i.e., separate means for 2 groups), whereas the intercept-only model says that estimating 1 parameter provides similar predictions. 1.3.5.4 \\(t\\) vs. \\(F\\) Tests The group mean difference can also be tested if it is an estimated parameter, as it is in the baseline-group approach ## ## Call: ## lm(formula = POS ~ mood.f, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.7416 -5.6706 -0.4135 4.4780 17.4323 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.311 1.010 -4.268 4.55e-05 *** ## mood.fpositive 8.622 1.428 6.036 2.82e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.142 on 98 degrees of freedom ## Multiple R-squared: 0.271, Adjusted R-squared: 0.2636 ## F-statistic: 36.44 on 1 and 98 DF, p-value: 2.819e-08 Same \\(p\\) value, \\(t^2 = F\\) from ANOVA But group-specific intercepts (and slopes) can also be advantageous Each groups intercept and slopes are model parameters, rather than functions of parameters when using a baseline group 1.3.6 Constant vs. Varying Slopes Suppose we are investigating the possibility that this grouping variable moderates the effect of another (focal) predictor: Need for cognition (NFC). See this primer for a review. To represent the \\(H_0\\) that NFC has a constant effect on positive thoughts across groups, we can add it as a predictor to the model without an intercept. In this case, rather than group-specific means, the dummy codes are group-specific intercepts. slope0 &lt;- lm(POS ~ -1 + mood.f + NFC, data = dat) summary(slope0) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.1713 -5.4855 -0.2585 4.5490 16.9279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.3263 1.0109 -4.280 4.39e-05 *** ## mood.fpositive 4.3263 1.0109 4.280 4.39e-05 *** ## NFC 0.4743 0.5115 0.927 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 97 degrees of freedom ## Multiple R-squared: 0.2774, Adjusted R-squared: 0.2551 ## F-statistic: 12.42 on 3 and 97 DF, p-value: 6.115e-07 An interaction implies that slopes differ across groups With a baseline group, the reference-groups slope is estimated, along with how much each of \\(G-1\\) groups slopes differ from it ## or POS ~ mood.f * NFC slope1 &lt;- lm(POS ~ mood.f + NFC + mood.f:NFC, data = dat) summary(slope1) ## ## Call: ## lm(formula = POS ~ mood.f + NFC + mood.f:NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.3300 -5.3633 0.3697 4.7769 15.2801 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.2954 0.9858 -4.357 3.3e-05 *** ## mood.fpositive 8.6715 1.3943 6.219 1.3e-08 *** ## NFC -0.4893 0.6353 -0.770 0.4430 ## mood.fpositive:NFC 2.5130 1.0259 2.450 0.0161 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.969 on 96 degrees of freedom ## Multiple R-squared: 0.32, Adjusted R-squared: 0.2987 ## F-statistic: 15.06 on 3 and 96 DF, p-value: 4.168e-08 With group-specific parameters, each groups own intercept and slope are estimated, but multiplying each groups dummy code against the moderator ## no &quot;main effect&quot; of NFC without a reference group slope2 &lt;- lm(POS ~ -1 + mood.f + mood.f:NFC, data = dat) summary(slope2) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + mood.f:NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.3300 -5.3633 0.3697 4.7769 15.2801 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.2954 0.9858 -4.357 3.30e-05 *** ## mood.fpositive 4.3761 0.9860 4.438 2.42e-05 *** ## mood.fneutral:NFC -0.4893 0.6353 -0.770 0.4430 ## mood.fpositive:NFC 2.0237 0.8055 2.512 0.0137 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.969 on 96 degrees of freedom ## Multiple R-squared: 0.32, Adjusted R-squared: 0.2916 ## F-statistic: 11.29 on 4 and 96 DF, p-value: 1.498e-07 1.4 Summary Statistics 1.4.1 Reimport Data The next section is easier if all the variables in the data.frame are numeric, so lets just use the given effects-coded MOOD dat &lt;- foreign::read.spss(&quot;MODMED.sav&quot;, to.data.frame = TRUE)[c(2, 3, 5, 7)] 1.4.2 Raw Data vs. Summary Statistics Regression models include both mean-structure parameters (intercepts) and covariance-structure parameters (slopes, residual variance). Much of SEM involves the analysis of mean and covariance structures (MACS). OLS regression minimizes discrepancies between observed and predicted casewise observations of \\(y\\) \\(y - \\hat{y}\\) LS and ML estimators in SEM minimize discrepancies between observed and predicted summary statistics mean vector: \\(\\bar{y} - \\widehat{\\mu}\\) \\(\\bar{y}\\) = observed sample means \\(\\widehat{\\mu}\\) = expected/predicted/model-implied population means covariance matrix: \\(\\mathbf{S} - \\widehat{\\Sigma}\\) \\(\\mathbf{S}\\) = observed sample (co)variances \\(\\widehat{\\Sigma}\\) = expected/predicted/model-implied population (co)variances Rather than raw data in \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), we can obtain the same estimates using summary statistics alone 1.4.3 What Is Covariance? Variance (\\(\\sigma^2_y\\)) quantifies how individual scores vary the mean of squared deviations from \\(\\bar{y}\\) \\[\\text{Var}(y) = \\frac{\\sum_{i=1}^N (y_i - \\bar{y})^2}{N-1} = \\frac{\\sum_{i=1}^N (y_i - \\bar{y})(y_i - \\bar{y})}{N-1}\\] Covariance (\\(\\sigma_{xy}\\)) quantifies how \\(x\\) and \\(y\\) are linearly related covary = vary together \\[\\text{Cov}(x,y) = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{N-1}\\] Difficult to interpret its absolute value bound by \\(\\pm \\sigma_x \\sigma_y\\) so, dividing \\(\\sigma_{xy}\\) by \\(\\sigma_x \\sigma_y\\) limits the range to \\(\\pm 1\\) For \\(&gt;2\\) variables in a data set \\(\\mathbf{Y}\\), we can simultaneously calculate all (co)variances using matrix algebra: Mean-center the data-matrix \\(\\mathbf{Y} - \\bar{\\mathbf{Y}}\\) Square it by (pre)multiplying it by its transpose \\[\\Sigma = (\\mathbf{Y} - \\bar{\\mathbf{Y}})^{&#39;} (\\mathbf{Y} - \\bar{\\mathbf{Y}})\\] Variances are on the diagonal of \\(\\Sigma\\), and each off-diagonal cell contains the covariance between the row-variable and the column-variable. 1.4.4 Interpreting Covariance We can scale the covariance to provide familiar interpretations. Slopes are change in \\(y\\) (or \\(x\\)) per unit \\(x\\) (or \\(y\\)) divide by variance: \\(\\beta_{yx} = \\frac{\\sigma_{xy}}{\\sigma^2_x} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_x}\\) (or \\(\\frac{\\sigma_{xy}}{\\sigma^2_y} = \\frac{\\sigma_{xy}}{\\sigma_y \\sigma_y}\\)) Correlations are standardized covariances (bound by \\(\\pm 1\\)) divide by both \\(SD\\)s: \\(\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\) This is how slopes can be standardized (\\(\\beta^*\\)), which is a correlation when there is only 1 predictor \\[\\begin{align*} \\beta^*_{yx} &amp;= \\beta_{yx} \\times \\frac{\\sigma_x}{\\sigma_y} \\\\ &amp;= \\frac{\\sigma_{xy}}{\\mathcolorbox{orange}{\\sigma_x} \\sigma_x} \\times \\frac{\\mathcolorbox{orange}{\\sigma_x}}{\\sigma_y} \\text{ (\\colorbox{orange}{these} cancel out)} \\\\ &amp;= \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y} (= \\rho_{xy}) \\end{align*}\\] 1.5 Covariance Matrix With mean-centered predictors, \\((\\mathbf{X}^{&#39;} \\mathbf{X})^{-1}\\) is their covariance matrix and \\(\\mathbf{X}^{&#39;} \\mathbf{y}\\) captures their covariances with the outcome n &lt;- nrow(dat) - 1L Xc &lt;- scale(dat[c(&quot;MOOD&quot;,&quot;NFC&quot;,&quot;POS&quot;)], center = TRUE, scale = FALSE) cbind(t(Xc) %*% Xc , t(Xc) %*% y) / n ## MOOD NFC POS ## MOOD 1.01010101 -0.03243879 4.3546465 6.841187 ## NFC -0.03243879 1.97289018 0.7953901 2.598788 ## POS 4.35464646 0.79539011 69.2629723 87.914121 cov(dat) ## MOOD NFC POS ATT ## MOOD 1.01010101 -0.03243879 4.3546465 6.841187 ## NFC -0.03243879 1.97289018 0.7953901 2.598788 ## POS 4.35464646 0.79539011 69.2629723 87.914121 ## ATT 6.84118687 2.59878834 87.9141208 282.059756 1.5.1 OLS with Summary Statistics: \\(\\beta = (\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} \\mathbf{X}^{&#39;} \\mathbf{y}\\) In simple regression (1 predictor), the slope is simply the covariance scaled by the predictors variance: \\(\\beta_1 = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}\\) \\(\\mathbf{X}^{&#39;} \\mathbf{X} = \\text{Var}(x)\\), so \\((\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} = \\frac{1}{\\text{Var}(x)}\\) matrix-multiplication by inverse \\(\\approx\\) division \\(\\mathbf{X}^{&#39;} \\mathbf{y} = \\text{Cov}(x,y)\\) In multiple regression, the same formulas additionally account for covariances of \\(x\\) and \\(y\\) with covariates cov(dat)[&quot;POS&quot;,&quot;ATT&quot;] / var(dat$POS) ## [1] 1.26928 coef(lm(ATT ~ POS, data = dat))[[&quot;POS&quot;]] ## [1] 1.26928 1.5.2 Intercepts and Means With uncentered data, \\(X\\) can include a constant (all 1s) in the first column, which effectively partials out the mean. With no other predictors (an intercept-only model): \\[\\begin{align*} y_i &amp;= \\beta_0 (1) + \\varepsilon_i \\\\ &amp;= \\bar{y} + (y_i - \\bar{y}) \\end{align*}\\] the intercept is the mean residuals are mean-centered \\(y\\) (same variance) OLS simplifies to the arithmetic mean formula: \\(\\bar{y} = \\frac{\\Sigma y}{N}\\) \\((\\mathbf{X}^{&#39;} \\mathbf{X})^{-1} = \\frac{1}{N}\\) \\(\\mathbf{X}^{&#39;} \\mathbf{y} = \\sum y\\) 1.5.3 Intercepts and Means ## using matrix algebra ONE &lt;- X[ , &quot;(Intercept)&quot;] solve(t(ONE) %*% ONE) %*% t(ONE) %*% y ## [,1] ## [1,] 1.980723 ## fitted linear model with only an intercept coef(lm(y ~ 1)) ## (Intercept) ## 1.980723 ## calculate the mean mean(y) ## [1] 1.980723 1.5.4 What Are Mean and Covariance Structures? Regression models already illustrate how means are structured A mean (\\(\\bar{y}\\)) is an expected value under a particular condition (\\(\\beta_0\\) when \\(X=0\\)) plus predictors means times their slopes: \\(\\bar{y} = \\beta_0 + \\sum \\beta_j \\bar{x}_j\\) Recall how often statistical procedures involve partitioning variance into components Regression: explained (\\(R^2\\)) vs. unexplained (residual) variance ANOVA: variance between vs. within groups Multilevel modeling: Level-1 v. -2 variance and (un)explained at each level Between vs. within multiple imputations of missing data Covariance can also be partitioned into components Variance is merely a variable covarying with itself We can account for multiple reasons why 2 variables are related The decomposition of covariances will be explained after introducing path analysis. 1.6 References Bollen, K. A. (1989). Structural equations with latent variables. Wiley. http://dx.doi.org/10.1002/9781118619179 Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 136. http://dx.doi.org/10.18637/jss.v048.i02 Schumacker, R. E., &amp; Lomax, R. G. (2016). A beginners guide to structural equation modeling (4th ed.). Lawrence Erlbaum. "],["using-lavaan-to-run-regression-and-anova-as-a-sem.html", "2 Using lavaan to Run Regression and ANOVA as a SEM 2.1 Prepare Data and Workspace 2.2 Multiple Regression in SEM 2.3 Caveats with SEM 2.4 Multigroup SEM", " 2 Using lavaan to Run Regression and ANOVA as a SEM 2.1 Prepare Data and Workspace 2.1.1 Import Example Data We will use data from a tutorial published on the Social Change Labs web site. dat &lt;- foreign::read.spss(&quot;MODMED.sav&quot;, to.data.frame = TRUE)[c(2, 3, 5, 7)] ## Save summary statistics M &lt;- colMeans(dat) S &lt;- cov(dat) N &lt;- nrow(dat) Recall from Chapter 1 that: SEM can equivalently be fitted to summary statistics rather than raw data It is possible to fit a multigroup SEM to obtain parameter estimates separately per group This chapter will show how to fit a multigroup SEM to raw data as well as to summary statistics. For the latter, we require group-specific summary statistics, which Chapter 1 discussed how to obtain: ## Create factor from effects-coded conditions dat$mood.f &lt;- factor(dat$MOOD, levels = c(-1, 1), labels = c(&quot;neutral&quot;,&quot;positive&quot;)) ## Save lists of group-specific summary statistics CC &lt;- c(&quot;ATT&quot;,&quot;NFC&quot;,&quot;POS&quot;) # when group = &quot;mood.f&quot; gM &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) colMeans(dat[dat$mood.f == g, CC]) ) gS &lt;- sapply(c(&quot;neutral&quot;,&quot;positive&quot;), simplify = FALSE, FUN = function(g) cov(dat[dat$mood.f == g, CC]) ) gN &lt;- table(dat$mood.f) 2.1.2 Load lavaan into Workspace library(lavaan) 2.2 Multiple Regression in SEM 2.2.1 lavaan Syntax The standard syntax for regression is a formula object. To regress ATTitude on MOOD condition, Need For Cognition, and POSitive thoughts: ATT ~ MOOD + NFC + POS SEM is a multivariate modeling framework There can be many outcome variables, each with its own formula Outcomes can also predict other outcomes There can be unobserved (latent) variables So lavaan requires collecting these simultaneous equations in a character vector, even when there is only one outcome &#39; ATT ~ MOOD + NFC + POS &#39; 2.2.1.1 OLS Regression Using the familiar lm() function to obtain OLS estimates of a regression model: ols &lt;- lm(ATT ~ MOOD + NFC + POS, data = dat) summary(ols) ## ## Call: ## lm(formula = ATT ~ MOOD + NFC + POS, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.061 -8.440 -0.417 8.718 28.010 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9807 1.3107 1.511 0.134 ## MOOD 1.8839 1.5388 1.224 0.224 ## NFC 0.8884 0.9422 0.943 0.348 ## POS 1.1406 0.1862 6.126 1.98e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.11 on 96 degrees of freedom ## Multiple R-squared: 0.4094, Adjusted R-squared: 0.3909 ## F-statistic: 22.18 on 3 and 96 DF, p-value: 5.382e-11 2.2.1.2 MLE in lavaan with Raw or Summary Data When data are complete, maximum likelihood estimation (MLE) provides equivalent results using either raw or summary data. There are multiple model-fitting functions in lavaan: lavaan() is the main engine, but expects that models are fully specified in complete detail sem() is a wrapper that calls lavaan() with some sensible defaults that apply to most SEMs (e.g., automatically estimating residual variances, automatically estimating covariances among predictors) cfa() is meant for fitting confirmatory factor analysis (CFA) models, discussed in a later chapter. cfa() and sem() actually behave identically (i.e., they call lavaan() with the same defaults) growth() is meant for very simple latent growth curve models, also discussed in a later chapter For now, we will use the sem() function similar to the way we use the lm() function, including how we obtain results from summary(). ## Raw Data mle &lt;- sem(&#39;ATT ~ MOOD + NFC + POS&#39;, data = dat, meanstructure = TRUE) # explicitly request intercept ## Summary Data mle &lt;- sem(&#39;ATT ~ MOOD + NFC + POS&#39;, sample.nobs = N, sample.cov = S, sample.mean = M) Now inspect the output. Identify each parameter estimate, and compare it to the corresponding OLS estimate in the lm() results. summary(mle, rsquare = TRUE, header = FALSE, nd = 4) ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## ATT ~ ## MOOD 1.8839 1.5077 1.2495 0.2115 ## NFC 0.8884 0.9232 0.9623 0.3359 ## POS 1.1406 0.1824 6.2519 0.0000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .ATT 1.9807 1.2842 1.5424 0.1230 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .ATT 164.9191 23.3231 7.0711 0.0000 ## ## R-Square: ## Estimate ## ATT 0.4094 sigma(ols)^2 # compare to residual variance from OLS ## [1] 171.7908 2.2.1.3 Compare/Contrast Estimates lavaans point estimates match OLS, except residual variance (\\(\\sigma^2_\\varepsilon\\)) OLS estimates \\(\\sigma^2_\\varepsilon\\) by dividing the residual \\(SS\\) by the residual \\(df\\) SEM estimators choose estimates that reproduce \\(\\bar{y}\\) and \\(S\\) If we calculate variance of the residuals (\\(y_i - \\hat{y}\\)) using the population formula (dividing by \\(N\\)), we obtain lavaans result sigma(ols)^2 # residual variance from OLS ## [1] 171.7908 sum(resid(ols)^2) / N # manually calculated using POPULATION formula ## [1] 164.9191 coef(mle)[[&quot;ATT~~ATT&quot;]] # matches lavaan ## [1] 164.9191 Note: The double-tilde (~~) operator signifies a two-headed arrow, in contrast to the one-headed arrow (~) of a directed effect 2.2.1.4 Compare/Contrast \\(SE\\)s The estimated \\(SE\\)s are smaller from lavaan because SEM is an asymptotic method estimator assumes \\(N\\) is sufficiently large that \\(S \\approx \\Sigma\\) Wald \\(z\\) statistics replace OLSs \\(t\\) statistics Model comparison results in a \\(\\chi^2\\) rather than \\(F\\) statistic The asymptotic assumption yields more power, but inflated Type I error rates when sample size is low (relative to number of estimated parameters). Note that lavaan simply calculates sample.cov= and sample.mean= from data=, but they can be passed directly. Also note that raw data= are required to analyze incomplete data (using full information MLE rather than listwise deletion), obtain robust statistics (e.g., to account for nonnormality of continuous outcomes), or model discrete (binary or ordinal) outcomes (which require a threshold model, explained later in a chapter about categorical data) 2.3 Caveats with SEM 2.3.1 Model Comparison with OLS To test the \\(H_0\\) that the experimental manipulation (MOOD) explains \\(R^2=0\\)% of variance in POSitive thoughts, compare models that represent the \\(H_0\\) and \\(H_A\\) ols0 &lt;- lm(POS ~ 1, data = dat) ols1 &lt;- lm(POS ~ MOOD, data = dat) anova(ols0, ols1) ## Analysis of Variance Table ## ## Model 1: POS ~ 1 ## Model 2: POS ~ MOOD ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 6857.0 ## 2 98 4998.5 1 1858.6 36.439 2.819e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This result matches the \\(F\\) test at the bottom of summary(ols1) 2.3.2 Model Comparison with lavaan Omitting variables from an SEM means they are removed from \\(\\bar{y}\\) and \\(S\\), returning a warning mle0 &lt;- sem(&#39;POS ~ 1&#39;, data = dat) mle1 &lt;- sem(&#39;POS ~ 1 + MOOD&#39;, data = dat) anova(mle0, mle1) ## ... models are based on a different set of observed variables Instead, keep predictor(s) in the model, but fix the slope(s) to zero mle0 &lt;- sem(&#39;POS ~ 1 + 0*MOOD&#39;, data = dat) anova(mle0, mle1) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mle1 0 680.96 688.78 0.000 ## mle0 1 710.57 715.78 31.614 31.614 1 1.88e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.3.3 Moderation Caveats When evaluating moderation using a product term, that product term is a new variable in \\(\\bar{y}\\) and \\(S\\) e.g., we want to test MOODs effect, controlling for NFC before comparing adjusted group means, we should evaluate the homogeneity-of-slopes assumption As in formula objects, lavaan syntax recognizes the colon (:) operator but the asterisk (*) is reserved for assigning labels or values to parameters POS ~ 1 + MOOD + NFC + MOOD:NFC MOOD:NFC is now an additional variable in \\(\\bar{y}\\) and \\(S\\), so comparison to any model without that interaction term requires including the product term but fixing its effect to zero 2.3.3.1 Estimate and Test an Interaction with OLS ols.hom &lt;- lm(POS ~ MOOD + NFC, data = dat) ols.het &lt;- lm(POS ~ MOOD + NFC + MOOD:NFC, data = dat) anova(ols.hom, ols.het) ## Analysis of Variance Table ## ## Model 1: POS ~ MOOD + NFC ## Model 2: POS ~ MOOD + NFC + MOOD:NFC ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 4954.6 ## 2 96 4663.1 1 291.47 6.0005 0.01611 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using ML estimation, model comparison would analogously yield an analysis of deviance a.k.a. likelihood ratio test (LRT) 2.3.3.2 Estimate and Test an Interaction with lavaan mle.hom &lt;- sem(&#39;POS ~ 1 + MOOD + NFC + 0*MOOD:NFC&#39;, data = dat) mle.het &lt;- sem(&#39;POS ~ 1 + MOOD + NFC + MOOD:NFC&#39;, data = dat) anova(mle.hom, mle.het) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mle.het 0 678.01 691.04 0.0000 ## mle.hom 1 682.08 692.50 6.0629 6.0629 1 0.0138 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can reject \\(H_0\\) of homogeneous NFC slopes across MOOD groups 2.3.3.3 Compare Adjusted Means Suppose we had failed to reject the \\(H_0\\) of homogeneity of slopes, and we wanted a single comparison of (adjusted) group means, controlling for NFC. MOOD is effect coded, so its slope (which is identical using OLS regression or SEM) is interpreted as the difference between a groups mean and the grand mean half the group mean difference The \\(t\\) test (OLS) or Wald \\(z\\) test (SEM) is sufficient to test the \\(H_0: \\widehat{y}_\\text{Treatment}|x = \\widehat{y}_\\text{Control}|x\\) summary(mle.hom) ## lavaan 0.6-11 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 4 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 6.063 ## Degrees of freedom 1 ## P-value (Chi-square) 0.014 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## POS ~ ## MOOD 4.326 0.704 6.145 0.000 ## NFC 0.474 0.504 0.941 0.346 ## MOOD:NFC 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 0.000 0.704 0.000 1.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 49.546 7.007 7.071 0.000 But the MOOD slope could also be fixed to zero (or dropped) to obtain a LRT from SEM (or \\(F\\) test from ANOVA). 2.3.4 Summary of Caveats SEMs are only comparable when they are fitted to the same data same observations (rows) and same variables (columns) Instead of removing predictors, fix slopes to zero Interaction terms are additional variables, added to \\(\\bar{y}\\) and \\(S\\) only compare models with same variables products of variables can be saved in the data.frame or specified using the colon (:) operator (e.g., x1:x2), not the asterisk (*) standardized solution is incorrect because it treats product terms as separate variables (e.g., if they were independently transformed to \\(z\\) scores rather than products of \\(z\\) scores) 2.4 Multigroup SEM 2.4.1 Group-Specific Intercepts in OLS Recall that omitting the intercepts allows each group to have a dummy code in the model slope0 &lt;- lm(POS ~ -1 + mood.f + NFC, data = dat) summary(slope0) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.1713 -5.4855 -0.2585 4.5490 16.9279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.3263 1.0109 -4.280 4.39e-05 *** ## mood.fpositive 4.3263 1.0109 4.280 4.39e-05 *** ## NFC 0.4743 0.5115 0.927 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 97 degrees of freedom ## Multiple R-squared: 0.2774, Adjusted R-squared: 0.2551 ## F-statistic: 12.42 on 3 and 97 DF, p-value: 6.115e-07 This still assumes homoskedasticity of residuals, and comparing intercepts (or adjusted means) requires the , which is a way to estimate the SE of a function of parameters (e.g., difference between 2 slopes) from the SEs of the original parameter estimates. car::deltaMethod(slope0, &quot;b2 - b1&quot;, rhs = 0, parameterNames = paste0(&quot;b&quot;, 1:3)) ## Estimate SE 2.5 % 97.5 % Hypothesis z value Pr(&gt;|z|) ## b2 - b1 8.6527 1.4298 5.8504 11.4549 0.0000 6.0519 1.432e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.4.2 Group-Specific Intercepts in SEM? lavaan automatically uses the delta method when parameters are labeled in the model syntax (before * operator) model syntax includes user-defined parameters (:= operator) However, SEM cannot fit group-specific intercepts because they are linearly dependent (recall (multi)collinearity among predictors from a previously read regression text). That is, the positive-mood dummy code is simply 1 minus the neutral-mood dummy code, so they are perfectly (negatively) correlated. Therefore, \\(S\\) would not be , which is a term that indicates some redundancy among variables. This means that fitting a single-group SEM with group-specific intercepts is not possible: dat$treat &lt;- dat$mood.f == &quot;positive&quot; dat$control &lt;- 1 - dat$treat # linear dependency ## label slopes &quot;b1&quot; for neutral group, &quot;b2&quot; for positive group mod1 &lt;- &#39; POS ~ b1*mood.fneutral + b2*mood.fpositive + NFC ## user-defined parameter adj_mean_diff := b2 - b1 &#39; fit1 &lt;- sem(mod1, data = dat) # lavaan ERROR: ## sample covariance matrix is not positive-definite 2.4.3 Multiple Groups in SEM Rather than including MOOD as variable(s) in the model, we can fit the remainder of the model (POS ~ 1 + NFC) separately in each group each group gets unique parameter estimates constraints across groups can be added homoskedasticity of residuals homogeneous slopes MOODs effect is how group-specific intercepts differ each group gets unique parameter labels, in a vector below: POS ~ c(b1, b2)*1 use the same label for homogeneous slopes below: POS ~ c(b3, b3)*NFC mod2 &lt;- &#39; POS ~ c(b1, b2)*1 + c(b3, b3)*NFC ## user-defined parameter adj_M_diff := b2 - b1 &#39; fit2 &lt;- sem(mod2, data = dat, group = &quot;mood.f&quot;) summary(fit2) ## lavaan 0.6-11 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## Number of equality constraints 1 ## ## Number of observations per group: ## neutral 50 ## positive 50 ## ## Model Test User Model: ## ## Test statistic 6.037 ## Degrees of freedom 1 ## P-value (Chi-square) 0.014 ## Test statistic for each group: ## neutral 2.217 ## positive 3.820 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [neutral]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## POS ~ ## NFC (b3) 0.443 0.502 0.882 0.378 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS (b1) -4.325 0.982 -4.404 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 48.225 9.645 5.000 0.000 ## ## ## Group 2 [positive]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## POS ~ ## NFC (b3) 0.443 0.502 0.882 0.378 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS (b2) 4.325 1.009 4.288 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .POS 50.870 10.174 5.000 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## adj_M_diff 8.651 1.408 6.143 0.000 ## compare to OLS, which assumes homoskedasticity summary(slope0) ## ## Call: ## lm(formula = POS ~ -1 + mood.f + NFC, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.1713 -5.4855 -0.2585 4.5490 16.9279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## mood.fneutral -4.3263 1.0109 -4.280 4.39e-05 *** ## mood.fpositive 4.3263 1.0109 4.280 4.39e-05 *** ## NFC 0.4743 0.5115 0.927 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 97 degrees of freedom ## Multiple R-squared: 0.2774, Adjusted R-squared: 0.2551 ## F-statistic: 12.42 on 3 and 97 DF, p-value: 6.115e-07 ## delta method to test adj_M_diff car::deltaMethod(slope0, &quot;b2 - b1&quot;, rhs = 0, parameterNames = paste0(&quot;b&quot;, 1:3)) ## Estimate SE 2.5 % 97.5 % Hypothesis z value Pr(&gt;|z|) ## b2 - b1 8.6527 1.4298 5.8504 11.4549 0.0000 6.0519 1.432e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the point and SE estimates from this delta-method result (using OLS) can differ from the corresponding results using SEM. Because SEM does not assume residual homoskedasticity, it may be more robust, but there is a trade-off because OLS is not biased by small samples the way that SEM is. 2.4.3.1 Summary Statistics for Multigroup SEM lavaan calculates summary statistics from the raw data= separately in each group, but lists can be passed directly gN # vector of sample sizes ## ## neutral positive ## 50 50 gM # mean vectors ## $neutral ## ATT NFC POS ## -4.7920520 0.0321142 -4.3110960 ## ## $positive ## ATT NFC POS ## 8.7534980 -0.0321146 4.3111040 gS # list of covariance matrices ## $neutral ## ATT NFC POS ## ATT 245.020884 4.812796 61.799307 ## NFC 4.812796 2.456274 -1.201971 ## POS 61.799307 -1.201971 47.663230 ## ## $positive ## ATT NFC POS ## ATT 231.2417228 0.8817017 56.235120 ## NFC 0.8817017 1.5276643 3.091531 ## POS 56.2351197 3.0915313 54.346483 fit2 &lt;- sem(mod2, sample.nobs = gN, # &quot;group=&quot; is implied by lists sample.mean = gM, sample.cov = gS) 2.4.3.2 Advantages of Multigroup SEM Less restrictive assumptions parameters differ by default More interpretable parameters each group has their own intercept and slopes Intuitive to specify \\(H_0\\) as user-defined parameter e.g., the difference between the intercepts in the treatment group (b2) and control group (b1) is zero Intuitive to represent assumptions about equality constraints by using the same labels across groups e.g., the same NFC effect (b3) testable by comparing models with(out) constraints Can easily test \\(H_0\\) about parameters other than means Are variances or correlations equal across groups? 2.4.3.3 Disadvantages: asymptotic assumption (large \\(N\\)) applies to each group single-group regression better in small samples "],["path-models.html", "3 Path Models 3.1 Illustrative example 3.2 Conceptual explanation 3.3 Matrix explanation 3.4 Path analysis using lavaan 3.5 References 3.6 Appendix I. Derivations of equation 3.08 and 3.09", " 3 Path Models Path analysis is one of the families of statistical analyses within the structural equation modeling (SEM) framework. Path analysis is used to describe the dependencies between a set of observed variables, where a structural model defines the directional relations between the variables. Path analysis builds on regression analysis principles, but can be used for more complex models that include multiple dependent variables or (multiple) mediator variables. We will first introduce the illustrative example that is used throughout this and subsequent chapters on path models. Then, we will explain the path model both conceptually and technically. Finally, we will illustrate how to fit a path model with an empirical data example using the lavaan program. 3.1 Illustrative example To illustrate path analysis we will use data from a study by Affrunti and Woodruff-Borden (2014) who investigated the development of child anxiety. They hypothesized that perfectionistic parents will engage in behaviors characterized by overcontrol, which then will increase child anxiety. In addition, parent anxiety was thought to influence child anxiety. In order to test their hypothesized model the variables parent perfectionism, parental overcontrol, parent anxiety and child anxiety were measured in 77 families (see Affrunti &amp; Woodruff-Borden, 2014 for more information on data collection and operationalization of the variables). In this chapter we will use a slightly adapted version of the model on the development of child anxiety. 3.2 Conceptual explanation Figure 3.1 is a graphical display of the path diagram of the structural model for the development of child anxiety that serves our illustrative purposes. A path diagram is a graphical representation of the path model in which the directional relationships between observed variables are defined. The four squares represent the observed variables parental anxiety (PA), parental perfectionism (PP), parental overcontrol (PO) and child anxiety (CA). The effects between the observed variables are represented by one sided arrows (). In our example, the model consists of direct effects from parent anxiety and parent perfectionism on parental overcontrol, and a direct effect of parental overcontrol on child anxiety. Parent anxiety and parent perfectionism are so-called exogenous variables. The states or values of exogenous variables are not influenced by other variables in the model. They are sometimes also referred to as independent variables or predictor variables. Parental overcontrol and child anxiety are so-called endogenous variables, as they are assumed to be influenced by the states of the other variables in the model. An endogenous variable is sometimes referred to as a dependent variable. However, an endogenous variable can also have an effect on another endogenous variable in the model. In our example, parental overcontrol is influenced by parent anxiety and parent perfectionism (i.e., parental overcontrol is an endogenous variable). Parental overcontrol, in turn, also has a direct effect on child anxiety. This path model therefore represents a mediation model, where the effects of parent anxiety and parent perfectionism on child anxiety are fully mediated through parental overcontrol. Figure 3.1. A simple path diagram of the development of child anxiety Figure 3.1 not only gives a graphical representation of the set of directional relations between the observed variables, but also includes the circles \\({\\zeta}_{PA}\\), \\({\\zeta}_{PF}\\), \\({\\zeta}_{PO}\\), and \\({\\zeta}_{CA}\\) that represent the so-called residual factors. Residual factors are unobserved, latent variables that represent all factors that fall outside the model but may influence the states of the corresponding observed variables. The directional effects from the residual factors to the corresponding observed variables are accompanied by the numeral 1. This indicates that these effects are fixed to 1, which is a so-called scaling constant (which will be explained in more detail in Chapter 12). Because these scaling constants for residual factors apply to almost all models, they are often not displayed in path diagrams. For endogenous variables, the residual factor can be viewed as a kind of container variable, that contains all variables that also affect the specific endogenous variable, but that were not included in the model. The residual factors therefore symbolize the probabilistic causality of path analysis, as they indicate that endogenous variables are not only influenced by other variables in the model but also by (unobserved) variables that are outside the model (i.e., unmeasured causes). For example, in the model for the development of child anxiety, it would be unrealistic to assume that the anxiety of a child is only determined by parents behavior of overcontrol. It might be that there are also other factors, e.g. past experiences, contact with peers, or genetic predisposition, that have an effect on the anxiety of a child. To enable a model where child anxiety is only partially influenced by parental overcontrol, the unmeasured causes of child anxiety are incorporated in the model through the residual factor. The residual factors of endogenous variables are therefore sometimes also referred to as disturbance factors. The residual factors of exogenous variables can also be viewed as a container variable that contains all (unobserved) variables that are outside the model (i.e., unmeasured causes) that influence the corresponding exogenous variable. However, because exogenous variables are not influenced by other variables in the model, the residual factor of an exogenous variable represents the total variances of the corresponding variable. In addition, the residual factors of different exogenous variables covary. For example, in the path diagram of Figure 1 there is a covariance between the two residual factors of parent anxiety and parent perfectionism, as reflected by the double headed arrow (&lt;&gt;). This reflects the assumption that the unmeasured causes of the corresponding exogenous variables may covary, although the model does not provide an explanation for this association (i.e., they are outside the model). For example, parental neuroticism may be a common cause of parental anxiety and parental perfectionism, leading to a covariance between the two variables. Because the common causes are not modelled, and we may not even know all possible sources of shared variance between exogenous variables, it is very unrealistic to assume that the exogenous variables would not be correlated. For both exogenous and endogenous variables, the residual factor also reflects measurement error. The variance of a residual factor therefore partly consists of variance of the corresponding variable due to random error fluctuations. Thus, one cannot distinguish residual variance due to measurement error (i.e., unsystematic variance) and residual variance due to unmeasured causes (i.e., systematic variance). When the measure of the observed variable is unreliable this will inflate the variance of the corresponding residual factor and can be confounded with variance due to unmeasured causes of the variable. 3.3 Matrix explanation In general, the aim of path analysis is to describe the dependencies between a set of observed variables, based on some theory. In order to do this, we try to define a theoretically sensible path model that can accurately describe the variances and covariances of the observed variables in the population and formulate the following hypothesis: \\(\\mathbf{\\Sigma}_{population} = \\mathbf{\\Sigma}_{model}\\), where \\(\\mathbf{\\Sigma}_{population}\\) is the matrix of population variances and covariances of the observed variables, and \\(\\mathbf{\\Sigma}_{model}\\) is the matrix of variances and covariances of the observed variables as implied by the path model. The population variances and covariances are unknown, as data are usually only collected in a sample that was drawn from the population. Therefore, we estimate the population variances and covariances based on the sample variances and covariances: \\(\\mathbf{\\Sigma}_{population} \\approx \\mathbf{\\hat\\Sigma}_{population}\\) , where \\(\\mathbf{\\hat\\Sigma}_{population} = \\mathbf{\\Sigma}_{sample}\\). As a covariance matrix is a symmetric matrix, usually the upper triangle of the matrix is not provided: \\[ \\mathbf{\\Sigma}_{population} = \\begin{bmatrix} \\sigma_{11} &amp; &amp; \\\\ \\sigma_{21} &amp; \\sigma_{22} &amp; \\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_{33} \\\\ \\sigma_{41} &amp; \\sigma_{42} &amp; \\sigma_{43} &amp; \\sigma_{44} \\end{bmatrix} \\approx \\begin{bmatrix} \\hat\\sigma_{11} &amp; &amp; \\\\ \\hat\\sigma_{21} &amp; \\hat\\sigma_{22} &amp; \\\\ \\hat\\sigma_{31} &amp; \\hat\\sigma_{32} &amp; \\hat\\sigma_{33} \\\\ \\hat\\sigma_{41} &amp; \\hat\\sigma_{42} &amp; \\hat\\sigma_{43} &amp; \\hat\\sigma_{44} \\end{bmatrix}. \\] The variances of the variables are given on the diagonal of the matrix (\\(\\sigma_{11}\\) to \\(\\sigma_{44}\\)) and the covariances are given on the lower triangle of the matrix (e.g., \\(\\sigma_{21}\\) represents the covariance between variable \\(\\mathrm{y}_2\\) and variable \\(\\mathrm{y}_1\\)). The values of the upper triangular are equal to the lower triangular (e.g., \\(\\sigma_{21}\\) is equal to \\(\\sigma_{12}\\)). The population variances and covariances are estimated using the sample variances and covariances, where: \\[ \\mathbf{\\Sigma}_{sample} = \\begin{bmatrix} s_{11} &amp; &amp; \\\\ s_{21} &amp; s_{22} &amp; \\\\ s_{31} &amp; s_{32} &amp; s_{33} \\\\ s_{41} &amp; s_{42} &amp; s_{43} &amp; s_{44} \\end{bmatrix}. \\] Here, the elements of \\(\\mathbf{\\Sigma}_{sample}\\) are denoted with s. As an example, the observed sample variance of the first variable, \\(s_{11}\\), serves as an estimate of the population variance of the first variable, \\(\\hat\\sigma_{11}\\). The sample covariance matrix of our illustrative example of child anxiety is given by: \\[ \\mathbf{\\Sigma}_{sample} = \\begin{bmatrix} 91.58 &amp; &amp; \\\\ 53.36 &amp; 194.32 &amp; \\\\ 28.39 &amp; 50.90 &amp; 130.19 \\\\ 9.21 &amp; 4.98 &amp; 9.41 &amp; 7.56 \\end{bmatrix}. \\] Thus, the variance of the variable parent anxiety is 91.58, and the covariance between the variables parent anxiety and parent perfectionism is 53.36. Similarly, we do not know the true variances and covariances of the observed variables in the population as implied by the true model parameters in the path model. Instead, we derive an estimated variance-covariance matrix as implied by the model: \\(\\mathbf{\\Sigma}_{model} \\approx \\mathbf{\\hat\\Sigma}_{model}\\), where \\(\\mathbf{\\hat\\Sigma}_{model}\\) is a function of sample estimates of the unknown model parameters. Thus, to enable calculation of \\(\\mathbf{\\hat\\Sigma}_{model}\\) we first need to derive estimates of all model parameters that feature in the path model. Below, we will describe the relationships between the observed variables in terms of model parameters, and explain how they can be used to find expressions for the variances and covariances. Figure 3.2 shows the Greek symbols that represent the variables and parameters that feature in the path model. The observed variables parental anxiety, parental perfectionism, parental overcontrol and child anxiety are represented by the squares \\(\\mathrm{y}_1\\), \\(\\mathrm{y}_2\\), \\(\\mathrm{y}_3\\), and \\(\\mathrm{y}_4\\) respectively. The direct effects between the observed variables are denoted \\(\\beta_{31}\\), \\(\\beta_{32}\\), and \\(\\beta_{43}\\). These direct effects are also called path coefficients and can be interpreted as regression coefficients. In general, \\(\\beta_{ij}\\) is the regression of variable \\(\\mathrm{y}_{i}\\) on \\(\\mathrm{y}_{j}\\), or, equivalently, the effect of \\(\\mathrm{y}_{j}\\) on \\(\\mathrm{y}_{i}\\). In addition, the circles are residual factors \\(\\zeta_{1}\\), \\(\\zeta_{2}\\), \\(\\zeta_{3}\\), and \\(\\zeta_{4}\\) that represent all factors that fall outside the model, i.e., unmeasured causes. Figure 3.2. Path diagram of the development of child anxiety including all path coefficients Using the symbols described above, the relations between the variables can be represented by the following equations: \\[\\begin{equation} \\mathrm{y}_1 = \\zeta_1 \\tag{3.1} \\end{equation}\\] \\[\\begin{equation} \\mathrm{y}_2 = \\zeta_2 \\tag{3.2} \\end{equation}\\] \\[\\begin{equation} \\mathrm{y}_3=\\beta_{31}\\mathrm{y}_1+\\beta_{32}\\mathrm{y}_2 + \\zeta_3 \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} \\mathrm{y}_4 = \\beta_{43} \\mathrm{y}_3 + \\zeta_4 \\tag{3.4} \\end{equation}\\] Here, we can see that exogenous variables \\(\\mathrm{y}_1\\) and \\(\\mathrm{y}_2\\) are only affected by factors that fall outside the model (\\(\\zeta_1\\) and \\(\\zeta_2\\)), whereas endogenous variable \\(\\mathrm{y}_3\\) and \\(\\mathrm{y}_4\\) are affected also by the other variables in the model. Note that the effects of the residual factors are left out of these Equations because they are constrained to 1 (i.e., we could write \\(\\mathrm{y}_1\\) = (direct effect)\\(\\zeta_1\\), where (direct effect) = 1, which thus simplifies to \\(\\mathrm{y}_1\\) = \\(\\zeta_1\\)). Instead of writing down the equation for each observed variable, the equations for the observed variables \\(\\mathrm{y}_1\\), \\(\\mathrm{y}_2\\), \\(\\mathrm{y}_3\\), and \\(\\mathrm{y}_4\\) can also be written in matrix form: \\[\\begin{equation} \\mathbf{y} = \\mathbf{B} \\mathbf{y} + \\boldsymbol{\\zeta} \\tag{3.5} \\end{equation}\\] where \\(\\mathbf{y}\\) is a vector of all observed variables, \\(\\boldsymbol{\\zeta}\\) is a vector of all residual factors, and \\(\\mathbf{B}\\) is a matrix of regression coefficients: \\[ \\mathbf{y} = \\begin{bmatrix} \\mathrm{y}_1 \\\\ \\mathrm{y}_2 \\\\ \\mathrm{y}_3 \\\\ \\mathrm{y}_4 \\end{bmatrix} , \\boldsymbol{\\zeta} = \\begin{bmatrix} \\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3 \\\\ \\zeta_4 \\end{bmatrix}, \\mathrm{and} \\hspace{1mm} \\mathbf{B} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\beta_{31} &amp; \\beta_{32} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\beta_{43} &amp; 0 \\end{bmatrix}. \\] Matrix \\(\\mathbf{B}\\) contains three non-zero elements: \\(\\beta_{31}\\), \\(\\beta_{32}\\), \\(\\beta_{43}\\). The regression coefficient \\(\\beta_{31}\\) represent the effect ofvariable \\(\\mathrm{y}_1\\) on variable \\(\\mathrm{y}_3\\), the regression coefficient \\(\\beta_{32}\\) represent the effect of variable \\(\\mathrm{y}_2\\) on variable \\(\\mathrm{y}_3\\), and the regression coefficient \\(\\beta_{43}\\) represent the effect of variable \\(\\mathrm{y}_3\\) on variable \\(\\mathrm{y}_4\\). Substituting these matrices into Equation (3.5) gives: \\[\\begin{equation} \\begin{bmatrix} \\mathrm{y}_1 \\\\ \\mathrm{y}_2 \\\\ \\mathrm{y}_3 \\\\ \\mathrm{y}_4 \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\beta_{31} &amp; \\beta_{32} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\beta_{43} &amp; 0 \\end{bmatrix} \\times \\begin{bmatrix} \\mathrm{y}_1 \\\\ \\mathrm{y}_2 \\\\ \\mathrm{y}_3 \\\\ \\mathrm{y}_4 \\end{bmatrix} + \\begin{bmatrix} \\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3 \\\\ \\zeta_4 \\end{bmatrix}, \\tag{3.6} \\end{equation}\\] which yields: \\[\\begin{equation} \\begin{bmatrix} \\mathrm{y}_1 \\\\ \\mathrm{y}_2 \\\\ \\mathrm{y}_3 \\\\ \\mathrm{y}_4 \\end{bmatrix} = \\begin{bmatrix} &amp; &amp; \\zeta_1 &amp; &amp; \\\\ &amp; &amp; \\zeta_2 &amp; &amp; \\\\ \\beta_{31} \\mathrm{y}_1 &amp; + &amp; \\beta_{32} \\mathrm{y}_2 &amp; + &amp; \\zeta_3 \\\\ &amp; &amp; \\beta_{43} \\mathrm{y}_3 + \\zeta_4 &amp; \\end{bmatrix}. \\tag{3.7} \\end{equation}\\] Here, we can see that the equations for variables \\(\\mathrm{y}_1\\), \\(\\mathrm{y}_2\\), \\(\\mathrm{y}_3\\), and \\(\\mathrm{y}_4\\) are the same as separate Equations (3.1) through (3.4). The path model as described by Equation 3.07 is used to describe the relationships between the observed variables. In order to find the model for the variances and covariances of the observed variables, we have to re-write Equation (3.5) as a function of model parameters: \\[\\mathrm{y} = \\mathbf{B} \\mathrm{y} + \\zeta \\Leftrightarrow\\] \\[\\begin{equation} \\mathrm{y} = (\\mathbf{I} - \\mathbf{B})^{-1} \\zeta \\tag{3.8} \\end{equation}\\] where \\(\\mathbf{I}\\) is an identity matrix that needs to be introduced to come to the end result, and \\((\\mathbf{I} - \\mathbf{B})^{-1}\\) denotes the inverse of the matrix \\((\\mathbf{I} - \\mathbf{B})\\). Derivations of Equation (3.8) are given in Appendix 1. We now have an expression for the scores on the variables. However, in standard SEM, we do not model the observed scores directly, but the variances and covariance of the observed scores. Therefore, we need to find an expression for the model implied covariance matrix. Using Equation 3.08 and some covariance algebra, we obtain the expression for the variances and covariances of \\(\\mathrm{y}\\), called \\(\\mathbf{\\Sigma}_{model}\\) = COV\\((\\mathbf{y},\\mathbf{y})\\): \\[\\begin{equation} \\mathbf{\\Sigma}_{model} = (\\mathbf{I} - \\mathbf{B})^{-1} \\boldsymbol{\\Psi} (\\mathbf{I} - \\mathbf{B})^{-1\\mathrm{T}} \\tag{3.9} \\end{equation}\\] where \\(\\boldsymbol{\\Psi}\\) is written for COV(\\(\\boldsymbol\\zeta\\),\\(\\boldsymbol\\zeta\\)), and \\((\\mathbf{I} - \\mathbf{B})^{-1\\mathrm{T}}\\) denotes the transpose of the matrix \\((\\mathbf{I} - \\mathbf{B})^{-1}\\). Derivations of Equation (3.9) are given in Appendix 1. The symmetric matrix \\(\\boldsymbol{\\Psi}\\) contains the variances and the covariances of the residual factors \\(\\boldsymbol\\zeta\\). For the model given in Equation (3.7), matrix \\(\\boldsymbol{\\Psi}\\) is \\[ \\boldsymbol{\\Psi} = \\begin{bmatrix} \\psi_{11} &amp; \\psi_{12} &amp; 0 &amp; 0 \\\\ \\psi_{21} &amp; \\psi_{22} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\psi_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\psi_{44} \\end{bmatrix} \\] where \\(\\psi_{11}\\) and \\(\\psi_{22}\\) represent the variances of \\(\\zeta_1\\) and \\(\\zeta_2\\) and are thus equivalent with the variances of the exogenous variables \\(\\mathrm{y}_1\\) and \\(\\mathrm{y}_2\\). The covariance of \\(\\zeta_{1}\\) and \\(\\zeta_{2}\\) is represented by \\(\\psi_{12}\\) and is thus equivalent with the covariance of \\(\\mathrm{y}_1\\) and \\(\\mathrm{y}_2\\). Because a covariance matrix is a symmetric matrix, \\(\\psi_{12}\\) is equal to \\(\\psi_{21}\\). Parameters \\(\\psi_{33}\\) and \\(\\psi_{44}\\) are the variances of \\(\\zeta_3\\) and \\(\\zeta_4\\), equivalent with the residual variances (or disturbance variances) of \\(\\mathrm{y}_3\\) and \\(\\mathrm{y}_4\\), that is, that part of the variances of \\(\\mathrm{y}_3\\) and \\(\\mathrm{y}_4\\) that are not explained by the model.Note that the \\(\\zeta\\) variances associated with endogenous variables \\(\\mathrm{y}_3\\) and \\(\\mathrm{y}_4\\) have the same interpretation as the \\(\\zeta\\) variances associated with exogenous variables \\(\\mathrm{y}_1\\) and \\(\\mathrm{y}_2\\), but whereas all variance of \\(\\mathrm{y}_1\\) and \\(\\mathrm{y}_2\\) is unexplained by the model (and thus \\(\\psi_{11}\\) and \\(\\psi_{22}\\) are equivalent with the variances of \\(\\mathrm{y}_1\\) and \\(\\mathrm{y}_2\\)), only part of the variances of \\(\\mathrm{y}_3\\) and \\(\\mathrm{y}_4\\) is unexplained by the model. Now that we have found the general expression for the variances and covariances as a function of model parameters, we can evaluate Equation 3.09 of our example. Substituting \\(\\mathbf{B}\\) and \\(\\boldsymbol{\\Psi}\\) of our example we obtain: \\(\\mathbf{\\Sigma}_{model} = (\\mathbf{I} - \\mathbf{B})^{-1} \\boldsymbol{\\Psi} (\\mathbf{I} - \\mathbf{B})^{-1\\mathrm{T}}\\), where: \\[ (\\mathbf{I} - \\mathbf{B})^{-1} = \\Bigg( \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} - \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\beta_{31} &amp; \\beta_{32} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\beta_{43} &amp; 0 \\end{bmatrix} \\Bigg) ^{-1} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ \\beta_{31} &amp; \\beta_{32} &amp; 1 &amp; 0 \\\\ \\beta_{31}\\beta_{43} &amp; \\beta_{32}\\beta_{43} &amp; \\beta_{43} &amp; 1 \\end{bmatrix}, \\] \\[ \\boldsymbol{\\Psi} = \\begin{bmatrix} \\psi_{11} &amp; \\psi_{21} &amp; 0 &amp; 0 \\\\ \\psi_{21} &amp; \\psi_{22} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\psi_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\psi_{44} \\end{bmatrix}, \\mathrm{and} \\] \\[ (\\mathbf{I} - \\mathbf{B})^{-1\\mathrm{T}} = \\Bigg( \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} - \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\beta_{31} &amp; \\beta_{32} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\beta_{43} &amp; 0 \\end{bmatrix} \\Bigg)^{-1\\mathrm{T}} = \\begin{bmatrix} 1 &amp; 0 &amp; \\beta_{31} &amp; \\beta_{31}\\beta_{43} \\\\ 0 &amp; 1 &amp; \\beta_{32} &amp; \\beta_{32}\\beta_{43} \\\\ 0 &amp; 0 &amp; 1 &amp; \\beta_{43} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Which yields: \\[\\begin{equation} \\Sigma_{model} = \\begin{bmatrix} \\\\ \\psi_{11} &amp; &amp; &amp; \\\\ \\\\ \\psi_{21} &amp; \\psi_{22} &amp; &amp; \\\\ \\\\ \\beta_{31} \\psi_{11} + &amp; \\beta_{31} \\psi_{21} + &amp; (\\beta_{31}\\psi_{11}+\\beta_{32}\\psi_{31}) \\beta_{31} + &amp; \\\\ \\beta_{32} \\psi_{21}&amp;\\beta_{32} \\psi_{22}&amp;(\\beta_{31}\\psi_{21}+\\beta_{32}\\psi_{22}) \\beta_{32}\\psi_{33}&amp; \\\\ \\\\ \\beta_{31}\\beta_{43}\\psi_{11} + &amp; \\beta_{31}\\beta_{43}\\psi_{21} + &amp; (\\beta_{31}\\beta_{43}\\psi_{11} + \\beta_{32}\\beta_{43}\\psi_{21}) \\beta_{31} + &amp; (\\beta_{31}\\beta_{43}\\psi_{11} + \\beta_{32}\\beta_{43}\\psi_{21})\\beta_{31}\\beta_{43} + \\\\ \\beta_{32}\\beta_{43}\\psi_{21} &amp; \\beta_{32}\\beta_{43}\\psi_{22} &amp; (\\beta_{31}\\beta_{43}\\psi_{21} + \\beta_{32}\\beta_{43}\\psi_{22}) \\beta_{32} + &amp; (\\beta_{31}\\beta_{43}\\psi_{21} + \\beta_{32}\\beta_{43} \\psi_{22})\\beta_{32}\\beta_{43} + \\\\ &amp;&amp;\\beta_{43}\\psi_{33} &amp; {\\beta_{32}}^2 \\psi_{33} + \\psi_{44} \\\\ &amp;&amp;&amp;\\\\ \\end{bmatrix} \\tag{3.10} \\end{equation}\\] Now that we have found the expression of \\(\\mathbf{\\Sigma}_{model}\\) as a function of model parameters, this enables the estimation of the model parameters, by choosing the values for each model parameter in such a way, that the resulting model implied variances and covariances of \\(\\mathbf{\\hat\\Sigma}_{model}\\) are as close as possible to the sample variances and covariances, \\(\\mathbf{\\hat\\Sigma}_{population}\\). A discrepancy function can be used to obtain parameter estimates, where an iterative procedure yields model parameters that keep the discrepancy between \\(\\mathbf{\\hat\\Sigma}_{population}\\) and \\(\\mathbf{\\hat\\Sigma}_{model}\\) to a minimum. In general, smaller values of discrepancy functions indicate better fit of the model to the data, and a value of zero indicates the fit is perfect, i.e., if the parameter estimates can perfectly reproduce the sample covariance matrix. Several discrepancy functions exist, but maximum likelihood estimation is most commonly applied. The resulting discrepancy value can be used for the assessment of overall goodness-of-fit using the so-called chi-square test of exact fit, where a significant chi-square value indicates a significant deviation between model and data (i.e., implying that \\(\\mathbf{\\hat\\Sigma}_{population} \\ne \\mathbf{\\Sigma}_{model}\\)). See also Chapters 10 and 11, where the topics of Estimation and Evaluation of Model Fit are explained in more detail. In our illustrative example of child anxiety, maximum likelihood estimation leads to the following model parameter estimates: \\[ \\hat{\\mathbf{B}} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\hat\\beta_{31} &amp; \\hat\\beta_{32} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\hat\\beta_{43} &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0.19 &amp; 0.21 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.07 &amp; 0 \\end{bmatrix}, \\] and \\[ \\hat{\\boldsymbol{\\Psi}} = \\begin{bmatrix} \\hat\\psi_{11} &amp; \\hat\\psi_{21} &amp; 0 &amp; 0 \\\\ \\hat\\psi_{21} &amp; \\hat\\psi_{22} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\hat\\psi_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\hat\\psi_{44} \\end{bmatrix} = \\begin{bmatrix} 92.58 &amp; 53.36 &amp; 0 &amp; 0 \\\\ 53.36 &amp; 194.32 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 114.16 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 6.88 \\end{bmatrix}. \\] Matrix \\(\\hat{\\mathbf{B}}\\) contains the estimated regression coefficients \\(\\hat\\beta_{31}\\), \\(\\hat\\beta_{32}\\) and \\(\\hat\\beta_{43}\\). The estimated regression of parental overcontrol on parent anxiety (\\(\\hat\\beta_{31}\\)) is 0.19. As the parameter estimate is unstandardized, this indicates that with 1 point increase in parent anxiety there will be 0.19 point increase in parental overcontrol, holding constant all other variables in the model. The estimated effect of parent perfectionism on parental overcontrol (\\(\\hat\\beta_{32}\\)) is 0.21, indicating that controlled for all other variables in the model, 1 point increase in parent perfectionism will lead to 0.21 point increase in parental overcontrol. Lastly, the estimated effect of parental overcontrol on child anxiety is 0.07, where 1 point increase in parental overcontrol will result in a 0.07 increase in child anxiety (controlled for all other variables in the model). All estimated effects between the variables are positive, and they support the hypothesis that parent anxiety and perfectionism increase parental overcontrol, which then increases child anxiety. However, the unstandardized parameter estimates do not give a direct interpretation of the size of the effects, and additional information is needed to judge the significance of the effects (see also Chapters 5 and 6). Matrix \\(\\hat{\\boldsymbol{\\Psi}}\\) contains the estimated variances and covariance of the residual factors. Here, we can see that the variance of the residual factor of parent anxiety (\\(\\psi_{11}\\)) is 91.58 and the variance of the residual factor of parent perfectionism (\\(\\psi_{22}\\)) is 194.32. Remember that the variances of exogenous variables are not explained by any observed variables, and thus all variance in parent anxiety and parent perfectionism is represented by the variances of their corresponding residual factors. In addition, the covariance between parent anxiety and parent perfectionism is equal to the covariance between their corresponding residual factors (\\(\\psi_{21}\\)). The variance of the residual factor of overcontrol (\\(\\psi_{33}\\)) can be interpreted as the variance of overcontrol that is unexplained by the model. The same is true for the estimated variance of the residual factor of child anxiety (\\(\\psi_{44}\\)). As the estimated model parameters of the psi-matrix provide information about the amount of variance that is unexplained by the model, we can also calculate the amount of variance that is explained by the model. The variance explained by the model is a result that is often reported in ordinary regression analysis as an indication of how well the model fits the data. Therefore, just as in regression analysis, it is informative to calculate the proportion of explained variance by using the estimated residual factor variance (i.e., the unexplained variance). In our example, the unexplained variance of the observed variable child anxiety is 6.88. We can calculate the proportion of explained variance of child anxiety using the following formula: \\(R^2 = 1 - \\frac{unexplained variance}{total variance}\\). In our example the proportion of explained variance of child anxiety is 0.09, indicating that only 9\\(\\%\\) of the variance of child anxiety is explained by the path model that was specified (see also Chapter 5 on standardized parameters). 3.4 Path analysis using lavaan 3.4.1 Installing lavaan The software that we will use to run our structural equation modeling analyses is lavaan (Rosseel, 2012), which is free open-source software that is available as a package in R. To install the lavaan package you can type the following command in the R console: install.packages(&quot;lavaan&quot;, dependencies = TRUE) This will open a window where you have to select a CRAN mirror (select [your country], [closest city]) and will install the package lavaan, including all the packages that it is dependent upon. You only need to install the package once (on a specific computer), to add the package to the R library. Once the package is installed, you can activate all the functionalities that are available in the package by using the command: library(lavaan) Every time you start R you need to use this command to activate the lavaan package into the current R workspace. Therefore, it is advisable to start every script with this command. 3.4.2 Fitting a path model Script 3.1 fits the path model of Figure 3.2 to the covariance matrix that is given in Affrunti and Woodruff-Borden (2014). All commands will be explained below. Script 3.1 # names observed variables obsnames &lt;- c(&quot;parent_anx&quot;, &quot;perfect&quot;, &quot;overcontrol&quot;, &quot;child_anx&quot;) # covariance matrix as given by Affrunti and Woodruff-Borden (2014) AWcov &lt;- matrix(data = c(91.58, 53.36, 28.39, 9.21, 53.36, 194.32, 50.90, 4.98, 28.39, 50.90, 130.19, 9.41, 9.21, 4.98, 9.41, 7.56), nrow = 4, ncol = 4, dimnames = list(obsnames,obsnames)) # specify the path model AWmodel &lt;- &#39;# regression equations overcontrol ~ b31*parent_anx + b32*perfect child_anx ~ b43*overcontrol # (residual) variance parent_anx ~~ p11*parent_anx perfect ~~ p22*perfect overcontrol ~~ p33*overcontrol child_anx ~~ p44*child_anx # covariance exogenous variables parent_anx ~~ p21*perfect &#39; # build the model AWmodelOut &lt;- lavaan( model = AWmodel, sample.cov = AWcov, sample.nobs = 77, likelihood = &quot;wishart&quot;, fixed.x = FALSE) We start by defining the observed covariance matrix and the names of the associated observed variables. It is required to provide these names with the input matrix. First we created the object obsnames that contains the names of the variables. Note that the order of the variables corresponds to the labels \\(\\mathrm{y}_1\\) to \\(\\mathrm{y}_4\\) in Figure 3.2. obsnames &lt;- c(&quot;parent_anx&quot;, &quot;perfect&quot;, &quot;overcontrol&quot;, &quot;child_anx&quot;) The names are given as a list with two elements, one vector of row names and one vector of column names. As a covariance matrix is a symmetric matrix, row and column names are the same, and we can use the command list(obsnames, obsnames) to provide the labels for the observed covariance matrix. The observed covariance matrix is stored in the object AWcov, by creating a matrix with the values of the elements, number of rows, number of columns, and the name vectors of the two dimensions. AWcov = matrix(...) To check whether you successfully specified the observed covariance matrix, check the results by typing AWcov in the R console. And check, for example, whether the matrix is indeed symmetrical by typing AWcov == t(AWcov) or isSymmetric(AWcov). The next step is to specify the model that has to be fitted to the observed data. We create the object AWmodel where the model specifications are given, encapsulated by single quotes. We need to specify all free parameters that need to be estimated: regression coefficients and (residual) variances and covariances. The structural part of the model is specified using regression equations: AWmodel &lt;- &#39; # regression equations overcontrol ~ b31*parent_anx + b32*perfect child_anx ~ b43*overcontrol The tilde sign, ~, is the regression operator. On the left hand of this operator we have the dependent variable, and on the right hand side of this operator we have the independent variables, separated by the + operator. The regression coefficients are named after their position in the \\(\\mathbf{B}\\) matrix. For example, the regression of overcontrol on parent anxiety is labeled with b31, which corresponds to the position of the estimate of the regression coefficient (\\(\\beta_{31}\\)) in the \\(\\mathbf{B}\\) matrix. As such, the names b31, b32, b43 are chosen to represent \\(\\beta_{31}\\), \\(\\beta_{32}\\), and\\(\\beta_{43}\\). The residual terms are not explicitly included in the formulas. These formulas therefore only specify the structural part of the model. The (residual) variances are specified using double tildes (\\(\\sim \\sim\\)), where the same variable name is given both left and right of this operator. Covariances are specified similarly. The estimates of the (co)variances can be named by providing a label before the variable name that is to the right from the \\(\\sim \\sim\\) operator. Here, we used the names p11 to p44 and p21 to refer to the position of the parameter estimates in the \\(\\boldsymbol{\\Psi}\\) matrix. # (residual) variance parent_anx ~~ p11*parent_anx perfect ~~ p22*perfect overcontrol ~~ p33*overcontrol child_anx ~~ p44*child_anx # covariance exogenous variables parent_anx ~~ p21*perfect &#39; When all separate elements of the model are created, we can fit (run) the model using the lavaan() function and storing the output in AWmodelOut: AWmodelOut &lt;- lavaan( model = AWmodel, sample.cov = AWcov, sample.nobs = 77, likelihood = &quot;wishart&quot;, fixed.x = FALSE) The lavaan() function takes as arguments the specified model (model = AWmodel), the specified covariance matrix (sample.cov = AWcov), and the total number of observations (sample.nobs = 77). The final two arguments are used to turn off two default settings of lavaan. That is, we use Wishart likelihood (likelihood = \"wishart\") because we are only analyzing covariance structure (when we add mean structure later in the semester, we will use normal likelihood). We also specify that we want to freely estimate (fixed.x = FALSE) rather than fix the (co)variances of the exogenous variables to their sample values. The output is stored in AWmodelOut, and we can inspect the result using the summary() function: summary(AWmodelOut) This will show you some fit results of the model and give you the parameter estimates and their associated standard errors. The model has 2 degrees of freedom, and the chi-square value is 7.333. The summary of the output is given below: ## lavaan 0.6-11 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Number of observations 77 ## ## Model Test User Model: ## ## Test statistic 7.333 ## Degrees of freedom 2 ## P-value (Chi-square) 0.026 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## overcontrol ~ ## parnt_nx (b31) 0.187 0.140 1.341 0.180 ## perfect (b32) 0.210 0.096 2.194 0.028 ## child_anx ~ ## ovrcntrl (b43) 0.072 0.026 2.741 0.006 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## parent_anx ~~ ## perfect (p21) 53.360 16.481 3.238 0.001 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## parnt_nx (p11) 91.580 14.856 6.164 0.000 ## perfect (p22) 194.320 31.523 6.164 0.000 ## .ovrcntrl (p33) 114.157 18.519 6.164 0.000 ## .child_nx (p44) 6.880 1.116 6.164 0.000 Here we see the result for the regression coefficients \\(\\beta_{31}\\), \\(\\beta_{32}\\), and \\(\\beta_{43}\\). lavaan also provides the associated standard error of the estimate (0.140, for \\(\\beta_{31}\\)). This gives an indication of the precision of the parameter estimate and can be used to judge whether the parameter estimate differs significantly from zero (see also [Chapter 5][05-IndirectEffects]). Especially when parameters are labeled, lavaan will have to shorten some of the longer variable names (only in the printed output, not internally!), so that the output conforms to a certain format. You may also notice that some variances are preceded by a period (.). This indicates that it is a residual variance for an endogenous variable, rather than a variance for an exogenous variable. If you run models with residual correlations between endogenous variables, those residual covariances will also use the (.) prefix. You can also request to see \\(R^2\\) for each endogenous variable: summary(AWmodelOut, rsquare = TRUE) ## lavaan 0.6-11 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Number of observations 77 ## ## Model Test User Model: ## ## Test statistic 7.333 ## Degrees of freedom 2 ## P-value (Chi-square) 0.026 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## overcontrol ~ ## parnt_nx (b31) 0.187 0.140 1.341 0.180 ## perfect (b32) 0.210 0.096 2.194 0.028 ## child_anx ~ ## ovrcntrl (b43) 0.072 0.026 2.741 0.006 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## parent_anx ~~ ## perfect (p21) 53.360 16.481 3.238 0.001 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## parnt_nx (p11) 91.580 14.856 6.164 0.000 ## perfect (p22) 194.320 31.523 6.164 0.000 ## .ovrcntrl (p33) 114.157 18.519 6.164 0.000 ## .child_nx (p44) 6.880 1.116 6.164 0.000 ## ## R-Square: ## Estimate ## overcontrol 0.123 ## child_anx 0.090 3.4.3 Syntax shortcuts Script 3.1 specifies every single nonzero parameter depicted in the path model of Figure 3.2. In larger models, this can become quite cumbersome, increase the chance of making an error, and make the model syntax more difficult to read (e.g., to find mistakes). Some parameters can be specified automatically in the lavaan() call. For example, setting the option auto.var = TRUE will tell lavaan to freely estimate all (residual) variances of variables included in the model syntax, and the option fixed.x = FALSE already tells lavaan to freely estimate covariances among exogenous variables. Thus, adding these options to the lavaan() call means that the model syntax requires only the specification of regression parameters, which is quite easy to read: ## shorter model specification AWmodel &lt;- &#39; # regression equations overcontrol ~ b31*parent_anx + b32*perfect child_anx ~ b43*overcontrol &#39; ## free all (residual) variances using auto.var = TRUE AWmodelOut &lt;- lavaan( model = AWmodel, auto.var = TRUE, sample.cov = AWcov, sample.nobs = 77, likelihood = &quot;wishart&quot;, fixed.x = FALSE) In fact, the auto.var = TRUE option is set by default in lavaans shortcut function, sem(), which sets several other default options that are typically desired in a path analysis (or structural regression among latent variables, which you will learn about later). AWmodelOut &lt;- sem( model = AWmodel, sample.cov = AWcov, sample.nobs = 77, likelihood = &quot;wishart&quot;, fixed.x = FALSE) 3.4.4 Extracting results from lavaan output in matrix form To be able to inspect the parameter estimates of a path model in matrix form (namely the \\(\\mathbf{B}\\) and \\(\\boldsymbol{\\Psi}\\) matrices), you can use the lavInspect() function, which can be used to extract many kinds of information about the fitted lavaan model (see a complete list on the help page: ?lavInspect). The lavInspect() function takes two arguments: (1) the lavaan object (in this case, AWmodelOut) and (2) a character string naming what specific information you want to inspect (in this case, the estimated coefficients: coef). SEM involves more than just the \\(\\mathbf{B}\\) and \\(\\boldsymbol{\\Psi}\\) matrices, and the output will always include them even if they are empty (e.g., full of zeros, or an identity matrix). The following code can be used to store the \\(\\mathbf{B}\\) and \\(\\boldsymbol{\\Psi}\\) matrices in the objects BETA and PSI to control what is displayed. Estimates &lt;- lavInspect(AWmodelOut, &quot;est&quot;) BETA &lt;- Estimates$beta[obsnames, obsnames] PSI &lt;- Estimates$psi[obsnames, obsnames] The resulting BETA and PSI matrices are shown below. Because lavaan ignores the order of the variables in the input covariance matrix, its matrices correspond to variables in the order that they appeared in the model syntax. The order is arbitrary, but to make them easier to read (e.g., to compare this output to the matrices described in the previous sections of this chapter), the above syntax used the square-bracket operators to specify that the order of [rows, columns] should be the same order as our input matrix. ## [1] &quot;BETA&quot; ## parent_anx perfect overcontrol child_anx ## parent_anx 0.0000000 0.000000 0.00000000 0 ## perfect 0.0000000 0.000000 0.00000000 0 ## overcontrol 0.1873575 0.210491 0.00000000 0 ## child_anx 0.0000000 0.000000 0.07227898 0 ## [1] &quot;PSI&quot; ## parent_anx perfect overcontrol child_anx ## parent_anx 91.58 53.36 0.0000 0.000000 ## perfect 53.36 194.32 0.0000 0.000000 ## overcontrol 0.00 0.00 114.1569 0.000000 ## child_anx 0.00 0.00 0.0000 6.879855 Here, we can see that the variance of the residual factor of parent anxiety (\\(\\psi_{11}\\)) is 91.580 and the variance of the residual factor of parent perfectionism (\\(\\psi_{22}\\)) is 194.320. 3.5 References Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 1-36. 3.6 Appendix I. Derivations of equation 3.08 and 3.09 Derivation of Equation 3.08 \\(\\mathbf{y} = \\mathbf{B} \\mathbf{y} + \\boldsymbol{\\zeta} \\Leftrightarrow\\) \\(\\mathbf{y} - \\mathbf{B} \\mathbf{y} = \\boldsymbol{\\zeta} \\Leftrightarrow\\) \\(\\mathbf{I} \\mathbf{y} - \\mathbf{B} \\mathbf{y} = \\boldsymbol{\\zeta} \\Leftrightarrow\\) \\((\\mathbf{I} - \\mathbf{B}) \\mathbf{y} = \\boldsymbol{\\zeta} \\Leftrightarrow\\) (premultiply both sides by \\((\\mathbf{I} - \\mathbf{B})^{-1}\\) \\((\\mathbf{I} - \\mathbf{B})^{-1} (\\mathbf{I} - \\mathbf{B}) \\mathbf{y} = (\\mathbf{I} - \\mathbf{B})^{-1} \\boldsymbol{\\zeta} \\Leftrightarrow\\) \\(\\mathbf{y} = (\\mathbf{I} - \\mathbf{B})^{-1} \\zeta\\) Derivation of Equation 3.09 \\(\\mathbf{\\Sigma} = \\mathrm{COV}(\\mathbf{y},\\mathbf{y}) \\Leftrightarrow\\) (substitute Equation 3.08) \\(\\mathbf{\\Sigma} = \\mathrm{COV}( ( \\mathbf{I} - \\mathbf{B})^{-1} \\boldsymbol{\\zeta}, (\\mathbf{I} - \\mathbf{B})^{-1} \\boldsymbol{\\zeta}) \\Leftrightarrow\\) \\(\\mathbf{\\Sigma} = ( \\mathbf{I} - \\mathbf{B})^{-1} \\mathrm{COV}(\\boldsymbol{\\zeta}, \\boldsymbol{\\zeta}) ( \\mathbf{I} - \\mathbf{B}) ^{-1\\mathrm{T}} \\Leftrightarrow\\) (variances and covariances of \\(\\boldsymbol{\\zeta}\\) are denoted \\(\\boldsymbol{\\Psi}\\)) \\(\\mathbf{\\Sigma} = ( \\mathbf{I} - \\mathbf{B})^{-1} \\boldsymbol{\\Psi} ( \\mathbf{I} - \\mathbf{B})^{-1\\mathrm{T}}\\) "],["standard-errors-and-confidence-intervals.html", "4 Standard Errors and Confidence Intervals 4.1 Standard Errors 4.2 Confidence Intervals 4.3 Obtaining standard errors and confidence intervals in lavaan 4.4 References", " 4 Standard Errors and Confidence Intervals 4.1 Standard Errors When the estimates of the parameter in a path model are obtained, and one assumes that the model is correct, researchers usually want to know whether the model parameters are statistically significantly different from zero. One way to judge the significance of a parameter estimate is to look at the associated standard error (\\(SE\\)). For example, we found that the effect of over-control on child anxiety was 0.072, with associated \\(SE\\) of 0.026. Although the interpretation of \\(SE\\)s and using them for judging statistical significance in structural equation modelling is no different than in any other statistical procedures (e.g., testing the difference between means in a \\(t\\)-test, or testing the significance of a regression coefficient in regression analysis), we shortly explain the procedure here. The \\(SE\\) reflects the standard deviation (\\(SD\\)) of the sampling distribution of the estimate. We obtained the parameter estimate of 0.072 based on a sample of 77 childparent dyads. These 77 childparent dyads are assumed to be a random sample from the much larger population of childparent dyads. If we would have drawn another randomly selected sample of \\(N\\) = 77 from the same population, we would have probably obtained a slightly different estimate of the same effect. Imagine that we would draw all possible random samples of \\(N\\) = 77 from the population, fit the path model to each sample, and collect the estimated effect of interest. The obtained estimates would follow a normal distribution, called the sampling distribution. The mean of this distribution is assumed to be equal to the population value. The smaller the \\(SD\\), the smaller the range of the estimates from the sampling distribution and the closer a sample estimate is expected to be to the population value (i.e., the estimate is more precise). With larger samples, the estimates are assumed to vary less across samples, and the \\(SD\\) of the sampling distribution will be smaller. Of course, it is impossible to draw all possible samples and observe the sampling distribution directly. Instead, we rely on estimates of the sampling distribution. The \\(SE\\) of an estimated direct effect in one sample, is an estimate of the \\(SD\\) of the sampling distribution of that direct effect. The \\(SE\\)s are often given in the output of SEM programs by default. For example, the parameter estimates with SEs for the anxiety example are given in Table 1. Parameter Estimate SE z p (&lt;|z|) b11 0.187 0.140 1.341 .180 b32 0.210 0.096 2.194 .028 b43 0.072 0.026 2.741 .006 p21 53.360 16.481 3.238 .001 p11 91.580 14.856 6.164 &lt; .001 p22 194.320 31.523 6.164 &lt; .001 p33 114.157 18.519 6.164 &lt; .001 p44 6.880 1.116 6.164 &lt; .001 \\(SE\\)s can be used to judge the statistical significance of parameter estimates. This is done by taking the ratio of the parameter estimate over its estimated \\(SE\\), which asymptotically follows a standard normal (\\(z\\)) distribution (with small samples a t distribution, but we assume sufficiently large samples). The z values can be tested against critical values associated with a given \\(\\) level. For example, to test them against an \\(\\) level of .05 two-sided, you would compare them to the critical value of 1.96. If the ratio is larger than 1.96 or smaller than -1.96, the parameter estimate is significantly different from zero, at an \\(\\) level of .05. The last column of Table 1 gives the associated probability of finding the \\(z\\) value or larger, under the null-hypothesis that the parameter is zero. Using an \\(\\) level of .05, the effect of Parental Anxiety on over-control is not significantly larger than zero, but the effects of Perfectionism on over-control, and the effect of over-control on child anxiety are significantly larger than zero. Also, the association between parental anxiety and perfectionism is statistically significant. 4.2 Confidence Intervals The \\(SE\\)s can be used to calculate confidence intervals (CIs), which can also be used to judge significance of the unstandardized parameter estimates. The lower and upper bound of a CI around some parameter estimate is given by \\(Est\\) ± \\(Z_{crit}\\) × \\(SE\\), where \\(Est\\) is the parameter estimate, \\(Z_{crit}\\) is the critical z value given the desired significance level, and SE is the standard error of the parameter estimate. If the CI around a parameter estimate does not include 0, we can conclude that this parameter differs significantly from 0. When 0 is included in the interval, the parameter is not significantly different from 0. Suppose you want to calculate 95% CIs for the effect of over-control on child anxiety. The lower confidence limit is given by 0.072  1.96 × 0.026 = 0.021, and the upper confidence limit is given by 0.072 + 1.96 × 0.026 = 0.123. As zero is not in between 0.021 and 0.123, the parameter is considered statistically significant. Using CIs in this way to judge the statistical significance of parameters leads to the same conclusion as evaluating the p value against the \\(\\) level. However, CIs are more informative than \\(p\\) values, and are recommended over \\(p\\) values (Cumming, 2013). Specifically, the size of the CI gives more descriptive information of the effect, as it can be gives an indication of the precision of the estimated effect. The interpretation of a 95% CI is that if you would calculate the CI for all random samples from the sampling distribution, 95% of those intervals will contain the population value. So, given a CI obtained from one sample, there is a 95% chance that it includes the population value, and a 5% chance that it doesnt include the population value. Although the semantic difference seems subtle, it would be incorrect to say that the probability of the population value lying within a given confidence interval is 95%, because the population value is a fixed value, so we should not speak of the probability of a population value (unless speaking about posterior probabilities in Bayesian analyses). Rather, it is the method of calculating the CI that has a 95% probability of capturing a parameter. 4.2.1 Likelihood-based confidence intervals Likelihood-based CIs are another type of CI. They do not rely on the SEs, but are obtained through the likelihood of the model. After parameter estimates are obtained, for each parameter separately, the parameter is moved up (all other parameters held fixed) until the \\(^2\\) statistic is increased to exactly the critical \\(^2\\) value associated with the chosen  level (e.g., 3.84 with \\(\\) = .05). This value is the upper confidence limit. Next, the parameter is moved in the negative direction until the \\(^2\\) is increased with the same critical value. This is, the lower confidence limit. Two large benefits of likelihood-based CIs are that they do not necessarily have to be symmetrical around the parameter estimate (which SE-based confidence intervals are), and that they can be obtained from functions of transformed parameters or functions of parameters as well (Neale &amp; Miller, 1997). Likelihood-based confidence are available in OpenMx but not in lavaan. 4.2.2 Bootstrapped confidence intervals Another type of CI can be obtained using bootstrapping. In our path model example, the sample size is 77. Bootstrapping involves resampling (with replacement) samples of size 77 from the original sample. This treats the observed data as though they are an infinite population, so each original observation can be included multiple times in a resampled sample (i.e., bootstrapping mimics the process of repeatedly drawing samples from the same population). By fitting the path model to each bootstrapped sample, and saving the parameter estimate of interest from each sample, one obtains a sampling distribution of the parameter estimate, which can be used for statistical inference. Bootstrap CIs are particularly useful in situations where obtaining the correct \\(SE\\)s analytically is difficult, such as with small samples or when the sampling distribution of a parameter is unknown, or known to be nonnormal. Bootstrapping is only possible when analyzing the raw data instead of the covariance matrix. 4.3 Obtaining standard errors and confidence intervals in lavaan In the standard summary output of lavaan, the \\(SE\\)s of parameter estimates are given in the column after the parameter estimates, and the ratios of the parameter estimates over their SEs (Wald \\(z\\) value) is given in the next column. To request confidence intervals from the summary, use the argument ci = TRUE. The regression parameter output look like this: summary(AWmodelOut, ci = TRUE) ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## overcontrol ~ ## parnt_nx (b31) 0.187 0.140 1.341 0.180 -0.087 0.461 ## perfect (b32) 0.210 0.096 2.194 0.028 0.022 0.399 ## child_anx ~ ## ovrcntrl (b43) 0.072 0.026 2.741 0.006 0.021 0.124 One can also obtain this information from the parameterEstimates() function, which provides more flexibility. For example, we can ask for any width of the interval (although 99%, 95% and 90% are most common), and we can specify particular types of bootstrap CIs. The format is similar to the summary() output, but it is a data.frame, so the parameters are indicated by the left-hand side (lhs), operator (op), and right-hand side (rhs) of the equation used to specify a parameter in lavaan model syntax. For example, to calculate a 90% CI: parameterEstimates(AWmodelOut, level = .90) ## lhs op rhs label est se z pvalue ci.lower ## 1 overcontrol ~ parent_anx b31 0.187 0.140 1.341 0.180 -0.042 ## 2 overcontrol ~ perfect b32 0.210 0.096 2.194 0.028 0.053 ## 3 child_anx ~ overcontrol b43 0.072 0.026 2.741 0.006 0.029 ## 4 parent_anx ~~ parent_anx p11 91.580 14.856 6.164 0.000 67.144 ## 5 perfect ~~ perfect p22 194.320 31.523 6.164 0.000 142.469 ## 6 overcontrol ~~ overcontrol p33 114.157 18.519 6.164 0.000 83.696 ## 7 child_anx ~~ child_anx p44 6.880 1.116 6.164 0.000 5.044 ## 8 parent_anx ~~ perfect p21 53.360 16.481 3.238 0.001 26.251 ## ci.upper ## 1 0.417 ## 2 0.368 ## 3 0.116 ## 4 116.016 ## 5 246.171 ## 6 144.617 ## 7 8.716 ## 8 80.469 4.4 References Cumming, G. (2013). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. New York, NY: Routledge. Neale, M. C., &amp; Miller, M. B. (1997). The use of likelihood based confidence intervals in genetic models. Behavior Genetics, 27, 113120. "],["direct-indirect-and-total-effects.html", "5 Direct, Indirect, and Total Effects 5.1 Testing significance of indirect effects 5.2 Higher order indirect effects 5.3 Using matrix algebra to calculate total, total indirect and specific indirect effects 5.4 Calculating total, total indirect and specific indirect effects using lavaan output 5.5 Calculating specific indirect effects in lavaan", " 5 Direct, Indirect, and Total Effects In the path model from our illustrative example (see Figure 1), there is no arrow pointing from Parental Anxiety (PA) to Child anxiety (CA). This does not mean that the model implies that there is no effect of PA on CA. The effect runs through Parental Overcontrol (PO). PA has a direct effect on PO, which in turn has a direct effect on CA. PA thus has an indirect effect on CA, reflecting that the effect of PA on CA is mediated by PO. The size of the indirect effect equals the product of the direct effects that constitute the indirect effect. So, in this example, the indirect effect of PA on CA through PO is equal to .187 (\\(_{31}\\)) times .072 (\\(_{43}\\)) = .013. Note that it makes sense that the indirect effect equals the product of the direct effects. Holding PP constant, if PA increases with one point, PO is expected to increase with .187 point. If PO increases with 1 point, CA is expected to increase with .072 point. So if PO increases with .187 points instead of 1 point, CA is expected to increase with .187 * .072 = .013 points. Similarly, the indirect effect of Parental Perfectionism (PP) on CA is .210 * .072 = .015, indicating that 1 point increase in PP is expected to result in .015 point increase of CA. Figure 5.1. Full Mediation Model If an effect of one variable on the other is only indirect, this reflects full mediation. Very often, there is also a direct effect next to the indirect effect, reflecting that the effect is not fully mediated, but partially mediated. Figure 5.2 shows the path model example of the partial mediation model (i.e., with direct effects of PA and PP on CA) and the associated parameter estimates. The direct effect of PA on CA is .091. This is the part of the total effect that is not mediated by PO. The total effect of PA on CA is the sum of the indirect effect(s) and the direct effect: .091 + .187 * .058 = .102. Total effects thus are the sum of all indirect and direct effects of one variable on another. This is an example of consistent mediation, i.e., the direct effect and the indirect have the same sign (they are both positive in this case). It could also be that there is inconsistent mediation, where the direct effect and indirect have opposite signs. In our example, there is inconsistent mediation of the effect of PP on CA as the direct effect is negative while the indirect effect is positive. Inconsistent mediation could therefore lead to the situation in which the direct effect and indirect effect cancel each other out. Figure 5.2. Partial Mediation Model 5.1 Testing significance of indirect effects Testing the significance of indirect effects can be done in the same way as for testing the significance of direct effects, using the standard error (\\(SE\\)) of the estimated effect. However, because the indirect effect is the product of two direct effects, the sampling distribution of the estimated indirect effects are complex and the associated SEs are unknown. Sobel (1982) suggested to use the so-called delta-method (Rao, 1973; Oehlert, 1992) to obtain an approximation of the standard errors for unstandardized indirect effects. Suppose a is the effect of the predictor on the mediator, and b is the effect of the mediator on the outcome variable, then the estimated standard error for the indirect effect \\(ab\\) is: \\[\\begin{equation} SE_{ab} = \\sqrt{b^2SE^2_a + a^2SE^2_b} \\tag{5.1} \\end{equation}\\] When the sample size is large, the ratio \\(ab\\) / \\(SEab\\) can be used as the \\(z\\) test of the unstandardized indirect effect. This is called the Sobel-test (Sobel, 1982). With small samples, if the raw data is available, it is preferred to bootstrap confidence intervals around indirect effects (Preacher &amp; Hayes, 2008). If raw data is not available, bootstrapping is not an option. In this case one could use likelihood based confidence intervals (Cheung, 2008). Likelihood based confidence intervals are not available in lavaan. lavaan will provide standard errors based on the delta-method. Table 1 gives an overview of the direct, indirect and total effects of PA and PP on CA. In our example, none of the indirect effects are significant. Table 1. Direct, total indirect and total effects on Child Anxiety with standard errors, z-values and p-values and standardized values for the model from Figure 2. Parameter Estimate SE Z-value p(&gt;|z|) LB 95%CI UB 95%CI Std. Effects from PA on CA Direct .091 .033 2.771 .006 .027 .155 .317 Total indirect .011 .009 1.148 .251 -.008 .029 .038 Total .102 .033 3.047 .002 .036 .168 .355 Effects from PP on CA Direct -.015 .023 -.635 .525 -.060 .030 -.074 Total indirect .012 .008 1.552 .121 -.003 .028 .062 Total -.002 .023 -.103 .918 -.047 .043 -.012 5.2 Higher order indirect effects In our example there are only two indirect effects, but there may be more complex indirect effects. Consider for example the model that was hypothesized by Affrunti and Woodruff-Borden (2014) in Figure 5.3. In the model of Figure 5.3, there are two indirect effects from PA on CA. One path goes from PA to PO and then from PO to CA, and is equal to \\(_{31} \\times _{43}\\). The other indirect effect goes from PA to PP, from PP to PO, and from PO to CA, and is calculated by \\(_{21} \\times _{32} \\times _{43}\\). Because this effect is build up of three direct effects, this is called a third order effect. The sum of all specific indirect effects is the total indirect effect. The total effect of PA on CA equals the sum of all indirect effects plus the direct effect (\\(_{41}\\)). This model also contains an indirect effect of PA on PO, which equals \\(_{21} \\times _{32}\\). Figure 5.3. The hypothesized model of Affrunti and Woodruff-Borden (2014) Judging the significance of higher order indirect effects can also be done using the delta-method and Sobel test described earlier, but the calculations quickly become more difficult. For example, the standard error of indirect effect abc can be calculated using \\(SE_{abc} = SE_{ab} \\times SE_{c}\\) using Equation (5.1). lavaan will provide standard errors based on the delta-method for all requested functions of parameters. Table 2 shows a decomposition of effects of PA on PO and of PA on CA. As can be seen in Table 2, none of the individual indirect effects, or the total indirect effects in the model are significantly larger than zero. The unstandardized total effect of PA on CA is estimated to be .101, and is statistically significant at \\(\\) = .05. As can be seen, the total effect can be attributed to the direct effect of PA on CA. The total effect of PA on PO is also statistically significant, although both the direct and the total indirect effects are not significant individually. Their combined effect leads to a total effect that is large enough to lead to statistical significance. Table 2. Direct, total indirect and total effects on CA and PO with standard errors, z-values and p-values and standardized values for path model from Figure 3. Parameter Estimate SE Z-value p(&gt;|z|) LB 95%CI UB 95%CI Std. Effects from PA on CA Direct .084 .031 2.271 .007 .023 .144 .292 Indirect via PP and PO .007 .005 1.409 .159 -.003 .016 .023 Indirect via PO .010 .009 1.133 .257 -.007 .028 .035 Total indirect .017 .011 1.563 .118 -.004 .038 .058 Total .101 .031 3.279 .001 .040 .161 .350 Effects from PA on PO Direct .187 .139 1.350 .177 -.085 .459 .157 Indirect via PP .123 .064 1.913 .056 -.003 .248 .103 Total indirect .123 .064 1.913 .056 -.003 .248 .103 Total .310 .131 2.363 .018 .053 .567 .260 After fitting a model, the estimated direct effects can be used to calculate the specific indirect effects, the total indirect effects and the total effects by hand. However, this is a tedious job. Therefore, it is often more attractive to use matrix algebra to obtain the indirect and total effects. 5.3 Using matrix algebra to calculate total, total indirect and specific indirect effects Instead of calculating all total effects by hand, one can use the model matrix \\(B\\) to calculate the total effects. In recursive models, the total effects in matrix \\(T\\) are given by: \\[\\begin{equation} T=\\sum _{k=1}^{p-1}{B}^{k} \\tag{5.2} \\end{equation}\\] where p is the number of variables. Thus, the total effects are calculated using the sum of powers of the matrix with regression coefficients (\\(B\\)), where the highest number of powers (\\(k\\)) is equal to the number of variables minus one (\\(p - 1\\)). For example, with four variables the total effects can be calculated using: \\(T = B^1 + B^2 + B^3\\). As \\(B^1 = B\\), the total effects can be decomposed into matrix B that contains all direct effects, and the sum of the powers of the \\(B\\) matrix that contain all indirect effects. Thus, the matrix with all indirect effects can be calculated by \\(T - B\\). In the anxiety example path model with higher order indirect effects the \\(B\\) matrix is: \\[ \\mathbf{B^1} = \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0 \\\\ \\beta_{21} &amp; 0 &amp; 0&amp;0\\\\ \\beta_{31} &amp; \\beta_{32} &amp; 0&amp;0 \\\\ \\beta_{41} &amp; 0 &amp; \\beta_{43} &amp; 0 \\end{bmatrix}. \\] which contains all direct effects (i.e., first-order effects). Multiplying matrix \\(B\\) with itself gives \\(B^2\\), which contains all second-order indirect effects: \\[ \\mathbf{B^2} = \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0 \\\\ 0 &amp; 0 &amp; 0&amp;0\\\\ \\beta_{21}\\beta_{32} &amp; 0 &amp; 0&amp;0 \\\\ \\beta_{31}\\beta_{43} &amp; \\beta_{32}\\beta_{43} &amp; 0 &amp; 0 \\end{bmatrix}. \\] Multiplying matrix \\(B\\) with itself three times gives the third-order indirect effects: \\[ \\mathbf{B^3} = \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0 \\\\ 0 &amp; 0 &amp; 0&amp;0\\\\ 0 &amp; 0 &amp; 0&amp;0 \\\\ \\beta_{21}\\beta_{32}\\beta_{43} &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] If we would continue and calculate \\(B^4\\) we will see that it contains only zeros (indicating that there cannot exist fourth-order indirect effects in the model with four variables). Similarly, if we would calculate the matrices of indirect effects for the example in Figure 2, matrix \\(B^3\\) would already be zero, because there are no third or higher order effects in this model. The sum of the direct effects and the indirect effects is equal to the total effects. For nonrecursive models (discussed in a later chapter), Equation (5.2) can often still be used to calculate total effects, but it is not necessarily adequate. That is, the number of times that the power of the matrix with regression coefficients has to be calculated (and summed) is not defined. The total effects are only defined when the power of the matrix with regression coefficients (\\(B^k\\)) converges to zero. 5.4 Calculating total, total indirect and specific indirect effects using lavaan output We will use the output of the anxiety model to compute a matrix that contains the total effects (\\(T\\)), a matrix that contains the total indirect effects (\\(U\\)) and illustrate the calculation of a specific indirect effects (I1). In recursive models, matrix \\(T\\) is calculated by addition of matrix B with multiplications of matrix \\(B\\), as often as the number of variables minus one. So, if there are 4 observed variables, the expression for \\(T\\) stops after \\(B\\) is multiplied with itself 3 times. \\(T\\) contains the total effects. Estimates &lt;- lavInspect(AWmodelOut, &quot;est&quot;) B &lt;- Estimates$beta[obsnames, obsnames] Tot &lt;- B + B %*% B + B %*% B %*% B Matrix \\(U\\) is calculated as the matrix with total effects, minus the matrix with direct effects. The result is a matrix with the total indirect effects. Please notice that if there are multiple indirect effects between two variables, matrix \\(U\\) has the sum of the indirect effects. U &lt;- Tot - B A single indirect effect is calculated by multiplying specific elements from matrix \\(B\\). For example, B[3,1] selects the element in row 3, column 1 from the \\(B\\) matrix, and this element is multiplied with element B[4,3]. This is the indirect effect of Parental anxiety on Child anxiety via Overcontrol (see Figure 1). We gave this effect the name I1, because you may want to calculate more indirect effects, which could then be named I2, I3, I4 etc. I1 &lt;- B[3,1] * B[4,3] The resulting matrices are shown by asking for the result: Tot ## parent_anx perfect overcontrol child_anx ## parent_anx 0.00000000 0.00000000 0.00000000 0 ## perfect 0.00000000 0.00000000 0.00000000 0 ## overcontrol 0.18735753 0.21049095 0.00000000 0 ## child_anx 0.01354201 0.01521407 0.07227898 0 U ## parent_anx perfect overcontrol child_anx ## parent_anx 0.00000000 0.00000000 0 0 ## perfect 0.00000000 0.00000000 0 0 ## overcontrol 0.00000000 0.00000000 0 0 ## child_anx 0.01354201 0.01521407 0 0 I1 ## [1] 0.01354201 5.5 Calculating specific indirect effects in lavaan In lavaan we can also calculate total, total indirect and specific indirect effects. However, we cannot use matrix algebra but have to specify each effect individually. The script below shows how to calculate the specific indirect effect, total indirect effect and total effect of Parental Anxiety and Perfectionism on Child anxiety. In order to refer to the direct effects that make up the indirect effects, we have labelled the specific direct effects in the lavaan model, using [label]\\*[variable]. # specify the path model AWmodel &lt;- &#39;# regression equations overcontrol ~ b31*parent_anx + b32*perfect child_anx ~ b43*overcontrol + b41*parent_anx + b42*perfect # (residual) variance parent_anx ~~ p11*parent_anx perfect ~~ p22*perfect overcontrol ~~ p33*overcontrol child_anx ~~ p44*child_anx # covariance exogenous variables parent_anx ~~ p21*perfect # specific, total indirect and total effects of PA on CA I1_PA := b31*b43 total_ind_PA := I1_PA total_PA := total_ind_PA + b41 # specific, total indirect and total effects of PE on CA I1_PE := b32*b43 total_ind_PE := I1_PE total_PE := total_ind_PE + b42 &#39; We can define new variables that are a function of free parameter estimates by using the operator :=. We name the indirect effect of PA on CA I1_PA. If there would have been more mediators than Overcontrol, we could name other indirect effects I2_PA, I3_PA etc. The total indirect effect is calculated by summing the specific indirect effects. In this example there is just one specific indirect effect, so the total indirect effect is equal to the specific indirect effect. We used similar code for the indirect, total indirect and total effect of PE on CA. I1_PA := b31*b43 total_ind_PA := I1_PA total_PA := total_ind_PA + b41 The summary output of lavaan will give the result of the calculations, including a standard error of the estimate (using the delta-method) and the associated \\(z\\)-statistic. This provides a test of significance for the indirect ant total effects. When you request standardized output these are also given for the indirect effects. The results are added at the end of the output under Defined parameters: Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) I1_PA 0.011 0.010 1.140 0.254 total_ind_PA 0.011 0.010 1.140 0.254 total_PA 0.102 0.034 3.027 0.002 I1_PE 0.012 0.008 1.542 0.123 total_ind_PE 0.012 0.008 1.542 0.123 total_PE -0.002 0.023 -0.102 0.918 "],["calculating-standardized-parameter-estimates.html", "6 Calculating Standardized Parameter Estimates 6.1 Standardized regression coefficients 6.2 Standardized residual factor variances and covariances 6.3 Calculating standardized coefficients in R using lavaan results 6.4 Request standardized output with lavaan 6.5 Standardizing indirect and total effects", " 6 Calculating Standardized Parameter Estimates 6.1 Standardized regression coefficients The \\(B\\) matrix from the path analysis model in Chapter 3 contains unstandardized parameter estimates. Unstandardized parameters are dependent on the units in which the variables are scaled. When predictor variables are measured on the same scale, comparison of unstandardized coefficients can provide information on their relative influence on the outcome. For example, when we would estimate the effect of the anxiety of the father and the anxiety of the mother on the anxiety of the child, the unstandardized effects can be directly compared to give an indication of the relative influence of mothers and fathers anxiety. However, comparison of effects is complicated when variables are measured on different scales. In our illustrative example, a meaningful comparison of the effects of parent anxiety and parent perfectionism on parental over-control is complicated by the fact that the interpretation of units on the scales of these variables are not equivalent (e.g., how does one unit on parent anxiety relate to one unit on parent perfectionism?). In such a case, it would be helpful to obtain standardized parameter estimates that are independent on the units in which the variables are scaled. The standardized \\(\\) is the regression coefficient that is scaled with respect to the \\(SD\\) of each variable. For example, \\(\\widehat{\\beta}_{21}\\) is the estimated regression coefficient representing the direct effect of variable 1 on variable 2. The standardized version of \\(\\widehat{\\beta}_{21}\\), \\(\\widehat{\\beta}^*_{21}\\), is given as: \\[\\begin{equation} \\widehat{\\beta}^*_{21} = \\widehat{\\beta}_{21} \\times \\frac{\\widehat{\\sigma}_1}{\\widehat{\\sigma}_2}, \\tag{6.1} \\end{equation}\\] where \\(\\widehat{\\sigma}_1\\) and \\(\\widehat{\\sigma}_2\\) are the estimated \\(SD\\)s of variable 1 and variable 2 as predicted by the model.The standardized parameter can be interpreted in terms of standardized units, where a single SD increase in the predictor variable (variable 1) would result in a change of \\(\\widehat{\\beta}^*_{21}\\) \\(SD\\)s in the outcome variable (variable 2). Substituting the parameter estimate \\(\\widehat{\\beta}_{31}\\) and model-implied \\(SD\\)s \\({\\widehat{\\sigma}_1}\\) and \\({\\widehat{\\sigma}_3}\\) from our illustrative example yields: \\[ \\widehat{\\beta}^*_{31} = 0.19 \\times \\frac{\\sqrt{91.58}}{\\sqrt{130.19}} = .16 . \\] Thus, controlling for the other variables in the model, a 1-SD increase in parent anxiety is expected to result in parental over-control increasing 0.16 SDs. In comparison, the standardized regression coefficient for the regression of parental over-control on parent perfectionism is: \\[ \\widehat{\\beta}^*_{32} = \\widehat{\\beta}_{32} \\times \\frac{\\widehat{\\sigma}_2}{\\widehat{\\sigma}_3} = 0.21 \\times \\frac{\\sqrt{194.32}}{\\sqrt{130.19}} = .26 . \\] The relative influence of parent perfectionism on parental over-control is thus larger than the influence of parent anxiety. In addition, standardized regression coefficients can be used to interpret the size of the effect (in terms of effect size \\(r\\)), where values between .10.30 indicate small effects, values between .30.50 indicate intermediate effect, and values larger than .50 indicate strong effects (Bollen, 1988). In our example both effects can be considered small. The calculation of standardized regression coefficients can be done for the complete matrix of estimated regression coefficients that are represented in matrix \\(\\widehat{B}\\). The matrix expression for the standardized \\(\\widehat{B}\\) matrix, \\(\\widehat{B}^*\\), is the following: \\[\\begin{equation} \\widehat{B}^* = diag(\\widehat{\\Sigma})^{-\\frac{1}{2}} \\widehat{B} \\hspace{1mm} diag(\\widehat{\\Sigma})^\\frac{1}{2} \\tag{6.2} \\end{equation}\\] In this expression,  model is the matrix containing the model-implied variances and covariances. As an alternative, the sample matrix of (co)variances could be used. Often, the sample variances and the model-implied variances are equal. However, in some situations the constraints imposed on the model can cause differences between the model-implied and sample variances. For reasons of consistency, we will always use the variances as implied by the model. In our example, the matrix of standardized regression coefficients is: \\[ \\widehat{B}^* = \\begin{bmatrix} 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ \\widehat{\\beta}_{31}^* &amp; \\widehat{\\beta}_{32}^* &amp;0&amp;0\\\\ 0&amp;0&amp;\\widehat{\\beta}_{43}^* &amp;0\\\\ \\end{bmatrix} = \\begin{bmatrix} 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0.16 &amp; 0.26 &amp;0&amp;0\\\\ 0&amp;0&amp; 0.30 &amp;0\\\\ \\end{bmatrix}, \\] where it can be seen that the standardized coefficient of the regression of over-control on parental anxiety is relatively small compared to the other coefficients. 6.2 Standardized residual factor variances and covariances Similarly, the matrix of residual factor variances and covariances can be standardized to obtain parameter estimates that are independent on the units in which the variables are measured. A standardized covariance is a correlation. If \\(COV_{21}\\) is the covariance between variable 2 and variable 1, the correlation \\(COR_{21}\\) is calculated as: \\[\\begin{equation} COR_{21} = \\frac{COV_{21}}{\\widehat{\\sigma}_1 \\widehat{\\sigma}_2} \\tag{6.3} \\end{equation}\\] For example, the correlation between the variables parent anxiety and parent perfectionism is: \\[ COR_{21} = \\frac{COV_{21}}{\\widehat{\\sigma}_1 \\widehat{\\sigma}_2} = \\frac{53.36}{\\sqrt{91.58 \\times 194.32}} = .40. \\] Like standardized regression slopes, correlation coefficients can be interpreted as effect size \\(r\\) (Bollen, 1988). Thus, in our illustrative example there is an intermediate correlation between the two exogenous variables. The matrix expression for standardized variances and covariances in the \\(\\widehat{}\\) matrix is: \\[\\begin{equation} \\widehat{}^* = diag(\\widehat{\\Sigma}_{model})^{-\\frac{1}{2}} \\widehat{} \\hspace{1mm} diag(\\widehat{\\Sigma}_{model})^\\frac{1}{2} \\tag{6.4} \\end{equation}\\] In our example, the matrix of standardized residual variances and covariances is: \\[ \\widehat{\\Psi}^* = \\begin{bmatrix} \\widehat{\\psi}_{11}^* &amp; \\widehat{\\psi}_{21}^*&amp;0&amp;0\\\\ \\widehat{\\psi}_{12}^* &amp; \\widehat{\\psi}_{22}^*&amp;0&amp;0\\\\ 0&amp;0 &amp;\\widehat{\\psi}_{33}^*&amp;0\\\\ 0&amp;0&amp;0 &amp;\\widehat{\\psi}_{44}^*\\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp;0.40 &amp; 0 &amp; 0\\\\ 0.40 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0.88 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0.91\\\\ \\end{bmatrix}. \\] The standardized variances of the residual factors can be interpreted as the proportion of unexplained variance. As the variances of the residual factors of exogenous variables are equal to the variances of the observed variables, their standardized value is 1, i.e., 100% of the variance of exogenous variables is unexplained by the model. For endogenous variables, the residual factor contains only that part of the variance of the observed variable that is not explained by the model. In our example, the proportion of unexplained variance of child anxiety is .91. This indicates that 91% of the variance of child anxiety cannot be explained by the specified model. Or, in other words, the model explains only 9% of the variance of child anxiety. Other variables, not included in the model, determine the remaining variance of child anxiety. 6.3 Calculating standardized coefficients in R using lavaan results There are several ways to obtain standardized parameter estimates from a path model. Here, we will show how to manually calculate standardized parameter estimates from the estimated (\\(\\widehat{B}\\) \\(\\widehat{\\Psi}\\)) and model-implied (\\(\\widehat{\\Sigma}\\)) matrices. Script 6.1 shows the syntax that standardizes the \\(\\widehat{B}\\) and \\(\\widehat{\\Psi}\\)matrices (\\(\\widehat{B}^*\\) and \\(\\widehat{\\Psi}^*\\)) from the output of the Affrunti &amp; Woodruff-Borden model that was saved in the object AWmodelOut. # parameter estimates from the model Estimates &lt;- lavInspect(AWmodelOut, &quot;est&quot;) BETA &lt;- Estimates$beta[obsnames, obsnames] PSI &lt;- Estimates$psi[obsnames, obsnames] # calculate model-implied covariance matrix IDEN &lt;- diag(1, nrow = 4) SIGMA &lt;- solve(IDEN - BETA) %*% PSI %*% t(solve(IDEN - BETA)) # calculate standard deviations of model-implied variances SD &lt;- diag(sqrt(diag(SIGMA))) # calculate standardized parameter estimates BETAstar &lt;- solve(SD) %*% BETA %*% SD PSIstar &lt;- solve(SD) %*% PSI %*% solve(SD) # give labels to the new matrices dimnames(BETAstar) &lt;- list(obsnames, obsnames) dimnames(PSIstar) &lt;- list(obsnames, obsnames) First, the unstandardized estimates for the \\(\\) and \\(\\) parameters are collected in the objects BETA and PSI, respectively, with the commands: Estimates &lt;- lavInspect(AWmodelOut, &quot;est&quot;) BETA &lt;- Estimates$beta[obsnames, obsnames] PSI &lt;- Estimates$psi[obsnames, obsnames] We also need the model-implied covariance matrix, \\(\\widehat\\) model, which we can calculate with the parameter estimates, using \\(\\widehat{\\Sigma}_{model} = (I-\\widehat{B})^{-1} \\widehat{\\Psi}[(I-\\widehat{B})^{-1}]^T\\). We already extracted the parameter estimates for the \\(\\widehat{B}\\) and \\(\\widehat{\\Psi}\\) matrices (into the objects BETA and PSI), so now we create an identity matrix IDEN with dimensions 4 × 4 and then calculate \\(\\widehat_{model}\\) using: SIGMA &lt;- solve(IDEN - BETA) %*% PSI %*% t(solve(IDEN - BETA)) Note, however, that you can also request the model-implied covariance matrix directly from lavaan: SIGMA &lt;- lavInspect(AWmodelOut, &quot;cov.ov&quot;)[obsnames, obsnames] For our calculations we only need the \\(SD\\)s, which we obtain by extracting the diagonal of SIGMA using the diag() function, then take the square-root of each element using diag(). Notice that the diag() function can also create a diagonal matrix, such as when we used it to create an identity matrix IDEN, in which case it needs the value to put on the diagonal and the number of rows (or columns). In the case of \\(SD\\), sqrt(diag(SIGMA)) is a vector with 4 elements, so diag() creates a matrix of 4 rows and columns with sqrt(diag(SIGMA)) on its diagonal. SD &lt;- diag(sqrt(diag(SIGMA))) The \\(\\widehat{B}\\) matrix is standardized by pre-multiplying \\(\\widehat{B}\\) with a diagonal matrix with the inverse of the \\(SD\\)s, and post-multiplying it with a diagonal matrix with the \\(SD\\)s: BETAstar &lt;- solve(SD) %*% BETA %*% SD The matrix \\(\\widehat{\\Sigma}\\) is standardized by pre- and post-multiplying \\(\\widehat{\\Sigma}\\) with a diagonal matrix with the inverse of the standard deviations. PSIstar &lt;- solve(SD) %*% PSI %*% solve(SD) The standardized variances and covariances (correlations) are collected in the object PSIstar. The standardized direct effects are in the object BETAstar. The same labels that were given to the matrices with unstandardized estimates, can also be used for the standardized parameter estimates: dimnames(BETAstar) &lt;- list(obsnames, obsnames) dimnames(PSIstar) &lt;- list(obsnames, obsnames) To view the standardized \\(\\widehat{B}^*\\) and \\(\\widehat{\\Sigma}^*\\) matrices, type: BETAstar ## parent_anx perfect overcontrol child_anx ## parent_anx 0.0000000 0.00000 0.0000000 0 ## perfect 0.0000000 0.00000 0.0000000 0 ## overcontrol 0.1571385 0.25716 0.0000000 0 ## child_anx 0.0000000 0.00000 0.2999438 0 PSIstar ## parent_anx perfect overcontrol child_anx ## parent_anx 1.000000 0.399997 0.0000000 0.0000000 ## perfect 0.399997 1.000000 0.0000000 0.0000000 ## overcontrol 0.000000 0.000000 0.8768487 0.0000000 ## child_anx 0.000000 0.000000 0.0000000 0.9100337 6.4 Request standardized output with lavaan The same standardized matrices can be extracted using lavaans build-in functions: lavInspect(AWmodelOut, &quot;std.all&quot;) ## $lambda ## ovrcnt chld_n prnt_n perfct ## overcontrol 1 0 0 0 ## child_anx 0 1 0 0 ## parent_anx 0 0 1 0 ## perfect 0 0 0 1 ## ## $theta ## ovrcnt chld_n prnt_n perfct ## overcontrol 0 ## child_anx 0 0 ## parent_anx 0 0 0 ## perfect 0 0 0 0 ## ## $psi ## ovrcnt chld_n prnt_n perfct ## overcontrol 0.877 ## child_anx 0.000 0.910 ## parent_anx 0.000 0.000 1.000 ## perfect 0.000 0.000 0.400 1.000 ## ## $beta ## ovrcnt chld_n prnt_n perfct ## overcontrol 0.0 0 0.157 0.257 ## child_anx 0.3 0 0.000 0.000 ## parent_anx 0.0 0 0.000 0.000 ## perfect 0.0 0 0.000 0.000 You can also request standardized estimates when you use the summary() or parameterEstimates() functions with the argument standardized = TRUE: summary(AWmodelOut, standardized = TRUE) ## lavaan 0.6-11 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Number of observations 77 ## ## Model Test User Model: ## ## Test statistic 7.333 ## Degrees of freedom 2 ## P-value (Chi-square) 0.026 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## overcontrol ~ ## parnt_nx (b31) 0.187 0.140 1.341 0.180 0.187 0.157 ## perfect (b32) 0.210 0.096 2.194 0.028 0.210 0.257 ## child_anx ~ ## ovrcntrl (b43) 0.072 0.026 2.741 0.006 0.072 0.300 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## parent_anx ~~ ## perfect (p21) 53.360 16.481 3.238 0.001 53.360 0.400 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## parnt_nx (p11) 91.580 14.856 6.164 0.000 91.580 1.000 ## perfect (p22) 194.320 31.523 6.164 0.000 194.320 1.000 ## .ovrcntrl (p33) 114.157 18.519 6.164 0.000 114.157 0.877 ## .child_nx (p44) 6.880 1.116 6.164 0.000 6.880 0.910 parameterEstimates(AWmodelOut, standardized = TRUE) ## lhs op rhs label est se z pvalue ci.lower ## 1 overcontrol ~ parent_anx b31 0.187 0.140 1.341 0.180 -0.087 ## 2 overcontrol ~ perfect b32 0.210 0.096 2.194 0.028 0.022 ## 3 child_anx ~ overcontrol b43 0.072 0.026 2.741 0.006 0.021 ## 4 parent_anx ~~ parent_anx p11 91.580 14.856 6.164 0.000 62.462 ## 5 perfect ~~ perfect p22 194.320 31.523 6.164 0.000 132.536 ## 6 overcontrol ~~ overcontrol p33 114.157 18.519 6.164 0.000 77.861 ## 7 child_anx ~~ child_anx p44 6.880 1.116 6.164 0.000 4.692 ## 8 parent_anx ~~ perfect p21 53.360 16.481 3.238 0.001 21.058 ## ci.upper std.lv std.all std.nox ## 1 0.461 0.187 0.157 0.157 ## 2 0.399 0.210 0.257 0.257 ## 3 0.124 0.072 0.300 0.300 ## 4 120.698 91.580 1.000 1.000 ## 5 256.104 194.320 1.000 1.000 ## 6 150.453 114.157 0.877 0.877 ## 7 9.067 6.880 0.910 0.910 ## 8 85.662 53.360 0.400 0.400 This will add two columns to the output, the column std.lv gives standardized parameters when only the exogenous variables are standardized, and the column std.all gives standardized parameters when both exogenous and endogenous variables are standardized. The latter one will give equivalent output as calculated above. You can also request standardized estimates using the standardizedSolution() function, which also provides \\(SE\\)s for the standardized estimates themselves. However, we recommend only testing the unstandardized estimates for making inferences about population parameters (e.g., null-hypothesis significance tests), and using standardized estimates only as a standardized measure of effect size. 6.5 Standardizing indirect and total effects Standardized indirect effects are obtained by multiplying the standardized direct effects instead of the unstandardized direct effects. Alternatively, one can standardize an indirect effect using formulas (6.1) and (6.2) replacing the estimated regression coefficient with the calculated indirect or total effect of interest. Note that the standard deviations of the mediating variables do not play a role in standardizing indirect or total effects. "],["identification.html", "7 Identification 7.1 Assessment of identification using the elements of \\(_{population}\\) and \\(\\Sigma_{model}\\) 7.2 Assessing identification through heuristics (rules of thumb) 7.3 Assessment of empirical model identification using lavaan 7.4 Appendix I", " 7 Identification The problem of identification considers the issue of whether there is enough known information to obtain unique estimates of the model that is specified (i.e., the unknown information). In structural equation modeling, the known pieces of information are the sample variances and covariances of the observed variables, and the unknown pieces of information are the model parameters. When the model is under identified, this means that there is not enough known information to yield unique parameter estimates. When there are just as many knowns as unknowns, the model is just identified. However, usually we are interested in so called over identified models, where there are more knowns than unknowns. Identification is an important but rather difficult topic of structural equation modeling, and there are many different ways to evaluate the identification status of a model. The first, and easiest way to evaluate whether a model might be identified is to evaluate whether the number of model parameters to be estimated is equal to or smaller than the number of nonredundant elements in the observed covariance matrix: \\[\\begin{equation} q \\le (p(p+1))^1/_2, \\tag{7.1} \\end{equation}\\] where p is the number of observed variables, and \\(q\\) is the number of free parameters in the model. This is a general rule that can be applied to all structural equation models. However, it is a necessary but not sufficient condition for identification. This means that models that have a negative number of degrees of freedom are surely under identified. Models that have at least zero degrees of freedom might be identified. Evaluation of Equation (7.1) for our illustrative example of child anxiety yields a total of 10 knowns (i.e., \\(p = 4\\)), and 8 unknowns (i.e., \\(\\beta_{31}\\), \\(\\beta_{32}\\), \\(\\beta_{43}\\), \\(\\psi_{11}\\), \\(\\psi_{22}\\), \\(\\psi_{33}\\), \\(\\psi_{44}\\) and \\(\\psi_{21}\\)). The difference between the number or nonredundant elements in the covariance matrix and the number of free parameters in the model are the degrees of freedom. The model of our example thus has 2 degrees of freedom. There are different approaches to further assess whether a model is identified. In this chapter we will first discuss the symbolic or theoretical identification of model parameters. In addition, we will explain some rules of thumb that can be applied in order to facilitate the evaluation of model identification. We only discuss identification of path models, identification of factor models will be discussed in a later chapter. Some conditions related to identification are sufficient, which means that if the condition holds, the model is surely identified (at least theoretically), but there may be other identified models for which the condition does not hold. Other conditions are necessary, which means that if the condition holds, the model might be identified. If a necessary condition doesnt hold, the model is surely not identified. 7.1 Assessment of identification using the elements of \\(_{population}\\) and \\(\\Sigma_{model}\\) A model is identified only when all unknown parameters of the model are identified. Identification of model parameters can be demonstrated by showing that the unknown parameters are functions of known parameters, i.e., that we can give a description of the free parameters in the model in terms of population variances and covariances (\\(_{population}\\)). The population variances and covariances are known-to-be-identified (i.e., known information) because the observed sample variances and covariances (\\(_{sample}\\)) are direct estimates of \\(_{population}\\). Therefore, if an unknown parameter can be written as a function of one or more elements of \\(_{population}\\), then that parameter is identified. As an example, we take \\(_{model}\\) from Equation (3.10). Here, the elements of the covariance matrix are written as a function of model parameters. In order to evaluate the identification status of a parameter, we need to write the equation the other way around, so that the model parameter is a function of population variances and covariances. If each unknown model parameter can be written as a function of known parameters, then the model of Equation (3.10) is identified. For example, the first three elements of the model implied covariance matrix are a function of single model parameters \\(_{11}\\), \\(_{22}\\), and \\(_{21}\\). These parameters can therefore be represented by the population variances \\(\\sigma_{11}\\), \\(\\sigma_{22}\\), and covariance \\(\\sigma_{21}\\): \\[\\begin{equation} \\psi_{11} = \\sigma_{11} \\tag{7.2} \\end{equation}\\] \\[\\begin{equation} \\psi_{22} = \\sigma_{22} \\tag{7.3} \\end{equation}\\] \\[\\begin{equation} \\psi_{21} = \\sigma_{21} \\tag{7.4} \\end{equation}\\] These equations thus have the same number of knowns as unknowns, which leads to a unique expression for each model parameter. The parameters \\(_{11}\\), \\(_{22}\\), and \\(_{21}\\) are thus identified. The equations for the population covariances \\(\\sigma_{31}\\) and \\(\\sigma_{32}\\) are: \\[\\begin{equation} \\sigma_{31} = \\beta_{31}\\psi_{11} + \\beta_{32}\\psi_{21} \\tag{7.5} \\end{equation}\\] \\[\\begin{equation} \\sigma_{32} = \\beta_{31}\\psi_{32} + \\beta_{32}\\psi_{22} \\tag{7.6} \\end{equation}\\] Given Equations (7.2)  (7.4), both equations have two unknowns, \\(\\beta_{31}\\) and \\(\\beta_{32}\\). Because both equations share the same two unknowns, we can rewrite the equations such that the model parameters \\(\\beta_{31}\\) and \\(\\beta_{32}\\) are on the left side of the equations and use this information to yield unique functions for each model parameter in terms population parameters. First, we rewrite Equations (7.5) and (7.6): \\[\\begin{equation} \\beta_{31} = \\frac{\\sigma_{31}-\\beta_{32}\\psi_{21}}{\\psi_{11}} \\tag{7.7} \\end{equation}\\] \\[\\begin{equation} \\beta_{32} = \\frac{\\sigma_{32}-\\beta_{31}\\psi_{21}}{\\psi_{22}} \\tag{7.8} \\end{equation}\\] Substituting Equation (7.8) into Equation (7.7) gives an equation with only one unknown: \\[\\begin{equation} \\beta_{31} = \\frac{\\sigma_{31}-\\frac{\\sigma_{32}-\\beta_{31}\\psi_{21}}{\\psi_{22}}\\psi_{21}}{\\psi_{11}} \\tag{7.9} \\end{equation}\\] Rewriting gives (for intermediate steps, see Appendix I: \\[\\begin{equation} \\beta_{31} = \\frac{\\sigma_{31}\\psi_{22}-\\sigma_{32}\\psi_{21}}{\\psi_{11}\\psi_{22}-{\\psi_{21}}^2} \\tag{7.10} \\end{equation}\\] The expression for \\(_{31}\\) contains the already known to be identified parameters \\(_{11}\\), \\(_{22}\\), and \\(_{21}\\). Substituting Equations (7.2), (7.2) and (7.4) into Equation (7.10) gives: \\[\\begin{equation} \\beta_{31} = \\frac{\\sigma_{31}\\sigma_{22}-\\sigma_{32}\\sigma_{21}}{\\sigma_{11}\\sigma_{22}-{\\sigma_{21}}^2} \\tag{7.11} \\end{equation}\\] This shows that the parameter \\(_{31}\\) can be written as a function of population variances and covariances, and thus \\(_{31}\\) is identified. Similarly, substituting Equation (7.7) into (7.8) and rewriting gives: \\[\\begin{equation} \\beta_{32} = \\frac{\\sigma_{32}\\psi_{22}-\\sigma_{31}\\psi_{21}}{\\psi_{11}\\psi_{22}-{\\psi_{21}}^2} \\tag{7.12} \\end{equation}\\] and substituting Equations (7.2), (7.3) and (7.4) into Equation (7.12) gives: \\[\\begin{equation} \\beta_{32} = \\frac{\\sigma_{32}\\sigma_{22}-\\sigma_{31}\\sigma_{21}}{\\sigma_{11}\\sigma_{22}-{\\sigma_{21}}^2} \\tag{7.13} \\end{equation}\\] Therefore, using the equations for the population covariances \\(\\sigma_{31}\\) and \\(\\sigma_{32}\\), both \\(\\beta_{31}\\) and \\(\\beta_{32}\\) are identified. The \\(\\sigma_{33}\\), \\(\\sigma_{41}\\) and \\(\\sigma_{44}\\) equations then have just a single unknown each (\\(\\psi_{33}\\), \\(\\beta_{43}\\) and \\(\\psi_{44}\\)) which indicates that these model parameters are also identified. In Appendix I we demonstrate that the unknown parameters \\(\\psi_{33}\\), \\(\\beta_{43}\\) and \\(\\psi_{44}\\) can indeed be written as a function of known parameters. All model parameters of Equation (3.10) are identified and therefore the model of Equation (3.10) is identified. Moreover, we did not use the \\(\\sigma_{42}\\) and \\(\\sigma_{43}\\) equations yet, which shows that the model given by Equation (3.10) is over-identified. Specifically, we have two more nonredundant elements in the population covariance matrix than the number of unknown model parameters (i.e., the model has two degrees of freedom). The equations for \\(\\sigma_{41}\\),\\(\\sigma_{42}\\) and \\(\\sigma_{43}\\) can all be used to express \\(\\beta_{43}\\) in terms of population variances and covariances. To illustrate this over identification of \\(\\beta_{43}\\) these different equations are given in Appendix I. The expression of model parameters in terms of population variances and covariances can be used to arrive at a theoretical assessment of identification. However, a model that is theoretically identified might not necessarily be empirically identified. The theoretical identification of model parameter rests on the assumption that for each population parameter there is at least one estimate in terms of sample parameters. However, this assumption might be violated when some of the observed variables in the model exhibit very large or very small correlations. For example, when the sample covariance between two variables is zero, then the equations for the unknown parameters might involve division by zero, which is not possible. Therefore, researchers should always be aware of identification problems, even if the model is theoretically identified. 7.2 Assessing identification through heuristics (rules of thumb) Because the symbolic assessment of identification quickly becomes very complex as the model grows, researchers have developed a number of rules of thumb to assess identification. The most well-known rules of thumb for path models are the Recursive Rule, the Order Condition, and the Rank Condition. These rules are explained in detail on pages 98-104 of Bollen (1988), but below we give a brief description of each of these rules. 7.2.1 The recursive rule The recursive rule is a sufficient, but not necessary condition for model identification. The rule entails that all recursive models are identified. Recursive models contain only unidirectional causal effects and have uncorrelated residual factors of endogenous variables. This means that there are no feedback-loops in the model. Because the recursive rule is a sufficient rule, it is possible that the model is still identified even though not all causal effects are unidirectional or there exist correlations between the residual factors of the endogenous variables. 7.2.2 The order condition For identification of nonrecursive models we need additional rules. Where the previous rules apply the model as a whole, the order condition (and the rank condition) apply to the separate equations of the endogenous variables in the model (i.e., Equations (3.3) and (3.4) from our illustrative example). Each equation has to meet the condition for the model to be identified. One other difference with the previous conditions, is that for the order and rank conditions it is assumed that all residual covariances between the residual factors of endogenous variables are free to be estimated. The advantage of this assumption is that when all the equations of a model meet the rank and order conditions, than all the elements in \\(\\Psi\\) that correspond to the residual factors of endogenous variables are identified. However, usually we know that some of these elements of \\(\\Psi\\) are restricted to zero, and such restrictions may help in the identification of model parameters. This knowledge is not taken into account with the rank and order conditions. The order condition entails that for each equation of an endogenous variable, the number of endogenous variables excluded from the equation needs to be at least one less than the total number of endogenous variables (\\(g  1\\)). The order condition is a necessary but not sufficient condition. In order to evaluate the order condition, one can calculate matrix \\(C\\), which is \\((I  B_g | -B_x)\\). \\(B_g\\) refers to the part of the \\(B\\) matrix that contains the regression coefficients between the endogenous variables, and \\(B_x\\) refers to the part of the \\(B\\) matrix that contains the regression coefficient between the exogenous and endogenous variables. For example, the \\(C\\) matrix for our illustrative example of the development of child anxiety is: \\[ C = \\begin{bmatrix} 1&amp;0&amp; -\\beta_{31} &amp; -\\beta_{32}\\\\ -\\beta_{43} &amp; 1 &amp; 0&amp;0 \\end{bmatrix} \\] In order to evaluate the order condition, we need to count the number of zero elements for each row. If a row has more zeros than \\(g  1\\), then the associated equation meets the order condition. In our example, the row corresponding to the equation for y3 (parental overcontrol) has one zero, and the row corresponding to the equation for variable y4 (child anxiety) has two zeros. Our model contains a total of two endogenous variables, i.e. \\(g  1 = 1\\). Thus, the equations are identified. The order condition is a necessary but not sufficient condition, which means that even when each equation meets the condition it does not guarantee identification of the model. 7.2.3 The rank condition The rank condition is a necessary and sufficient condition, indicating that if each equation meets the criteria than the model is identified. If the equations do not meet the condition, than the model is surely not identified. To evaluate the rank condition we again look at the \\(C\\) matrix, but now delete all columns of \\(C\\) that do not have zeros in the row of the equation of interest. Using the remaining columns to form a new matrix, \\(C_i\\), we know that equation \\(i\\) is identified when the rank of \\(C_i\\) equals \\(g  1\\). When we calculate the \\(C_3\\) matrix for the equation that refers to the y3 variable from our illustrative example, we get: \\[ C_3 = \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}. \\] The rank of a matrix or vector is the number of independent rows (or columns). A row is independent when it contains nonzero elements, and is linearly independent from other rows in the matrix (e.g., when a row is the sum of two other rows of the same matrix, it is not linearly independent from the other rows). For C3 the rank is 1. Therefore, the equation for parental overcontrol is identified. The C4 matrix of our illustrative example is: \\[ C_4 = \\begin{bmatrix} -\\beta_{31}&amp;-\\beta_{32}\\\\ 0&amp;0 \\end{bmatrix}. \\] The rank of this matrix is also 1 and thus this equation is also identified. 7.3 Assessment of empirical model identification using lavaan The methods for assessment of identification as described above are all methods that cannot be readily evaluated with a computer program such as lavaan. One method that can be evaluated with software is the so-called empirical check, where the model of interest is fitted to the (user defined) population covariance matrix of the specified model. When the empirical solution is equal to the model in the population, the model may be identified. When the parameter estimates of the empirical solution are different from the parameters specified in the population, the model is surely not identified (i.e., the condition is necessary but not sufficient). The empirical check has two steps. Step 1: Calculate a model implied covariance matrix for the model of interest. Step 2: Fit the model to the model implied covariance matrix from Step 1. If the parameter estimates you obtain in Step 2 are identical to the parameter values you chose in Step 1, the model may be identified. If you obtain different parameter estimates in Step 2, your model is surely not identified. Below, we will show two examples of the empirical check. In Script 7.1 we will check identification of the path model depicted in Figure 1. First we will check identification if we add a covariance between the residual factors of \\(y_3\\) and \\(y_4\\) (Model 1), and subsequently we will check identification when we additionally add a reciprocal effect from \\(y_4\\) to \\(y_3\\) (Model 2). ### Script 7.1 ### ## STEP 1: calculate the model implied covariance matrix BETA &lt;- matrix(c( 0, 0, 0,0, 0, 0, 0,0, .5,.5, 0,0, 0, 0,.5,0), nrow=4,ncol=4,byrow=TRUE) PSI &lt;- matrix(c( 1,.5, 0, 0, .5, 1, 0, 0, 0, 0,.8,.5, 0, 0,.5,.7), nrow=4,ncol=4,byrow=TRUE) IDEN &lt;- diag(1,4,4) SIGMA &lt;- solve(IDEN-BETA) %*% PSI %*% t(solve(IDEN-BETA)) obsnames = c(&quot;y1&quot;,&quot;y2&quot;,&quot;y3&quot;,&quot;y4&quot;) dimnames(SIGMA) &lt;- list(obsnames,obsnames) Figure 7.1. Path model for which identification is assessed in the situation when effects \\(_{43}\\) (Model 1) and both \\(_{43}\\) and \\(\\beta_{34}\\) (Model 2) are added. ## STEP 2: fit model to the covariance matrix from step 1 EmpiricalCheck &lt;- &#39; # regressions y4 ~ b43*y3 y3 ~ b32*y2 + b31*y1 # residual (co)variances y1 ~~ p11*y1 y2 ~~ p22*y2 y3 ~~ p33*y3 y4 ~~ p44*y4 y3 ~~ p43*y4 # covariance exogenous variables y1 ~~ p21*y2 &#39; EmpiricalCheckOut &lt;- lavaan(EmpiricalCheck, sample.cov=SIGMA, sample.nobs=100, likelihood = &quot;wishart&quot;, fixed.x=FALSE) summary(EmpiricalCheckOut) ## lavaan 0.6-11 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 1 ## P-value (Chi-square) 1.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y4 ~ ## y3 (b43) 0.500 0.097 5.150 0.000 ## y3 ~ ## y2 (b32) 0.500 0.085 5.906 0.000 ## y1 (b31) 0.500 0.085 5.906 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y4 ~~ ## .y3 (p43) 0.500 0.119 4.194 0.000 ## y2 ~~ ## y1 (p21) 0.500 0.112 4.450 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## y1 (p11) 1.000 0.142 7.036 0.000 ## y2 (p22) 1.000 0.142 7.036 0.000 ## .y3 (p33) 0.800 0.114 7.036 0.000 ## .y4 (p44) 0.700 0.139 5.035 0.000 When you run the script for Model 1, you will notice that the parameter estimates are exactly equal to our chosen values from Step 1. Thus, this indicates that the model may be identified. As this empirical check is a necessary requirement for identification, but not sufficient, we are not completely sure that the model is truly identified. In the next script we show the empirical check of Model 2, where we also add the effect from \\(y_4\\) to \\(y_3\\). ### Script 7.2 ### ## STEP 1: calculate the model implied covariance matrix BETA &lt;- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, .5,.5, 0,.5, 0, 0,.5, 0), nrow=4,ncol=4,byrow=TRUE) PSI &lt;- matrix(c( 1,.5, 0, 0, .5, 1, 0, 0, 0, 0,.8,.5, 0, 0,.5,.7), nrow=4,ncol=4,byrow=TRUE) IDEN &lt;- diag(1,4,4) SIGMA &lt;- solve(IDEN-BETA) %*% PSI %*% t(solve(IDEN-BETA)) obsnames = c(&quot;y1&quot;,&quot;y2&quot;,&quot;y3&quot;,&quot;y4&quot;) dimnames(SIGMA) &lt;- list(obsnames,obsnames) ## STEP 2: fit model to the covariance matrix from step 1 EmpiricalCheck &lt;- &#39; # regressions y4 ~ b43*y3 y3 ~ b32*y2 + b31*y1 + b34*y4 # residual (co)variances y1 ~~ p11*y1 y2 ~~ p22*y2 y3 ~~ p33*y3 y4 ~~ p44*y4 y3 ~~ p43*y4 # covariance exogenous variables y1 ~~ p21*y2 &#39; EmpiricalCheckOut &lt;- lavaan(EmpiricalCheck, sample.cov=SIGMA, sample.nobs=100, likelihood = &quot;wishart&quot;, fixed.x=FALSE) ## Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING: ## Could not compute standard errors! The information matrix could ## not be inverted. This may be a symptom that the model is not ## identified. summary(EmpiricalCheckOut) ## lavaan 0.6-11 ended normally after 21 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y4 ~ ## y3 (b43) 0.500 NA ## y3 ~ ## y2 (b32) 0.383 NA ## y1 (b31) 0.383 NA ## y4 (b34) 0.850 NA ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y4 ~~ ## .y3 (p43) 0.056 NA ## y2 ~~ ## y1 (p21) 0.500 NA ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## y1 (p11) 1.000 NA ## y2 (p22) 1.000 NA ## .y3 (p33) 0.265 NA ## .y4 (p44) 0.700 NA The output for the empirical check of Model 2 does not lead to the same parameter estimates as we used in Step 1. Therefore, we can be sure that this model is not identified. 7.4 Appendix I 7.4.1 Algebraic assessment of identification using elements of \\(\\Sigma_{population}\\) and \\(\\Sigma_{model}\\) \\[\\begin{equation} \\beta_{31} = \\frac{\\sigma_{31}-\\frac{\\sigma_{32}-\\beta_{31}\\psi_{21}}{\\psi_{22}}\\psi_{21}}{\\psi_{11}} \\tag{7.9} \\end{equation}\\] \\[\\begin{align} \\beta_{31}\\psi_{11} &amp; = \\sigma_{31}-\\frac{\\sigma_{32}\\beta_{31}\\psi_{21}}{\\psi_{22}}\\psi_{21} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7.09a)\\\\ \\\\ \\beta_{31}\\psi_{11}\\psi_{22} &amp; = \\sigma_{31}\\psi_{22} - (\\sigma_{32}-\\beta_{31} \\psi_{21})\\psi_{21} \\ \\ \\ \\ \\ \\ \\ \\ (7.09b)\\\\ \\\\ \\beta_{31}\\psi_{11}\\psi_{22} &amp; = \\sigma_{31}\\psi_{22} - \\sigma_{32}\\psi_{21} + \\beta_{31}\\psi^2_{22} \\ \\ \\ \\ \\ \\ \\ \\ (7.09c)\\\\ \\\\ \\beta_{31}\\psi_{11}\\psi_{22} - \\beta_{31}\\psi^2_{21} &amp; = \\sigma_{31}\\psi_{22} - \\sigma_{32}\\psi_{21} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7.09d)\\\\ \\\\ \\beta_{31}(\\psi_{11}\\psi_{22} -\\psi^2_{21}) &amp; = \\sigma_{31}\\psi_{22} - \\sigma_{32}\\psi_{21} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7.09e) \\end{align}\\] \\[\\begin{equation} \\beta_{31} = \\frac{\\sigma_{31}\\psi_{22}-\\sigma_{32}\\psi_{21}}{\\psi_{11}\\psi_{22}-{\\psi_{21}}^2} \\tag{7.10} \\end{equation}\\] \\[ \\begin{align} \\sigma_{33} = \\dots \\ \\ \\Longleftrightarrow \\ \\ \\psi_{33} = \\dots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7.13) \\end{align} \\] Substituting Equations (7.2), (7.3), (7.4), (7.11) and (7.13) into Equation (7.13) gives: \\[\\begin{equation} \\sigma_{33} = \\frac{\\sigma_{11}\\sigma_{22}\\sigma_{33}-2\\sigma_{21}\\sigma_{31}\\sigma_{32}-\\sigma_{11}\\sigma^2_{32}-\\sigma_{22}\\sigma^2_{31}\\sigma_{33}\\sigma^2_{21}}{\\sigma_{11}\\sigma_{22}\\sigma^2_{21}} \\tag{7.14} \\end{equation}\\] \\[\\begin{equation} \\sigma_{41} = \\dots  \\beta_{43} = \\frac{\\sigma_{41}}{\\beta_{31}\\psi{11} + \\beta_{32}\\psi_{21}} \\tag{7.15} \\end{equation}\\] Substituting Equations (7.2), (7.4), (7.11) and (7.13) into Equation (7.15) gives: \\[\\begin{equation} \\beta_{43} = \\frac{\\sigma_{41}}{\\sigma_{31}} \\tag{7.16} \\end{equation}\\] Although, to identify \\(\\beta_{43}\\), we can also use: \\[\\begin{equation} \\sigma_{42} \\Longleftrightarrow \\beta_{43} = \\frac{\\sigma_{42}}{\\beta_{31}\\psi_{21} + \\beta_{32}\\psi_{22}} \\tag{7.17} \\end{equation}\\] Substituting Equations (7.2), (7.3), (7.11) and (7.12) into Equation (7.17) gives: \\[\\begin{equation} \\beta_{43} = \\frac{\\sigma_{42}}{\\sigma_{31}} \\tag{7.18} \\end{equation}\\] So, now we already have two identifications of \\(\\beta_{43}\\), and we can also use: \\[ \\sigma_{43} = \\dots \\ \\ \\Longleftrightarrow \\ \\ \\beta_{43} = \\dots \\ \\ \\ , \\] yielding: \\[\\begin{equation} \\beta_{43} = \\frac{\\sigma_{43}}{\\sigma_{33}} \\tag{7.19} \\end{equation}\\] Equations (7.16), (7.17) and (7.19) show three identifications of \\(\\beta_{43}\\). \\(\\sigma_{44}\\) identifies \\(\\psi_{44}\\), through: \\[ \\sigma_{44} = \\dots \\ \\ \\Longleftrightarrow \\ \\ \\psi_{44} = \\dots \\ , \\] and substitution: \\[\\begin{equation} \\psi_{44} = \\sigma_{44} - \\sigma_{33}\\beta^2_{43} \\tag{7.20} \\end{equation}\\] Which is identified after substituting either Equation (7.16), (7.18) or (7.19). "],["autoregression.html", "8 Autoregression 8.1 Autoregression models with predictors 8.2 Cross-lagged panel models 8.3 References", " 8 Autoregression In an autoregression (or autoregressive) model, the observed variables are the same variable, measured at different time points. Each observed variable regresses on the same variable at the previous measurement occasion. Hence the term autoregression. An autoregression model allows us to study interindividual change. The direct effects between subsequent timepoints indicate how stable the ordering of individuals is across time. Therefore, the direct effects are also called stability coefficients. A simple autoregression model with four measurement occasions (collected in four years) is depicted in Figure 1. This example is taken from Duncan and Duncan (1996), who conducted a longitudinal study of alcohol use among adolescents. A sample of 321 adolescents were surveyed annually over a 4-year period, in which they indicated their alcohol consumption. Figure 8.1. Simple autoregression model. Script 8.1 fits this model to the data of Duncan and Duncan (1996). ### Script 8.1 ### ## observed data obsnames &lt;- c(&quot;year1&quot;,&quot;year2&quot;,&quot;year3&quot;,&quot;year4&quot;,&quot;gender&quot;,&quot;famstat&quot;) values &lt;- c(1.000, .640, 1.000, .586, .670, 1.000, .454, .566, .621, 1.000, .001, .038, .118, .091, 1.000, -.214, -.149, -.135, -.163, -.025, 1.000) SD &lt;- c(1.002,.960,.912,.920,.504,.498) duncancov &lt;- getCov(x=values, names = obsnames, sds = SD) names(duncanmeans) &lt;- obsnames ## define model duncanmodel &lt;- &#39; # COVARIANCES # regression equations year4 ~ b43*year3 year3 ~ b32*year2 year2 ~ b21*year1 # residual variances year4 ~~ p44*year4 year3 ~~ p33*year3 year2 ~~ p22*year2 # variance exogenous variable year1 ~~ p11*year1 &#39; ## run model duncanmodelOut &lt;- lavaan(duncanmodel, sample.cov=duncancov, sample.nobs=321, likelihood = &quot;wishart&quot;, fixed.x=FALSE) ## output summary(duncanmodelOut, fit = TRUE) The autoregression model is actually a simple path model. For the covariance structure we define the direct effects between the observed variables (matrix \\(B\\)) and the (residual) variances (matrix \\(\\Psi\\)). If an autoregressive model doesnt explain the correlations between measurements well enough, one could add second order autoregressive effects. In Figure 1 this would be a direct effect of Year 1 on Year 3, and a direct effect of Year 2 on Year 4. A common practice in autoregression models is to constrain the autoregression effects (i.e., the direct effects between the measurements) to be equal. This is done by providing equal labels for these effects in the \\(B\\) matrix: # regression equations year4 ~ Beq*year3 year3 ~ Beq*year2 year2 ~ Beq*year1 The difference in model fit between the model without equality constraints on the regression coefficients and the model with the equality constraints on the regression coefficients, can be used to determine whether this equality constraint is tenable. It is only theoretically justifiable to make this constraint if the lag (time between measurements) is equal. Because causal effects take time to occur, the size of the effect depends on the lag between measurements. For example, effects typically grow stronger for a short duration after the first measurement, but then the effect grows weaker over time. So when you report estimated autoregressive effects, make sure to be clear that the estimate applies only to the lag between those occasions (in this example, 1 year). 8.1 Autoregression models with predictors If an autoregression model fits the data well, predictors can be added to the model. These predictors are usually entered as exogenous variables, with a direct effect on the first measurement. When the predictor variable represents an intervention that took place after the first measurement, one would only add direct effects of that predictor on timepoints after the intervention. Script 8.2 fits an autoregression model, with second order autoregression effects, and gender and family status as predictors to the same data of Duncan and Duncan (1996) as in script 8.1. This model is depicted in Figure 2. Figure 8.2. Simple autoregression model, including predictor variables. ### Script 8.2 ### ## define model duncanmodel &lt;- &#39; # COVARIANCES # regression equations year4 ~ b43*year3 + b42*year2 year3 ~ b32*year2 + b31*year1 year2 ~ b21*year1 year1 ~ b15*gender + b16*famstat # residual variances year4 ~~ p44*year4 year3 ~~ p33*year3 year2 ~~ p22*year2 year1 ~~ p11*year1 # (co)variance exogenous variable gender ~~ p55*gender famstat ~~ p66*famstat gender ~~ p65*famstat&#39; ## run model duncanmodelOut &lt;- lavaan(duncanmodel, sample.cov=duncancov, sample.nobs=321, likelihood = &quot;wishart&quot;, fixed.x=FALSE) ## output summary(duncanmodelOut, fit = TRUE) If the fit of a model where only the first measurement is regressed on the predictors is not satisfactory, one might consider adding direct effects of the predictors on later measurements as well. The conceptual difference between those two models would be their theoretical interpretation. In the example above, the predictors only affect later outcomes indirectly, so this model represents the hypothesis that any sex or family differences that existed at Time 1 do not change over time. A model with additional direct effects of predictors on later outcomes would represent the hypothesis that group differences change over time. The \\(\\Delta\\chi^2\\) statistic allows you test the null hypothesis of no change in sex or family differences over time by comparing nested models with and without those additional effects. 8.2 Cross-lagged panel models If more than one variable are measured at several time points, one could also evaluate so called cross-lagged effects in a cross-lagged panel model. These are the effects of one variable on the other at a later time point. Suppose that in the earlier example the researchers did not only have measures of alcohol consumption (\\(x\\)) across four years, but also of depression (\\(y\\)). The model in Figure 2 can be used to evaluate the effect of alcohol consumption in year 1 on depression in year 2, controlled for depression in year 1 (and the effect of depression in year 2 on alcohol consumption in year 3, controlled for alcohol use in year 2, and so on for later time points). The residual factors (\\(\\)) of different variables at the same time point are often correlated, representing that the same unobserved variables may affect the responses on the two variables. For more information on cross-lagged panel models see Biesanz (2012). 8.3 References Biesanz, J.C. (2012). Autoregressive longitudinal models. In Hoyle, R.H. (Ed.), Handbook of Structural Equation Modeling (pp. 459-471). New York: Guilford Press. Duncan, S. C. &amp; Duncan, T. E. (1996). A multivariate latent growth curve analysis of adolescent substance use. Structural Equation Modeling, 3, 323-347. "],["model-specification-and-model-identification.html", "9 Model Specification and Model Identification 9.1 Backward and forward specification searches 9.2 Correlation residuals 9.3 Modification indices 9.4 References", " 9 Model Specification and Model Identification 9.0.1 From theory to model Structural equation modelling is a confirmatory technique, i.e., we specify a model that we believe to be true in the population and test whether the specified model fits the data that we have obtained. In principle, the hypothesized model that will be fitted to the data is formulated before the data are collected. Following the stages of the in the empirical cycle as formulated by A. D. de Groot (1961), one starts with observation (the researcher observes something that deserves investigation, for example, that perfectionistic parents often have anxious children), followed by induction (reflection on, or formulation of, theories that may explain the observed phenomena), followed by deduction (the formulation of testable research hypotheses that would be implied by theory, for example, that perfectionism of parents leads to over-controlling parenting behaviour, which in turn leads to anxiety in their children). The next stage is testing the hypothesis using data. This is the phase in which the structural model that represents the research hypotheses is tested against reality (i.e., data). The last phase is evaluation, which may possibly lead to new hypotheses or revision of the theory. If there are competing theories, that each postulate a different model based on the same variables, researchers may use SEM to test the models corresponding to the different theories and select the model that fits the data best. 9.0.2 Model parsimony As a researcher, you want to end up with a model that is simple, but that is still a good reflection of reality. In SEM, one indication of the simplicity or parsimony of a model is the degrees of freedom. For a given number of observed variables, the more degrees of freedom a model has, the more parsimonious the model (i.e., the less parameters it has, and thus the less complex the model). However, complex models with more parameters generally fit the data better. Unfortunately, this is not necessarily because the model is a more accurate description of reality; rather, freeing parameters in a data-driven way often leads to a model that is fit to sample-specific nuances. Finding the balance between parsimony and accuracy is a difficult issue. A famous principle that refers to this issue is Ockhams razor, or the law of parsimony which Albert Einstein translated into the advice: Everything should be kept as simple as possible, but no simpler. 9.0.3 A priori model specification Very often, researchers plan to fit several models. For example, if the goal is to test a mediation hypothesis, one may first fit a model with an indirect effect only, and subsequently add the direct effect to test partial mediation. Or in a full SEM model, it is common to use two-step modeling, which means that one first tests the measurement model, without imposing a structure on the correlations between latent variables. Only if the measurement model is accepted, one continues with specifying a path model or factor model on the latent variables. Sometimes, the hypotheses that the researchers want to tests arent only about the overall fit of the model, but about specific parameters in a model. For example, researchers may want to test whether certain parameters in the model are equal. This can be tested by fitting a model with and without equality constraints on these parameters, and investigate whether the fit of the model with the equality constraint is significantly worse than the fit of the model without the constraint. As long as the models that will be fitted, and the hypotheses that will be answered by these models, are formulated before looking at the data, this practice can still be called confirmatory. This is not the case if the model modifications are purely based on statistics, as will be described next. 9.0.4 Post hoc model modification Often, researchers just test one theoretical model. When the model does not fit the data well, we might need to reject the notion that the model (i.e., the theory) is a true reflection of reality. When this model does not fit the data well, the model is usually modified in order to obtain better model fit. The process of sequential model modification is also called a specification search. As post-hoc model modifications (modifications to the model based on information obtained after fitting the model to the data) are inherently data-driven, they are exploratory by nature. They should be executed with great care, as they often do not lead to a model that generalizes to other samples (MacCallum, 1986). To reduce the risk of ending up with a completely sample-specific model, the specification search should always be guided by theoretically meaningful modifications. Additional strategies to reduce the risk of meaningless data-driven model modifications are to formulate a pre-specified set of parameters that are candidates for modification, to fit the model on data from large samples, and formulate the initial model very carefully. Post-hoc model modification is a data-based approach, so any final model should be evaluated as stemming from an exploratory analysis strategy, which needs to be validated in other samples. It is also important to realize that by adding parameters to the model, the discrepancies between \\(\\Sigma_{population}\\) and \\(\\Sigma_{model}\\) will decrease, and thus the \\(^2\\) value will become closer to zero. However, models with more parameters are also more complex and can lead to unnecessary parameters that take on meaningless values. We should strive for models that are parsimonious and can be clearly understood. So, both with the originally hypothesized model and with modified models, one needs to balance trying to find a simple and interpretable model, and arriving at good model fit. 9.1 Backward and forward specification searches When altering a model post-hoc, there are basically two different strategies, and these strategies should not be mixed. The backward specification search involves fitting a model with many parameters, more than expected based on the theory, and then removing parameters one by one if they are not considered statistically significant. This is also called model trimming. The more popular strategy is the forward specification search, called model building. With model building one starts with a parsimonious model that only includes the parameters that are expected by theory, and add parameters when the model does not fit. The researcher can get information about which parameters will likely improve the fit of a model by inspection of correlation residuals or modification indices. If a model doesnt fit the data, it means that the model isnt explaining the variances and covariances of the variables well; that is, the discrepancy between \\(\\widehat{\\Sigma}_{population}\\) and \\(\\widehat{\\Sigma}_{model}\\) is substantial according to the fit criteria. It may be that all covariances are very different, but it could also be that the model appropriately models the relations between some variables, but doesnt sufficiently account for the covariances between other variables. In this case, adding parameters specifically to model the unexplained covariance may be sensible. Correlation residuals and modification indices can be used to find out from which part of the model the largest misfit arises. 9.2 Correlation residuals Covariance residuals are the differences between the observed and model-implied covariances. That is, the matrix of covariance residuals equals the matrix of observed sample variances and covariances minus the estimated model implied matrix of variances and covariances (i.e., \\(_{sample}\\) - \\(\\widehat{\\Sigma}_{model}\\)). Covariance residuals have no common scale, which makes them difficult to interpret. Therefore, it may be helpful to consider the correlation residuals instead. Correlation residuals are calculated by standardizing the observed and model-implied covariance matrices, then subtracting the model-implied from the sample correlation matrix (i.e., \\(^*_{sample}\\) - \\(\\widehat{\\Sigma}^*_{model}\\)). Table 1 shows the correlation residuals for the full mediation model of child anxiety (i.e., the model without direct effects of Parent Anxiety and Parent Perfectionism on Child Anxiety). It can be seen that some correlations are fully explained by the model, leading to a correlation residual of zero. The correlation residual between Parental Anxiety and Child Anxiety is the largest. If a correlation residual for two variables is large, there is unmodeled dependency. As a general rule of thumb, correlation residuals with an absolute value larger than .10 are often regarded as being large. Thus, the correlation residual between Parent Anxiety and Child Anxiety lies outside the cut-off value of .10 and therefore can be considered large. Now, the researchers has to think about which parameter could be added to explain the dependency between these variables better. Here, one can choose between a direct effect or a covariance. If there are no clear ideas about the directionality of the effect, adding a covariance may be the best option. In the current example, a direct effect of PA on CA seems to make the most sense, as we already included an indirect effect of PA on CA in the model. Adding a direct effect would mean that the effect of PA and CA is not fully, but partially mediated by over-control. If a parameter is added to the model, the correlation residuals will change. Therefore, one should recalculate the correlation residuals after each model modification. Table 1. Correlation residuals for the full mediation model of child anxiety model Parental Anxiety Perfectionism Overcontrol Child Anxiety Parental Anxiety .000 Perfectionism .000 .000 Overcontrol .000 .000 .000 Child Anxiety .272 .034 .000 .000 9.3 Modification indices Another option to investigate which possible modifications to the model could improve model fit is to look at so-called Modification Indices (MIs). MIs provide a rough estimate of how much the \\(^2\\) test statistic of a model would decrease if a particular fixed (or constrained) parameter were freely estimated. The size of a MI can be evaluated relative to a \\(^2\\) distribution with 1 \\(df\\). So, with a significance level of .05, one could call a MI larger than 3.84 significant. To control the inflation of Type I error rates due to multiple testing, it is recommended to (a) plan a priori which parameters modification indices you will inspect and (b) use a Bonferroni adjusted \\(\\) level to select your critical value. For instance, the full mediation model for child anxiety only has df = 2, so you should choose no more than two modification indices to inspect. A Bonferroni-adjusted \\( = .05 / 2 = .025\\) would yield a \\(^2\\) critical value of 5.02 (for a 1-\\(df\\) test). This is analogous to using a post hoc method (e.g., Tukey, Bonferroni, or Scheffé) to compare pairs of group means following a significant omnibus \\(F\\) test in ANOVA. MIs depend on sample size. The larger the sample, the larger the modification indices. So, with large sample sizes, small misfit may already lead to large MIs. Therefore, it is useful to inspect the expected parameter change (EPC) as well. The EPC gives an indication of how a specific parameter would change if it were freely estimated. If a MI is high, but the EPC is very low, addition of the parameter would not add much to the model substantively. In addition, to enable comparison of EPCs it is often convenient to inspect the standardized version of the EPC (SEPC), where values larger than .10 are considered to be substantial (Saris &amp; Stronkhorst, 1984). Note that MIs are univariate indices. As soon as parameter is added to (or removed from) the model, all MIs might change. Therefore, one should inspect MIs after each model modification. Table 2 shows the MIs and SEPCs for the full mediation model of child anxiety. As you can see, the MIs are given for every parameter that is not in the model, even for parameters that do not really make sense or are mathematically not possible to add. For example, you see a MI for the direct effect of Child Anxiety on Overcontrol, as well as a covariance between these two variables. It would be strange to add one of these parameters to the model, because we already specified a direct effect from Overcontrol on Child Anxiety. Both in terms of interpretation and in terms of estimation (i.e., the model would become nonrecursive), it would be difficult to add these parameters to the model. So, as a researcher, you have to make sure to evaluate the appropriateness of parameters to be added. In the current example, the highest MI is for a covariance between CA and PA, while the MI of a direct effect of PA on CA is very similar. This is because freeing either one of these parameters would lead to an equivalent model (not equivalent in interpretation, but statistically equivalent because both models would result in the same model-implied covariance matrix). Theoretically, it makes more sense to assume that Parental Anxiety affects Child Anxiety than the other way around. Therefore, a direct effect of PA on CA seems to be the most sensible parameter to add. Table 2. Modification indices for the full mediation model of child anxiety. Parameter MI SEPC Covariance: child_anx \\(\\) parent_anx 6.871 .264 Direct effect: child_anx \\(\\leftarrow\\) parent_anx 6.716 .292 Direct effect: parent_anx \\(\\leftarrow\\) child_anx 6.871 .290 Direct effect: overcontrol \\(\\leftarrow\\) child_anx 2.076 .459 Covariance: child_anx \\(\\) overcontrol 2.076 .418 Direct effect: perfectionism \\(\\leftarrow\\) child_anx .598 .087 Direct effect: child_anx \\(\\leftarrow\\) perfectionism .109 .038 Covariance: perfectionism \\(\\) child_anx .598 .079 The lavaan Script 9.0 below demonstrates how to make a minor model adjustment, such as adding a single parameter. Note that the model syntax does not need to be a single character string, but can be a vector of character strings, so we do not need to write out the whole model again. The new model can be fit to the data using all the same lavaan() arguments in Script 3.1, or we can use the update() function as a shortcut. Any specified arguments will replace the original model-fit arguments found in the object AWmodelOut. The first argument to lavaan() is called model), so we provide our new model syntax. All other (unspecified) arguments will be the same when the new model is fit, saving the results in the object AWmodel2Out. ### Script 9.0 ### # add a direct effect of Parent Anxiety on Child Anxiety to the path model AWmodel2 &lt;- c(AWmodel, &#39; child_anx ~ b41* parent_anx &#39;) # fit the new model by &quot;updating&quot; the original model AWmodel2Out &lt;- update(AWmodelOut, model = AWmodel2) In our illustrative example of child anxiety, the partial mediation model (where the regression of child anxiety on parent anxiety is added to the model) yields the following test result: 2(1) = 0.397, p = .53. Thus, the 2 value is no longer significant (at  = .05), so the null hypothesis of exact fit cannot be rejected. This gives support to the notion that the model gives a true description of reality. However, one could question the parsimony of this model with a single degree of freedom. The size of the direct effect in standardized metric is estimated to be .292, which is substantial. Note that the actual value matches the SEPC in Table 2 to the third decimal place. Expected and actual estimated values will not always correspond so perfectly, but they will typically be very close. 9.3.1 Cross-validation When post hoc model modification is used, one is actually not testing hypotheses anymore, but one is generating hypotheses based on the data under consideration. That is, it falls under exploratory research, and its results carry less scientific certainty than the results of confirmatory research. Future research could focus on trying to replicate the findings based on the final model from exploratory research. 9.3.2 Calculating correlation residuals in lavaan The residual() function (or resid() for short) provides covariance residuals by default, but you can request correlation residuals with an additional argument: resid(AWmodelOut, type = &quot;cor&quot;) ## the default standardization method is Bollen&#39;s (1989) formula resid(AWmodelOut, type = &quot;cor.bollen&quot;) ## optionally, arrange rows &amp; columns to match input order resid(AWmodelOut, type = &quot;cor&quot;)$cor[obsnames, obsnames] First, we created two objects that contain the sample (ssam) and model-implied (smod) covariance matrices. Although the sample covariance matrix is available as the object AWcov that was used as input data, this would not be the case if raw data were analysed. And although we could calculate the sample covariance matrix for complete data using the cov() function, when there are missing data, covariance and correlation residuals must be calculated using the estimated covariance matrix (also called the EM matrix) that is obtained by fitting the saturated model to the partially observed data. Thus, the most error-free way to extract this information from lavaan is to use the lavInspect() function to extract the sample statistics, or sampstat for short. Likewise, the model-implied covariance matrix can be extracted by requesting cov.ov (i.e., the model-implied covariance between observed variables). # extract the sample covariance matrix ssam &lt;- lavInspect(AWmodelOut, &quot;sampstat&quot;)$cov[obsnames, obsnames] # extract the model-implied covariance matrix smod &lt;- lavInspect(AWmodelOut, &quot;cov.ov&quot;)[obsnames, obsnames] These matrices need to be standardized. This is done with the R function cov2cor(), which transforms a covariance matrix to a correlation matrix. The model-implied correlation matrix is then subtracted from the observed correlation matrix, and we print the result rounded to the third decimal place: corres &lt;- cov2cor(ssam) - cov2cor(smod) round(corres, 3) The last two lines are optional, but may help you find the large (&gt; .10) and largest residuals, respectively, indicated by cells saying TRUE. abs(corres) &gt;= .10 abs(corres) == max(abs(corres)) The matrix of correlation residuals for the AW-model is given below. parent_anx perfect overcontrol child_anx parent_anx 0.000 0.000 0.000 0.272 perfect 0.000 0.000 0.001 0.034 overcontrol 0.000 0.001 0.000 0.001 child_anx 0.272 0.034 0.001 0.000 Note that there is also a slightly different formula available, which is implemented in Bentlers (1995) EQS software. Rather than standardizing each matrix with respect to their own SDs, then subtracting them, Bentler proposed subtracting the unstandardized covariance matrices, then standardizing the differences with respect to the observed-variable SDs only. Script 8.2 shows how to request these correlation residuals from lavaan, as well as how the procedure to calculate them differs. ### Script 9.2 ### ## request Bentler&#39;s (1995) formula round(resid(AWmodelOut, type = &quot;cor.bentler&quot;)$cor[obsnames, obsnames], 3) ## MANUAL PROCEDURE: ## calculate covariance residuals covres &lt;- ssam - smod ## standardize covariance residuals to make correlation residuals SDs &lt;- diag(sqrt(diag(ssam))) corres &lt;- solve(SDs) %*% covres %*% solve(SDs) dimnames(corres) &lt;- list(obsnames, obsnames) round(corres, 3) In this case, you will not notice any difference between the two procedures. The reason is that when no constraints are placed on the (residual) variance estimates, the estimated values will be chosen such that the model-implied total variance will exactly reproduce the observed total variance. In such situations, there is no practical difference between the two formulae. In situations where the variances are constrained (see, for example, later chapters on strict measurement invariance), the two formulae will diverge, and neither is right or wrong because they are both standardized, but with respect to different criteria. Because Bollens (1989) formula standardizes the observed and model-implied matrices first, the diagonals will always be 1 by definition, so diagonal elements of the correlation-residual matrix will always be 1  1 = 0. Bentlers (1995) formula, on the other hand, might be more helpful to identify invalid constraints on (residual) variances because the differences between observed and model-implied variances are calculated first, then standardized with respect to how large the true observed variances are. 9.3.3 Requesting MIs and (S)EPCs in lavaan In lavaan, MIs and EPCs can be inspected using the modificationIndices() function. This function returns a data frame with the complete list of model parameters that are fixed to a specific value (i.e., not for equality constraints), and prints each associated MI and (standardized and unstandardized) EPC. Using the following commands, the data frame is sorted with the highest MIs printed first. Also, we can specify a minimum value (i.e., a critical value, crit) to display only MIs that might be considered significant. Here, we define it as a 1-df 2 value corresponding to  = .05: crit &lt;- qchisq(.05, df = 1, lower.tail = FALSE) modificationIndices(AWmodelOut, sort. = TRUE, minimum.value = crit) The output for all nonzero MIs from the model are given below. lhs op rhs mi epc sepc.lv sepc.all sepc.nox 12 child_anx ~~ parent_anx 6.871 6.945 6.945 0.264 0.264 18 parent_anx ~ child_anx 6.871 1.009 1.009 0.290 0.290 15 child_anx ~ parent_anx 6.716 0.084 0.084 0.292 0.292 You can also request that MIs be printed at the bottom of the summary() output, but you have no options to sort or subset them: summary(AWmodelOut, modindices = TRUE) 9.4 References Bentler, P. M. (1995). EQS structural equations program manual. Encino, CA: Multivariate Software. Bollen, K. A. (1989). Structural equations with latent variables. Hoboken, NJ: Wiley. MacCallum, R. C. (1986). Specification searches in covariance structure modelling. Psychological Bulletin, 100(1), 107120. doi:10.1037/0033-2909.100.1.107 Saris, W. E., &amp; Stronkhorst, H. (1984). Causal modelling in nonexperimental research: An introduction to the LISREL approach. Amsterdam, NL: Sociometric Research Foundation. "],["estimation.html", "10 Estimation 10.1 Improper Solutions 10.2 Specifying starting values in lavaan", " 10 Estimation In the chapter on identification we saw that model parameters can be written as a function of elements from \\(\\Sigma_{population}\\). Subsequently, we can use these equations to estimate model parameters derived from the sample variances and covariances (\\(\\Sigma_{sample}\\)). For just-identified models, the number of known elements in \\(\\Sigma_{sample}\\) equals the number of unknown model parameters, and one can derive parameter estimates by solving the equations that satisfy \\(\\widehat\\Sigma_{model}\\) = \\(\\Sigma_{sample}\\). For models that are over identified (i.e., that have positive \\(df\\)) the estimation of model parameters is more complicated. In our illustrative example of child anxiety, we can derive three different equations in terms of population variances and covariances (i.e., by rewriting the equations of \\(\\sigma_{41}\\), \\(\\sigma_{42}\\) and \\(\\sigma_{43}\\)) for the expression of the single model parameter \\(\\beta_{43}\\). When we subsequently want to use these equations to estimate the model parameter \\(\\widehat\\beta_{43}\\) we can never derive an estimate so that all three elements \\(\\widehat\\sigma_{41}\\), \\(\\widehat\\sigma_{42}\\) and \\(\\widehat\\sigma_{43}\\) of \\(\\widehat\\Sigma_{model}\\) are equal to the corresponding elements of \\(\\Sigma_{sample}\\). Therefore, for over-identified models there will always be a certain amount of discrepancy between  model and sample. In such cases, the estimation of model parameters requires an iterative procedure to minimize the discrepancy between \\(\\Sigma_{sample}\\) and \\(\\widehat\\Sigma_{model}\\) (i.e., What value of \\(\\hat\\beta_{43}\\)will lead to expressions of \\(\\widehat\\sigma_{41}\\), \\(\\widehat\\sigma_{42}\\) and \\(\\widehat\\sigma_{43}\\) that are as close as possible to the corresponding elements in \\(\\Sigma_{sample}\\)?). The estimation procedures are iterative because the initial solution (i.e., the first attempt to estimate model parameters and evaluate the discrepancy between \\(\\Sigma_{sample}\\) and \\(\\widehat\\Sigma_{model}\\)) is improved through subsequent cycles of calculations until the improvement in model fit falls below a predefined minimum value (i.e., the decrease in discrepancy between \\(\\Sigma_{sample}\\) and \\(\\widehat\\Sigma_{model}\\) is very small). When this happens, the estimation process has converged. Convergence may be achieved more quickly when the initial solution, or the initial estimates of the parameters (i.e., start values) are reasonably accurate. If these initial estimates are completely inaccurate, then iterative estimation may even fail to converge. Most computer programs automatically generate reasonable start values and will give a warning if a solution has not converged. However, sometimes it may be necessary to provide user-specified start values in order for the solution to converge, especially for more complex models. Several discrepancy functions exist that can be used for the estimation of model parameters. Three desirable qualities of a good estimation procedure are unbiasedness, consistency and efficiency. Suppose we repeat the estimation procedure for a given model an infinite number of times on an infinite number of datasets. The estimator is unbiased when the expected parameter values (i.e., means of the sampling distributions of parameter estimates) are equal to the population values, i.e., they are not systematically biased upwards or downwards. The estimator is consistent when parameter estimates are closer to the population parameters with increased sample size, i.e., the larger the sample size, the closer the estimated values are to the population values (given that the sample is representative of the population). Efficiency refers to the variance of the sampling distribution of parameter estimates, where the most efficient estimator has a sampling distribution with the smallest variance (and thus the smallest \\(SE\\)s). The most intuitive method that could be used to minimize the discrepancies between \\(\\Sigma_{sample}\\) and \\(\\widehat\\Sigma_{model}\\) is to simply calculate the difference between the two matrices and try to minimize the values within the resulting residual covariance matrix. This is what is commonly referred to as the unweighted least squares (ULS) discrepancy function: \\[\\begin{equation} F_{ULS}(\\Sigma_{sample},\\widehat\\Sigma_{model}) = ½ trace ((\\Sigma_{sample} - \\widehat\\Sigma_{model})^2), \\tag{10.1} \\end{equation}\\] !missing footnote! which minimizes the squared deviations between the observed sample variances and covariances and the corresponding elements predicted by the model. If the deviations are zero (i.e., observed and model-implied matrices are identical), then FULS is zero, indicating perfect fit. Any nonzero discrepancies yield a positive FULS. Minimizing FULS yields unweighted least squares (ULS) estimates of all the model parameters given by Equations 3.09 and 3.10, under the assumption that the model is valid and identified. It leads to unbiased and consistent estimates of model parameters, and does not require that the observed variables have a particular distribution. However, it is not the most efficient estimator. A particularly attractive discrepancy function is given by: \\(F_{ML}(\\Sigma_{sample},\\widehat\\Sigma_{model}) = log|\\widehat\\Sigma_{model}|-log|\\Sigma_{sample}|+trace(\\Sigma_{sample}\\widehat\\Sigma^{-1}_{model})-p,\\) \\[\\begin{equation} F_{ML}(\\Sigma_{sample},\\widehat\\Sigma_{model}) = log|\\widehat\\Sigma_{model}|-log|\\Sigma_{sample}|+trace(\\Sigma_{sample}\\widehat\\Sigma^{-1}_{model})-p, \\tag{10.2} \\end{equation}\\] which gives so-called maximum likelihood (ML) estimates of all model parameters, assuming that the distribution of the scores of the observed variables (\\(p\\)) is multivariate normal and that the model is valid and identified. It is the most widely used fitting function for general structural equation models, and the estimator is unbiased, consistent and more efficient than ULS. Although it is a more complex nonlinear function, it has additional important properties. First, it allows for tests of statistical significance of parameter estimates as the estimators are asymptotically normally distributed. Second, unlike FULS, FML is scale invariant and scale free. These properties refer to the dependency of the fitting function on the units of measurement of the observed variables. When the fitting function is scale invariant, the value of the fit function is independent of the measurement scales of the variables, i.e., the value of the fit function will be the same for different scales of measurement. In addition, scale freeness refers to the property that when the scales of the observed variables are linearly transformed (i.e., multiplied by and/or added to a constant), the relationship between the parameter estimates of the transformed and untransformed solution can be derived too (i.e., they are linearly transformed in a similar fashion). In general, smaller values of discrepancy functions indicate better fit of the model to the data. A value of zero indicates the fit is perfect, i.e., the parameter estimates can perfectly reproduce the sample covariance matrix (\\(\\widehat\\Sigma_{model}=\\Sigma_{sample}\\)). Using Equation 10.02 to arrive at ML estimates of model parameter of the path model from our illustrative example, yields: \\[ \\widehat\\Sigma_{model} = \\begin{bmatrix} 91.58 &amp; \\\\ 53.36 &amp; 194.32 \\\\ 28.39 &amp; 50.90 &amp; 130.19 \\\\ 2.05 &amp; 3.68 &amp; 9.41 &amp; 7.56 \\end{bmatrix} \\] whereas the observed variances and covariances of the sample are given by: \\[ \\Sigma_{sample} = \\begin{bmatrix} 91.58 &amp; \\\\ 53.36 &amp; 194.32 \\\\ 28.39 &amp; 50.90 &amp; 130.19 \\\\ 9.21 &amp; 4.98 &amp; 9.41 &amp; 7.56 \\end{bmatrix} \\] When we compare \\(\\widehat\\Sigma_{model}\\) and \\(\\Sigma_{sample}\\) from our illustrative example, we can see that there are some discrepancies between the two matrices. Specifically, the elements \\(\\widehat\\sigma_{41}\\) and \\(\\widehat\\sigma_{42}\\) of \\(\\widehat\\Sigma_{model}\\) are different from the corresponding sample covariance. These two elements featured in the equations for the over-identified model parameter \\(\\widehat\\beta_{43}\\). As can be seen, the model parameter was estimated so that the element \\(\\widehat\\beta_{43}\\) of \\(\\widehat\\Sigma_{model}\\) equals the corresponding element of the sample covariance matrix, which thus leads to a misfit for the elements \\(\\widehat\\sigma_{41}\\) and \\(\\widehat\\sigma_{42}\\). The residual covariance matrix (i.e., \\(\\Sigma_{sample} - \\widehat\\Sigma_{model}\\)) is: \\[ \\Sigma_{residual} = \\begin{bmatrix} 0\\\\ 0&amp;0\\\\ 0&amp;0&amp;0\\\\ 7.2 &amp; 1.3 &amp; 0 &amp; 0 \\end{bmatrix} \\] This shows that the covariance between variable 4 (child anxiety) and variables 1 and 2 (parent anxiety and parent perfectionism) are underestimated by the model. When the residual values would be negative, this would indicate that the covariances are overestimated by the model. Model fit evaluation (which is the topic of Chapter 11) is used to evaluate whether the amount of misfit in these elements is substantial enough to reject the model, i.e., concluding that the specified model is not an adequate representation of the developmental process of child anxiety. It is important to note that models should only be fit to covariance matrices. Fitting a model to a correlation matrix without additional constraints yields incorrect \\(SE\\)s (Cudeck, 1989), so Wald \\(z\\) tests do not have nominal Type I error rates. 10.1 Improper Solutions Even when the estimation procedure has reached convergence, a researcher should always be aware of inadmissible solutions and so-called Heywood cases. Heywood cases refer to parameter estimates with an illogical value, like for example negative variance estimates, or estimated correlations with absolute values larger than 1.0. These kind of estimates have no plausible interpretations, and therefore such solutions are inadmissible. Sometimes the associated SEs of parameter estimates are very large (e.g., 999,999.99), which usually indicates that there is a problem with the model solution. Inadmissible solutions may be caused by several factors, including underidentification, misspecification, or bad starting values. Some computer programs give warnings that parameter estimates take on illogical values, but it is important to always carefully inspect all parameter estimates, unstandardized and standardized, to ensure that the solution is indeed admissible. Chen, Bollen, Paxton, Curran and Kirby (2001) describe the possible causes of, consequences of, and possible strategies to handle improper solutions in more detail. For instance, a Heywood case could result from either (a) model misspecification or (b) sampling error. So before trying to fix a Heywood case, one should first test the null hypothesis that the parameter is an admissible solution. For example, if the 95% CI for a residual variance includes positive values, then you cannot reject the null hypothesis (using  = .05) that the true population value is indeed positive; in this case, if the model fits well and there are no other signs of misspecification, you could conclude that the true parameter is simply close enough to zero that sampling error occasionally yields a negative estimate. 10.2 Specifying starting values in lavaan Methods for choosing default start values are quite excellent in recent software such as Mplus and lavaan, so it is rarely necessary for a user to provide start values manually. However, it may be useful to know how to do so, particularly when having difficulty converging on a proper (or any) solution. There are three ways to provide non-default starting values in lavaan. The first two involve specifying them in the lavaan model syntax, either using the start() modifier: "],["path-analysis-with-categorical-outcomes.html", "11 Path Analysis with Categorical Outcomes 11.1 Prepare Data and Workspace 11.2 Regression Models for Binary Variables 11.3 Latent Response Variable 11.4 Mediation Model with Categorical Outcomes 11.5 References", " 11 Path Analysis with Categorical Outcomes Structural equation modeling is a technique designed for continuous variables. In practice, variables are often not continuous but categorical, such as variables scored on discrete Likert scales (i.e., ordinal data) or correct/incorrect responses on test items (i.e., binary data). If endogenous variables in a path model (or in any SEM) are categorical, the SEM-method to deal with the categorical variables is to assume that there exists a latent (unobserved) continuous variable that underlies the observed categorical variable. The underlying variable is called a latent response variable (LRV). Categorical SEM fits the model to the LRVs instead of to the categorical variables. The observed categorical scores can be linked to the LRV using so-called thresholds that represent the value on the LRV beyond which individuals who score higher would get an observed categorical score in the higher category. For example, for a test item that asks How many degrees of freedom does this path model have?, the students could give the incorrect (scored 0) or the correct (scored 1) answer. The LRV here represents the ability (LRV) to calculate \\(df\\) for this path model on a continuous scale. Students whose ability is high enough would give the correct response; otherwise, the response would be incorrect. The threshold would reflect the minimum required ability to indicate the correct response. Fitting path models to categorical data thus involves linking the LRV to the observed categorical variables. In this chapter we first import example data, then we show how one can use probit and logit link functions in a generalized linear model (GLM). Next, we discuss the concept of the LRV for one ordinal variable, for multiple ordinal variables, and then we illustrate fitting a path model to ordinal variables. 11.1 Prepare Data and Workspace library(lavaan) 11.1.1 Import Example Data We will use example data provided on the M website} for a technical report (Muthén, 2011, p. 24). The example data can be downloaded onto your computer from the reports supplementary materials, or you can provide the URL directly to read.table(file=), as shown below. Note that the data are in summary format (i.e., a table with frequencies for each combination of categories for the 3 variables). The read.table() output is therefore transformed into a standard data.frame with 1 row per subject. dd &lt;- read.table(file = &quot;http://www.statmodel.com/examples/shortform/4cat%20m.dat&quot;, col.names = c(&quot;intention&quot;, &quot;intervention&quot;, &quot;ciguse&quot;, &quot;w&quot;)) dd$intention &lt;- dd$intention - 1L # make the lowest category 0, not 1 ## transform frequency table to casewise data myData &lt;- do.call(rbind, lapply(1:nrow(dd), function(RR) { data.frame(rep(1, dd$w[RR]) %*% as.matrix(dd[RR, 1:3])) })) Predictor: intervention (1 = treatment, 0 = control) Mediator: intention to smoke in the following 2 months (measured 6 months after intervention) 0 = No, 1 = I dont think so, 2 = Probably, 3 = Yes Outcome: ciguse in previous month (1 = smoked, 0 = not), measured a 1-year follow-up 11.1.2 Summarize Data The resulting data.frame looks like this: ## intention intervention ciguse ## 174 1 0 1 ## 812 0 0 0 ## 413 0 1 0 ## 300 0 1 0 ## 410 0 1 0 The marginal frequencies for each variable are: ## table for each variable lapply(myData, table) ## $intention ## ## 0 1 2 3 ## 644 103 60 57 ## ## $intervention ## ## 0 1 ## 371 493 ## ## $ciguse ## ## 0 1 ## 708 156 Lets begin with a frequency table to estimate the proportion of subjects who smoke cigarettes (i.e., the estimated probability of smoking: \\(\\widehat{\\pi}\\)) in each combination of categories for the predictors. tab3way &lt;- table(myData) probs &lt;- prop.table(tab3way, 1:2)[,,2] addmargins(probs, FUN = mean) ## Margins computed over dimensions ## in the following order: ## 1: intention ## 2: intervention ## intervention ## intention 0 1 mean ## 0 0.11583012 0.08311688 0.09947350 ## 1 0.26530612 0.20370370 0.23450491 ## 2 0.58823529 0.42307692 0.50565611 ## 3 0.68965517 0.67857143 0.68411330 ## mean 0.41475668 0.34711723 0.38093696 The treatment group (intervention == 1) smokes less, both on average and within each intention group. Smoking also increases with intent to smoke (both on average and within treatment/control groups). Treatment appears least effective among those who were certain they would continue smoking (row 4). 11.2 Regression Models for Binary Variables Suppose we used a linear model to predict the probability of smoking, treating intention as a continuous predictor to estimate only its linear effect. mod.lin &lt;- lm(ciguse ~ intervention + intention, data = myData) coef(mod.lin) ## (Intercept) intervention intention ## 0.1177588 -0.0439528 0.1927033 The estimated intercept indicates the predicted probability of smoking when predictors are zero (i.e., those in the control group with no intent to smoke). The estimated slopes reflect what we saw in the table: treatment reduces the probability of smoking (given the intent to smoke), and intent to smoke is associated with more smoking (given treatment). Note: No interaction is included here, but you can test for yourself that it was not significant. It would be problematic if the model predicted any probabilities outside the natural 01 range. We do not observed that problem in this specific sample: summary(fitted(mod.lin)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.07381 0.07381 0.11776 0.18056 0.26651 0.69587 This limitation of linear probability models is resolved with a link function in a GLM that allows for predicted values along the entire real-number line. Two such transformations are in frequent use: The probit (probability unit) transformation dates back to Bliss (1934). A probability can be transformed to a corresponding quantile in a cumulative distribution function (CDF), such as the standard-normal distribution. In other words, any probability \\(p\\) between 01 has a corresponding \\(z\\) score, such as can be requested from qnorm(p). Likewise, any predicted value on the probit scale can be transformed back into a probability using pnorm(probit). The GLM takes the form below, where \\(\\Phi()\\) is the standard-normal CDF, \\(\\Phi^{-1}()\\) is its inverse, and \\(\\mathbf{XB}\\) is the linear predictor (\\(\\widehat{y}\\)): \\[\\text{Probit}(y=1) = \\Phi(\\pi) = z = \\mathbf{XB}\\] \\[\\text{Pr}(y=1) = \\pi = \\Phi^{-1}(z)\\] The logit (logistic unit) transformation is often favored because its coefficients can be interpreted on the (natural-)log-odds scale. Recall the odds of an outcome is a ratio of its probability (\\(\\pi\\)) to its complement (\\(1-\\pi\\)), so its range is 0 to \\(+\\infty\\) (i.e., it is merely bound to nonnegative numbers). The natural (i.e., base-\\(e\\)) log of a nonnegative number can take any real number. The formulas below show how probabilities, odds, and logits are related. \\[\\text{Logit}(\\pi) = \\ln(\\text{odds}) = \\ln \\Bigg(\\frac{\\pi}{1 - \\pi} \\Bigg) = \\mathbf{XB}\\] \\[\\text{odds} = \\frac{\\pi}{1 - \\pi} = e^{\\text{Logit}(\\pi)} \\text{ , where } \\; \\; e = 2.7182818\\] \\[\\pi = \\frac{\\text{odds}}{1 + \\text{odds}} = \\frac{e^\\text{Logit}}{1 + e^\\text{Logit}}\\] There is a logit() function in the psych package, but it is simple enough to calculate the transformation (and its inverse): logit &lt;- log( p / (1-p) ) # odds &lt;- p / (1-p) p &lt;- exp(logit) / (1 + exp(logit)) # odds &lt;- exp(logit) Both probit and logistic regression models can be estimated using the glm() function, demonstrated below. 11.2.1 Fit a Logistic Regression Model In order to compare the results of fitting the logistic and the probit regression models, we first fit the GLM with the logit link. We continue to treat the ordinal mediator intention as continuous for now. mod.logit &lt;- glm(ciguse ~ intervention + intention, data = myData, family = binomial(&quot;logit&quot;)) coef(mod.logit) ## (Intercept) intervention intention ## -2.0177349 -0.3771387 1.0379733 11.2.2 Fit a Probit Regression Model mod.probit &lt;- glm(ciguse ~ intervention + intention, data = myData, family = binomial(&quot;probit&quot;)) coef(mod.probit) ## (Intercept) intervention intention ## -1.1894765 -0.2030216 0.6081512 11.2.3 Compare Models The GLMs make very similar predictions. We can compare all 3 models (linear, logit and probit) predictions to the observed probabilities in each group. ## intention intervention p.obs p.linear p.logit p.probit ## 1 3 1 0.67857143 0.65191604 0.67239696 0.66711311 ## 3 3 0 0.68965517 0.69586883 0.74954459 0.73727830 ## 5 2 1 0.42307692 0.45921270 0.42093726 0.43007009 ## 7 2 0 0.58823529 0.50316549 0.51454880 0.51070069 ## 9 1 1 0.20370370 0.26650936 0.20474455 0.21641830 ## 11 1 0 0.26530612 0.31046215 0.27293908 0.28051061 ## 13 0 1 0.08311688 0.07380601 0.08356445 0.08188581 ## 15 0 0 0.11583012 0.11775881 0.11735341 0.11712611 Notice how similar the predicted probabilities are across models, and how close they are to the observed proportions in each group. Each of these models estimates the same number of parameters, but their functional form differs (identity, logit, and probit link functions, with different models for error). So we can only compare the fit of each model to the data descriptively. Here are two examples: (1) sum the squared differences between each models predicted probabilities and the observed probabilities, summarized by Pearsons \\(\\chi^2\\) statistic (lower fits better): ## p.linear p.logit p.probit ## 0.04094642 0.01564608 0.01676192 Or compare their Tjurs pseudo-\\(R^2\\) values (higher fits better): ## linear logit probit ## 0.2042811 0.2074272 0.2063500 In both cases, the logit model performs slightly better than probit, both of which perform better than the linear model. But the discrepancies are small. 11.3 Latent Response Variable In SEM, the probit model is often preferred because predicted values are \\(z\\) scores that can be transformed into probabilities. This lends itself easily to a latent-response interpretation, which can be stated as follows for the cigarette-use example. Each subject has a latent propensity to smoke. This propensity is a continuous, normally distributed trait. Subjects whose propensity exceeds a threshold succumb to smoking, whereas those whose propensity does not exceed the threshold refrain from smoking. The probit model above can therefore be reframed in terms of this LRV, \\(y^*\\). \\[y^* = \\mathbf{XB} + \\varepsilon ,\\] where \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma)\\) and the observed binary response \\(y\\) is linked to the LRV by a threshold model: \\[y = I(y^* &gt; \\tau),\\] where the indicator function \\(I()\\) assigns 1 when its argument is TRUE and 0 when it is FALSE. Because the LRV is unobserved (i.e., latent), its distributional parameters cannot be estimated from the data. GLM software like the glm() function typically identify the model by fixing the residual variance \\(\\sigma=1\\) and the threshold \\(\\tau=0\\). For simplicity, consider an intercept-only model for cigarette use. mod0 &lt;- glm(ciguse ~ 1, data = myData, family = binomial(&quot;probit&quot;)) coef(mod0) ## (Intercept) ## -0.9132499 In lavaan the default is instead to fix the intercept to 0 and estimate the threshold. The model syntax below shows how thresholds are specified with the pipe (vertical bar: |) followed by t1 indicating the first threshold. mod0.ciguse &lt;- &#39;ciguse | t1&#39; # specify first (and only) threshold fit0.ciguse &lt;- sem(mod0.ciguse, data = myData) summary(fit0.ciguse, header = FALSE, nd = 7) ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse 0.0000000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse|t1 0.9132499 0.0498031 18.3372246 0.0000000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse 1.0000000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) ## ciguse 1.0000000 Binary variables have only 1 threshold that splits a normal distribution into 2 categories. Notice that it is arbitrary whether the intercept (mean) or threshold is fixed to 0. In the output of glm() the threshold was fixed at zero and intercept was estimated to be \\(-0.913\\). In the output of lavaan the intercept is fixed at zero and the threshold was estimated to be \\(+0.913\\). In either case, the threshold is the same distance from the LRVs mean, as depicted in the figure below. Notice that the distributions are identical. Only the \\(x\\)-axis differs, reflecting the change in (arbitrary) identification constraint. 11.3.1 Ordinal Outcomes The LRV interpretation is easily extended to polytomous ordinal variables, such as intent to smoke. This is called the cumulative probit model (and link function). Any ordinal variable with categories \\(c=0, \\ldots, C\\) can be interpreted as a crude discretization of an underlying LRV, whose \\(C\\) thresholds divide the latent distribution into \\(C+1\\) categories. \\[y=c \\ \\ \\ \\text{ if } \\ \\ \\ \\tau_c &lt; y^* \\le \\tau_{c+1} \\] Because the normal distribution is unbounded, the first category starts at \\(\\tau_0 = -\\infty\\), and the last category ends at \\(\\tau_{C+1} = +\\infty\\). For example, intent to smoke has 4 categories, so there are \\(C=3\\) thresholds. We do not have to specify thresholds in lavaan model syntax if we tell lavaan which variables are ordinal using the argument ordered=. When all modeled variables are ordinal, one can use TRUE as a shortcut. fit2 &lt;- sem(&#39;ciguse ~~ intention&#39;, data = myData, ordered = TRUE) summary(fit2, header = FALSE, standardized = TRUE) ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse ~~ ## intention 0.637 0.041 15.496 0.000 0.637 0.637 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 0.000 0.000 0.000 ## intention 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse|t1 0.913 0.050 18.337 0.000 0.913 0.913 ## intention|t1 0.660 0.046 14.280 0.000 0.660 0.660 ## intention|t2 1.101 0.054 20.570 0.000 1.101 1.101 ## intention|t3 1.506 0.066 22.867 0.000 1.506 1.506 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 1.000 1.000 1.000 ## intention 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 1.000 1.000 1.000 ## intention 1.000 1.000 1.000 Notice that intent has 3 thresholds, labeled with sequential integers following the letter t. This is how they would be specified in lavaan model syntax, e.g., &#39; intention | NA*t1 + NA*t2 + NA*t3 &#39; Alternatively, set the argument auto.th=TRUE to automatically estimate all thresholds. The LRV for intention can be interpreted as the degree to which someone intends to smokesimilar to the observed variable, but the degree increases continuously rather than in discrete increments. Subjects in Category 0 do not have a latent intent to smoke that exceeds the first threshold (\\(\\hat\\tau_1\\)), so they do not indicate that they intend to smoke. Subjects will only respond that they dont know when their latent intent exceeds \\(\\hat\\tau_1=\\) 0.66 \\(SD\\)s above the mean of that distribution. Subjects must exceed \\(\\hat\\tau_2=\\) 1.101 \\(SD\\) above the mean before they begin indicating probably. Only those whose latent intent exceeds \\(\\hat\\tau_3=\\) 1.506 \\(SD\\)s above the mean do they respond firmly yes. The plot below visualizes the classification rules above. 11.3.2 Estimating Thresholds Thresholds are univariate statistics (like means). Threshold estimates are based on the frequency distribution of a categorical variable. Recall that the probit function merely transforms a (predicted) probability (\\(\\pi\\)) into a corresponding \\(z\\) score. The (standardized) thresholds are these \\(z\\) scores. We can estimate them by plugging into qnorm() the cumulative proportions (\\(\\widehat\\pi_c\\)) up to each category \\(c\\). For example, here are the (cumulative) proportions in each category of intention. p.intent &lt;- prop.table(table(myData$intention)) # proportions per category cumsum(p.intent) # cumulative proportions (up to and including each category) ## 0 1 2 3 ## 0.7453704 0.8645833 0.9340278 1.0000000 By definition, 100% of the sample was observed in any of the categories up to (and including) the highest category, so the final cumulative proportion is 1. Likewise, no one in the sample was observed in a category lower than the lowest category (by definition), so we can append a zero as the first number in this vector of cumulative proportions. Then we can find the corresponding \\(z\\) scores in a normal distribution, which are the thresholds between categories: qnorm(c(0, cumsum(p.intent))) ## 0 1 2 3 ## -Inf 0.6599915 1.1011455 1.5064783 Inf The upper and lower thresholds are \\(+/-\\infty\\) by definition, corresponding to 0 and 100%. Those never need to be estimated because they are fixed by design. The remaining 3 thresholds are actual borders between observed categories, showing how far above/below the mean a subject must be in order to indicate a particular response category. 11.3.3 Estimating Polychoric Correlations With multiple ordinal variables, we can estimate the correlations between their associated LRVs by relying on the assumption that the LRVs are normally distributed (Olsson, 1979, 1982). These correlations are called polychoric correlations (Olsson, 1979), which are bivariate statistics (like the covariance matrix). If the model also includes a continuous variable, its correlation with an ordinal variables LRV is called the polyserial correlation (Olsson, 1982). Special names for 2-category variables are tetrachoric correlation (between 2 binary variables) or biserial correlation (between a binary and continuous variable). For example, the estimated covariance between cigarette use and intent to smoke in the model above is a polychoric correlation because each LRVs \\(SD\\) was fixed to 1 in that unrestricted (saturated) model. The reason that the probit model is so popular in SEM is that the standard SEM matrices and interpretations still apply to normal data, except those normal variables are latent. All we need to do is append the SEM with a threshold model in order to link these normal LRVs to their observed discrete counterparts. The default saturated model in lavaan estimates all thresholds and polychoric (and polyserial, if applicable) correlations, along with means and (co)variances among any continuous variables. This is used as input data when fitting any hypothesized model (even when fitting a regression model with \\(df=0\\)). We can see the estimated sample statistics for the model above: lavInspect(fit2, &quot;sampstat&quot;) ## $cov ## ciguse intntn ## ciguse 1.000 ## intention 0.637 1.000 ## ## $mean ## ciguse intention ## 0 0 ## ## $th ## ciguse|t1 intention|t1 intention|t2 intention|t3 ## 0.913 0.660 1.101 1.506 Notice that the means are 0 and variances are 1, consistent with the correlation metric and the assumption that the underlying LRV is a \\(z\\) score. These values are actually fixed to identify the saturated model. We will discuss scaling constraints and identification of latent variables more when we introduce the common-factor model. 11.3.4 Estimating an SEM with Estimated Input The problem with treating thresholds and polychoric correlations as observed data is that they were not observed. They are estimates of population parameters, based on the data. They each have an associated \\(SE\\) and 95% CI. In order to trust the \\(SE\\)s, 95% CIs, and test statistics in our hypothesized model, we need to take the uncertainty of the data into account. This is often accomplished using weighted least-squares estimation, where the weight matrix \\(\\mathbf{W}\\) is the sampling covariance matrix of the estimated thresholds and polychoric correlations. Note: Do not confuse the sampling covariance matrix of estimated parameters with the covariance matrix of variables. The latter is used as input data, and it is what we want our SEM to explain. The sampling covariance of parameter estimates is how we quantify their uncertainty: its diagonal contains sampling variances, the square-roots of which are the \\(SE\\)s that we use to calculate Wald \\(z\\) tests and CIs. The more variables (and categories) there are, the larger \\(\\mathbf{W}\\) becomes: there is one row/column for every estimated threshold and correlation. Even in samples as large as 1000, this makes estimation unstable. An alternative is to simply ignore the sampling covariances during estimation, so \\(\\text{diag}(\\mathbf{W})\\) is the weight matrix to obtain point estimates of parameters. This is called diagonally weighted least squares (estimator = \"DWLS\"), which is the default in lavaan. Even with DWLS, the full weight matrix is still needed to calculate \\(SE\\)s and the \\(\\chi^2\\) statistic. The \\(\\chi^2\\) statistic needs to be robust against the uncertainty of the input data, so it is adjusted by scaling it (a mean-adjusted statistic) and optionally shifting it (a mean- and variance-adjusted statistic). The default is both: estimator = \"WLSMV\" is a shortcut that implies lavaan(..., estimator = \"DWLS\", se = \"robust.sem\", test = \"scaled.shifted\") Another alternative is unweighted/ordinary least squares (ULS), where \\(\\mathbf{W}\\) is an identity matrix. This often yields good point estimates, but Type I error rates can differ from the \\(\\alpha\\) level, so it is still recommended to request a scaled/shifted \\(\\chi^2\\) statistic (e.g., estimator = \"ULSMV\"). There are also likelihood-based estimators. One option is marginal maximum likelihood (estimator = \"MML\"), which is currently disabled in lavaan because it was too computationally intensive and the developer is exploring better routines. A less computationally intensive alternative is pairwise maximum likelihood (estimator = \"PML\"), which maximizes only the (sum of) uni- and bivariate (log-)likelihoods. This is a quite robust procedure even in small samples, and because it is likelihood-based, information criteria (AIC and BIC) are available. Note: The robust statistics are only corrected for the two-stage estimation procedure (thresholds+pollychorics, followed by SEM parameters using DWLS or ULS) or for combining pairwise log-likelihoods (using PML). The LRV interpretation for categorical outcomes assumes a normal distribution for latent responses. Because LRVs are (by definition) unobserved, there is no way to estimate their degree of nonnormality, which would be necessary to correct for it. 11.3.4.1 Counting Degrees of Freedom For SEM with only continuous variables, the number of observed summary statistics \\(p^*\\) is a simply function of the number of variables \\(p\\): \\[p^* = \\frac{p(p+3)}{2} \\text{ (with mean structure) or } \\frac{p(p+1)}{2} \\text{ (without means)}\\] \\(p\\) means \\(p\\) variances \\(\\frac{p(p-1)}{2}\\) covariances The number of observed covariances does not differ between continuous and categorical variables, but some covariances are replaced by: polychoric correlations among LRVs (scaled) polyserial correlations between LRVs and observed variables But the number of univariate statistics can differ. Whereas continuous variables each contribute an observed mean and variance, categorical variables do not contribute either (i.e., those values are fixed to 0 and 1, respectively, to estimate thresholds and polychoric correlations). Instead, each categorical variable contributes \\(C\\) observed standardized thresholds for its \\(C+1\\) categories. So each binary variable contributes \\(C=1\\) threshold, each ternary (3-category) variable contributes \\(C=2\\) thresholds, and so on. Thus, the number summary statistics is a function of the number of observed continuous and discrete variables. Suppose we denote: \\(p_c\\) the number of continuous variables \\(p_d\\) the number of discrete variables so the total number of observed variables remains \\(p = p_c + p_d\\) the number of thresholds for discrete variables \\(d = 1, \\dots, D\\) is \\(C_d\\) the total number of thresholds is \\(K = \\sum_{d=1}^D C_d\\) Then the number summary statistics is \\(\\frac{p(p-1)}{2} + 2p_c + K\\). That is, both continuous and discrete variables contribute covariances/correlations (first term), only continuous variables contribute a mean and variance (second term), and only discrete variables contribute thresholds (third term). This has implications for determining the expected \\(df\\), which is an important part of verifying that you fit the model you intended to fit. Just as mean structures are typically saturated (e.g., for every observed mean, an intercept is estimated), each observed standardized threshold has a corresponding estimated threshold (not necessarily standardized, but is by default in lavaan). Likewise, intercepts and (residual or marginal) variances typically remain fixed for LRVs in hypothesized models, as they do in the default saturated model. Thresholds might be constrained (contributing \\(df\\)) in models for multiple groups or occasionsas might LRV intercepts or (residual or marginal) varianceswhich we discuss in later chapters. 11.3.4.2 Missing Data When multivariate-normal data are incomplete, all available information can be used to estimate parameters by setting missing = \"FIML\" (full-information ML estimation) to override the default listwise deletion method. Actually, FIML applies to nonnormal continuous data as well, since a robust correction for nonnormality is available with FIML results (estimator = \"MLR\"). FIML is also available for categorical outcome(s), but only when using (marginal) ML estimation. Until estimator = \"MML\" becomes available in lavaan, the FIML option is unfortunately unavailable. When at least 1 endogenous variable is binary/ordinal, missing = \"pairwise\" deletion retains as much information as possible, so it is still preferred over the default missing = \"listwise\" deletion. However, both deletion methods make the restrictive assumption that data are missing completely at random (MCAR). The reason pairwise deletion assumes MCAR data is that for each pair of variables, it estimates their covariance (or polychoric/polyserial correlation) using all jointly observed complete data for that pair of variables. So no information from other observed variables can be used to estimate the covariance/correlation. In contrast, FIML estimates parameters using each persons entire vector of observed data, so missing data on Variable A can be compensated by observed data on Variable B. Thus, FIML makes the less restrictive assumption that data are missing at random (MAR)that is, the missingness is still random conditional on the observed data (see Little et al., 2014, for more detailed conceptual explanations). Pairwise deletion is available with any (diagonally or un)weighted least-squares estimators, as well as with (P)ML. But with PML for categorical outcomes, a missing = \"doubly.robust\" estimation method is available that only requires assuming data are MAR (Katsikatsou et al., in press). It is computationally intensive, but it may be worth it if the MAR assumption is not feasible. 11.4 Mediation Model with Categorical Outcomes Consider the mediation model depicted below. The dashed line is the direct effect of treatment on smoking behavior, given the intent to smoke. Because SEM treats an exogenous binary predictor as numeric (like dummy-coded predictors in standard OLS regression models), these \\(p=3\\) variables include \\(p_c=1\\) continuous predictor (intervention) and \\(p_d=2\\) discrete outcomes. These contribute \\(p^* = \\frac{p(p-1)}{2} = 3\\) observed covariances: The polyserial correlations of intervention with ciguse and intention The polychoric correlation between ciguse and intention The predictor also contributes \\(2p_c=2\\) observed values: a mean and a variance. The endogenous binary variable ciguse contributes 1 threshold, and the endogenous ordinal variable intention contributes 3 thresholds, so \\(K=4\\). Thus, there are 3 + 2 + 4 = 9 summary statistics in this 3-variable system. There are 3 possible conclusions: No mediation: the indirect effect \\(\\beta_{21} \\times \\beta_{32}=0\\) because either \\(\\beta_{21}=0\\) or \\(\\beta_{32}=0\\) Partial mediation: the indirect effect \\(\\beta_{21} \\times \\beta_{32} \\ne 0\\) and the direct effect \\(\\beta_{31} \\ne 0\\) Full mediation: the indirect effect \\(\\beta_{21} \\times \\beta_{32} \\ne 0\\) but the direct effect \\(\\beta_{31} = 0\\) Omitting the dashed line (by fixing \\(\\beta_{31}=0\\)$) would imply full mediation. A full-mediation model would estimate 8 parameters: 2 regression slopes (\\(\\beta_{21}\\) and \\(\\beta_{32}\\)) 1 mean and 1 variance for the exogenous dummy coded intervention 3 thresholds for the mediator intention 1 threshold for the outcome ciguse Thus, the full-mediation model will have \\(df=1\\). The partial-mediation model would estimate the dashed line, reducing \\(df=0\\). No mediation could be represented by a model that estimates the dashed line but fixes either \\(\\beta_{21}=0\\) (a multiple-regression model for ciguse) or \\(\\beta_{32}=0\\) (a multivariate regression model for intention and ciguse). We will only fit the full and partial mediation models in the next section. 11.4.1 Estimate Mediation Models Begin by fitting the partial-mediation model to the data, to test \\(H_0: \\beta_{21} \\times \\beta_{32}=0\\) (no mediation). We will only specify the regression slopes and user-defined parameters because the residual variances are not estimated. The identification constraints require more explanation (see the following section). mod.part &lt;- &#39; ## regression paths intention ~ b21*intervention ciguse ~ b31*intervention + b32*intention ## define indirect and total effects ind := b21*b32 tot := ind + b31 &#39; fit.part &lt;- sem(mod.part, data = myData, ordered = TRUE) summary(fit.part, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-11 ended normally after 13 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 864 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.000 0.000 ## Degrees of freedom 0 0 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## intention ~ ## intrvntn (b21) -0.246 0.089 -2.758 0.006 -0.246 -0.121 ## ciguse ~ ## intrvntn (b31) -0.130 0.093 -1.402 0.161 -0.130 -0.064 ## intentin (b32) 0.631 0.042 15.105 0.000 0.631 0.629 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .intention 0.000 0.000 0.000 ## .ciguse 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## intention|t1 0.525 0.067 7.844 0.000 0.525 0.521 ## intention|t2 0.970 0.071 13.572 0.000 0.970 0.963 ## intention|t3 1.378 0.082 16.710 0.000 1.378 1.368 ## ciguse|t1 0.760 0.072 10.491 0.000 0.760 0.752 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .intention 1.000 1.000 0.985 ## .ciguse 0.602 0.602 0.591 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## intention 1.000 1.000 1.000 ## ciguse 1.000 1.000 1.000 ## ## R-Square: ## Estimate ## intention 0.015 ## ciguse 0.409 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ind -0.155 0.057 -2.713 0.007 -0.155 -0.076 ## tot -0.285 0.100 -2.845 0.004 -0.285 -0.140 Using the standard criterion \\(\\alpha=5\\%\\), the Wald \\(z\\) tests allow us to reject the \\(H_0\\) of no mediation: indirect effect \\(b\\) = \\(-0.155\\), \\(z = -2.713\\), p = 0.007, \\(\\beta = -0.076\\). We fail to reject the \\(H_0\\) of full mediation: direct effect b = \\(-0.13\\), \\(z = -1.402\\), p = 0.161, \\(\\beta = -0.064\\). Because the scales of the LRVs are arbitrary, it is safer to interpret the standardized slopes test a \\(H_0\\) using a \\(\\Delta \\chi^2\\) test We can fit a full-mediation model to the data to obtain the LRT. mod.full &lt;- &#39; ## regression paths intention ~ b21*intervention ciguse ~ b32*intention ## define indirect and total effects ind := b21*b32 &#39; fit.full &lt;- sem(mod.full, data = myData, ordered = TRUE) lavTestLRT(fit.full, fit.part) # or anova(), which then calls lavTestLRT() ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.part 0 0.0000 ## fit.full 1 1.2648 1.9567 1 0.1619 Notice that lavTestLRT() automatically detects the robust estimator and test statistic requested when fitting the models, so it appropriately uses the same robust correction (mean- and variance-adjustment) for the \\(\\Delta \\chi^2\\) statistic. This is why the Chisq Diff column does not match the difference between (uncorrected) \\(\\chi^2\\) values in the Chisq column. The \\(p\\) value is very close to the Wald tests \\(p\\) value (also calculated using a robust \\(SE\\)), which is expected when \\(N\\) is large. 11.4.2 Interpreting Coefficients Notice that the glm() estimates from the probit regression for ciguse do not match those from the sem() function. There are noteworthy differences in parameterization: The GLM approach fixes the threshold to 0 and estimates the intercept. The SEM approach fixes the intercept to 0 and estimates the threshold. As shown in the figure above, this distinction is arbitrary because the LRV has no location that we can identify from the data. The intercept and threshold can only be said to be a certain distance apart, which is why they take the same absolute value. For example, we can free the ciguse intercept in lavaan syntax and fix its threshold instead: ## fixed intercept coef(fit.part, type = &quot;all&quot;)[c(&quot;ciguse|t1&quot;,&quot;ciguse~1&quot;)] ## ciguse|t1 ciguse~1 ## 0.7596909 0.0000000 ## fixed threshold mod.trade &lt;- &#39; ciguse | 0*t1 # fix threshold ciguse ~ NA*1 # estimate intercept &#39; coef(sem(c(mod.part, mod.trade), data = myData, ordered = TRUE), type = &quot;all&quot;)[c(&quot;ciguse|t1&quot;,&quot;ciguse~1&quot;)] ## ciguse|t1 ciguse~1 ## 0.0000000 -0.7596909 The GLM approach treats all predictors as fixed, and predictors are distinct from outcomes. Intent to smoke was treated as continuous. Although we could have used polynomial contrast codes to estimate linear, quadratic, and cubic effects, we still would have to take the observed category weights (03) as given. The SEM approach allows outcomes to predict other outcomes. So the LRV underlying intent to smoke was used as a predictor of smoking propensity (LRV underlying ciguse). This allows us to extrapolate from our results what we would expect if we had measured intent as a continuous variable (and it happened to be normally distributed). A final distinction has to do with the identification constraint on the LRV scale. 11.4.3 Delta vs. Theta Parameterizations The SEM approach in lavaan (by default) fixes the marginal (i.e., total) variance of the LRV to 1. This is called the delta parameterization in the SEM literature. The GLM approach fixes the residual/conditional variance to 1. This is called the theta parameterization in the SEM literature. Under the delta parameterization, the residual variance is fixed such that it equals 1 minus the explained variance (\\(1-R^2\\)). Under the theta parameterization, the LRVs total variance is the sum of explained and unexplained variance (1 + explained variance). The total variances is not an explicit SEM parameter; instead, the scaling factors (Scales y* in the output) are the reciprocal of the total \\(SD\\) (i.e., \\(\\frac{1}{SD}\\)). Use the argument parameterization = \"theta\" to override the default delta parameterization, if there is a reason to do so (i.e., if you want to model residual variances). This will be discussed further in the chapter about item factor analysis (i.e., common-factor models for categorical indicators). This arbitrary distinction can have important consequences for the interpretation and decomposition of mediated effects. For example, the indirect effects would not be comparable between full- and partial-mediation models when fixing residual variances to 1, because doing so would change the scale of the LRV across the two models. That is, the relative amount of residual variance would be different for the full- and partial mediation models. The partial-mediation model explains less variance (lower \\(R^2\\), so its residual variance should be larger) than the full-mediation model, but residual variances are fixed to 1 in both cases, making the total variance appear smaller in the partial-mediation model. This change of scales changes the meaning of one unit increase across the models. Luckily, when using the delta parameterization in single-group models, the outcomes LRV scale (and any categorical mediators LRV scale) is already held constant by fixing the marginal variance to 1 (Breene et al., 2013). However, this good news would not apply in more complex situations that affect LRV scales, such as multigroup models. So when using path analysis with categorical outcomes, it is best to exercise caution. Use the delta parameterization, and do not use a multigroup models to compare paths across groups (i.e., moderated mediation) unless you understand how to equate the LRV scales across groups. 11.4.4 Decomposing Total Effects Furthermore, a common method of quantifying the size of an indirect effect is the proportion of the total effect: \\(\\frac{\\beta_{21} \\times \\beta_{32}}{(\\beta_{21} \\times \\beta_{32}) + \\beta_{31}}\\). This is already problematic when the direct and indirect effects have opposite signs (called inconsistent mediation). But when the scale of a LRV varies between models with(out) a mediator, then the sum of estimated direct and indirect effects would no longer match the total effect. For example, fit separate probit models for the mediator and outcome, as would be necessary using the GLM approach. For simplicity, only treat ciguse as categorical. ## simple regression to obtain the total effect (total &lt;- coef(sem(&#39;ciguse ~ total*intervention&#39;, data = myData, ordered = &#39;ciguse&#39;, parameterization = &quot;theta&quot;))[&quot;total&quot;]) ## total ## -0.2850428 ## model for mediator (b21 &lt;- coef(sem(&#39;intention ~ b21*intervention&#39;, data = myData))[&quot;b21&quot;]) ## b21 ## -0.1644697 ## model for outcome (b3 &lt;- coef(sem(&#39;ciguse ~ b31*intervention + b32*intention&#39;, data = myData, ordered = &#39;ciguse&#39;, parameterization = &quot;theta&quot;))[c(&quot;b31&quot;,&quot;b32&quot;)]) ## b31 b32 ## -0.2030216 0.6081512 ## estimate total effect from decomposition b3[[1]] + b21*b3[[2]] ## b21 ## -0.303044 This does not match the total effect from the simple probit regression because the LRV scales differ. That is, the simple regression has more residual variance (less explained variance without intention in the model), yet the residual variance is still fixed to 1. Imai et al. (2010) proposed a general solution to this problem, using the causal-modeling framework, implemented in the mediation (see their `vignette for examples). It is quite complicated, but here is how it would apply to our current example (see also Muthén, 2011, for corresponding Mplus syntax): mod.Imai &lt;- &#39; ciguse ~ c*intervention + b*intention intention ~ a*intervention # label threshold for ciguse ciguse | b0*t1 # biased SEs naive.indirect := a*b naive.direct := c # correct probit11 := (-b0+c+b*a)/sqrt(b^2+1) probit10 := (-b0+c )/sqrt(b^2+1) probit00 := (-b0 )/sqrt(b^2+1) indirect := pnorm(probit11) - pnorm(probit10) direct := pnorm(probit10) - pnorm(probit00) &#39; fit &lt;- sem(mod.Imai, data = myData, ordered = c(&quot;ciguse&quot;,&quot;intention&quot;)) summary(fit, std = TRUE) ## lavaan 0.6-11 ended normally after 13 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 864 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.000 0.000 ## Degrees of freedom 0 0 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse ~ ## interventn (c) -0.130 0.093 -1.402 0.161 -0.130 -0.064 ## intention (b) 0.631 0.042 15.105 0.000 0.631 0.629 ## intention ~ ## interventn (a) -0.246 0.089 -2.758 0.006 -0.246 -0.121 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ciguse 0.000 0.000 0.000 ## .intention 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse|t1 (b0) 0.760 0.072 10.491 0.000 0.760 0.752 ## intntn|t1 0.525 0.067 7.844 0.000 0.525 0.521 ## intntn|t2 0.970 0.071 13.572 0.000 0.970 0.963 ## intntn|t3 1.378 0.082 16.710 0.000 1.378 1.368 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ciguse 0.602 0.602 0.591 ## .intention 1.000 1.000 0.985 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ciguse 1.000 1.000 1.000 ## intention 1.000 1.000 1.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## naive.indirect -0.155 0.057 -2.713 0.007 -0.155 -0.076 ## naive.direct -0.130 0.093 -1.402 0.161 -0.130 -0.064 ## probit11 -0.884 0.062 -14.182 0.000 -0.884 -0.755 ## probit10 -0.752 0.070 -10.714 0.000 -0.752 -0.691 ## probit00 -0.643 0.063 -10.184 0.000 -0.643 -0.637 ## indirect -0.037 0.014 -2.647 0.008 -0.037 -0.020 ## direct -0.034 0.024 -1.403 0.161 -0.034 -0.017 However, Breen et al. (2013) proposed a simpler, less restrictive solution that holds the LRV scale consistent so that the decomposition holds. Their method is meant for researchers fitting separate regression models, and performed quite well in simulations (and similar to Imai et al.s method). Luckily, when using the delta parameterization in single-group models, their method is unnecessary when simultaneously estimating the full/partial-mediation model as a path analysis. This is because the outcomes LRV scale (and any categorical mediators LRV scale) is already held constant by fixing the marginal variance to 1. However, this good news would not apply in more complex situations that affect LRV scales, such as multigroup models. So when using path analysis with categorical outcomes, it is best to exercise caution. Use the delta parameterization, and do not use a multigroup models to compare paths across groups (i.e., moderated mediation) unless you understand how to equate the LRV scales across groups. 11.5 References Bliss, C. I. (1934). The method of probits. Science, 79(2037), 3839. https://doi.org/10.1126/science.79.2037.38 Breen, R., Karlson, K. B., &amp; Holm, A. (2013). Total, direct, and indirect effects in logit and probit models. Sociological Methods &amp; Research, 42(2), 164191. https://doi.org/10.1177/0049124113494572 Katsikatsou, M., Moustaki, I., &amp; Jamil, H. (in press). Pairwise likelihood estimation for confirmatory factor analysis models with categorical variables and data that are missing at random. British Journal of Mathematical and Statistical Psychology. https://doi.org/10.1111/bmsp.12243 Little, T. D., Jorgensen, T. D., Lang, K. M., &amp; Moore, E. W. G. (2014). On the joys of missing data. Journal of Pediatric Psychology, 39(2), 151162. https://doi.org/10.1093/jpepsy/jst048 Muthén, B. (2011). Applications of causally defined direct and indirect effects in mediation analysis using SEM in Mplus [Technical report]. Retrieved from http://www.statmodel.com/download/causalmediation.pdf Olsson, U. (1979). Maximum likelihood estimation of the polychoric correlation coefficient. Psychometrika, 44(4), 443460. https://doi.org/10.1007/BF02296207 Olsson, U., Drasgow, F., &amp; Dorans, N. J. (1982). The polyserial correlation coefficient. Psychometrika, 47(3), 337347. https://doi.org/10.1007/BF02294164 "],["fitting-path-models-with-multilevel-data.html", "12 Fitting path models with multilevel data 12.1 Intraclass correlation 12.2 Multilevel structure as a nuisance: Correcting for the dependency 12.3 Two-level path models 12.4 Obtaining ICCs, and estimates of \\(\\Sigma_\\text{W}\\) and \\(\\Sigma_\\text{B}\\) 12.5 Fitting a two-level path model 12.6 References", " 12 Fitting path models with multilevel data Research in the field of child development or education often involves nested observations. For example, data may be gathered by first selecting schools, and then selecting students within those schools, or researchers may select families and then children within those families. Such two-stage sampling schemes lead to individual observations that are not statistically independent. Two students from the same classroom have shared experiences (e.g. the same teacher, same classmates, same neighborhood where they live) that may make their responses more similar to each other than the responses obtained from two students from different classrooms. This dependency leads to structural differences across classrooms (for example, some classrooms have higher average math achievement because they have a better teacher). It is common to call the highest level of differences (the classroom-level in this example) the between level or Level 2, and the lowest level (the student-level here) the within level or Level 1. There exist several ways to evaluate path models with multilevel data, depending on the research question. All methods require analysis of raw data. Sometimes the research question involves all Level 1 variables, and the fact that the data have a nested structure are just the result of the sampling design. In such a case, the multilevel structure is regarded a nuisance that the researcher actually wants to get rid of. Alternatively, there may be research questions that involve variables that operate at both levels of analysis. For example, a researcher may be interested in the effect of students self-esteem on student achievement, and also in the effect of the average self-esteem in the classroom on the average student achievement. In such a situation the multilevel nature of the data is regarded as interesting, and one would analyse a model that features relations at multiple levels. 12.1 Intraclass correlation Intraclass correlations (ICCs) indicate what proportion of a variables variance exists at Level 1 and what part exists at Level 2. For example, if the ICC of some variable is 0.10, that indicates that 10% of the total variance is caused by differences between clusters (classrooms), and 90% is caused by differences between the individuals within those clusters. The ICC value can also be interpreted as the correlation that you would expect between the scores of two individuals that are part of the same cluster. When you see the multilevel structure as a nuisance, you hope that the ICCs of your variables are small. If you have research questions involving Level 2 variables, you hope that the ICCs of those variables are relatively high. Script 13.1 shows how to obtain the ICCs using lavaan. 12.2 Multilevel structure as a nuisance: Correcting for the dependency The problem with multilevel data is that the individual observations are not independent. For example, when you observe scores from 100 individuals that are clustered in groups of 10 people, and the ICCs of the measured variables are not zero, that means that there is overlap in information that you obtain from individuals within clusters. The effective sample size is then actually smaller than 100. If you would ignore the multilevel structure, you are effectively overstating the information that you have. In practice, that means that the \\(SE\\)s will be too small, and that the \\(\\chi^2\\) statistic will be too large. One solution is to correct the \\(SE\\)s and fit statistics for the dependency in the data. If you specify a single-level model in lavaan, but you add the argument cluster = clustervariable, then lavaan will report cluster-robust \\(SE\\)s (Williams, 2000) and a corrected test statistic. This would be an acceptable approach when your hypotheses are about Level-1 processes, and you just want to correct for the nested data. 12.3 Two-level path models If a research question involves variables at Level 2 as well as at Level 1, one would conduct two-level path analysis. When fitting two-level models in lavaan, each observed variable is decomposed into a within component and a between-component. For example, Given the multivariate response vector \\(y_{ij}\\), with scores from subject i in cluster j, the scores are decomposed into means (\\(\\mu_j\\)), and individual deviations from the cluster means (\\(\\eta_{ij}\\)): \\[\\begin{equation} \\begin{split} y_{ij} &amp;= \\mu_j + (y_{ij} - \\mu_j) \\\\ &amp;= \\mu_j + \\eta_{ij} \\end{split} \\end{equation}\\] where \\(\\mu_j\\) and \\(\\eta_{ij}\\) are independent. The overall covariances of \\(y_{ij}\\) (\\(\\Sigma_{total}\\)) can be written as the sum of the covariances of these two components: \\[\\begin{equation} \\begin{split} \\Sigma_{total} &amp;= COV(\\mu_j, \\mu_j) + COV(\\eta_{ij}, \\eta_{ij}) \\\\ &amp;= \\Sigma_\\text{between} + \\Sigma_\\text{within} \\\\ &amp;= \\Sigma_\\text{B} + \\Sigma_\\text{W} \\end{split} \\end{equation}\\] One can postulate separate models for \\(\\Sigma_{B}\\) and \\(\\Sigma_{W}\\). This model specification is denoted the within/between formulation (Muthén, 1990, 1994; Schmidt, 1969), and implies random intercepts for all observed variables. The observed variables can have variance at one or both of the levels in two-level data. For example, in data from children in school classes, the variable Teacher gender only has variance at Level 2, since all children in the same school class share the same teacher. The gender of the child varies within school classes, and will have variance at Level 1, but not at Level 2, in cases where the distribution of boys and girls is equal across classes. In practice, variables that have variance at Level 1, often also have variance at Level 2. For example, childrens scores on a mathematical ability test may differ across different children from the same school class (Level 1), while the classroom average test scores are also likely different (Level 2). As an example dataset we use scores on four variables, obtained from 1377 students in 58 schools (Schijf &amp; Dronkers, 1991; Hox, Moerbeek &amp; van de Schoot, 2017). The four variables we use are: Education of the father (Feduc), Education of the mother (Meduc), Teachers advice about secondary education (Advice), and the result on a school achievement test (Galo). We will fit the two-level model that is depicted below. Figure 13.1. A two-level path diagram 12.4 Obtaining ICCs, and estimates of \\(\\Sigma_\\text{W}\\) and \\(\\Sigma_\\text{B}\\) In script 13.1, we fit saturated models to Level 1 and Level 2, by letting all variables covary with each other at both levels. The means of the variables will be estimated only at Level 2 because Level-1 means are zero by definition. That is, Level-1 components of variables represent individual deviations from the cluster mean, and cluster-mean-centered variables have means of zero (both within clusters and across clusters). Fitting a saturated model will give us estimates of the within-level and between-level covariance matrices. The variance estimates from this model can be used to calculate ICCs. Fitting this model thus serves as a first step to obtain information about the variance distribution across levels. library(lavaan) data &lt;- read.table(&quot;GaloComplete.dat&quot;, header = TRUE) head(data) ## school galo advice feduc meduc ## 1 1 78 1 1 1 ## 2 1 104 4 4 3 ## 3 1 93 2 1 1 ## 4 1 114 4 2 1 ## 5 1 95 2 2 2 ## 6 1 98 2 1 1 satmodel &lt;- &#39; level: 1 galo ~~ advice + feduc + meduc advice ~~ feduc + meduc feduc ~~ meduc level: 2 galo ~~ advice + feduc + meduc advice ~~ feduc + meduc feduc ~~ meduc galo ~ 1 advice ~ 1 feduc ~ 1 meduc ~ 1 &#39; fitsat &lt;- lavaan(model = satmodel, data = data, cluster = &quot;school&quot;, auto.var = TRUE) The clustering variable should be specified as an argument to the lavaan() function. The keywords level: 1 (not level 1:) and level:2 (not level 2:, which would return an error) identify blocks in the model syntax, notifying lavaan that we are specifying a model at each level of analysis. If the model syntax does not contain separate models for levels 1 and 2, but the clustering variable is still specified, then lavaan will fit a single-level model and report cluster-robust \\(SE\\)s and fit statistics. One can obtain the ICC estimates for each variable, and the estimated covariance matrices at Level 1 and Level 2 from the saturated (or any) multilevel model using the following code. # ICCs (ICCs &lt;- lavInspect(fitsat,&quot;icc&quot;)) ## galo advice feduc meduc ## 0.156 0.143 0.293 0.211 # Sigma_within (Sigma_w &lt;- lavInspect(fitsat, &quot;sampstat&quot;)$within$cov) ## galo advice feduc meduc ## galo 143.572 ## advice 12.802 1.730 ## feduc 6.935 0.767 4.425 ## meduc 5.901 0.704 2.265 3.764 # Sigma_between (Sigma_b &lt;- lavInspect(fitsat, &quot;sampstat&quot;)$school$cov) ## galo advice feduc meduc ## galo 26.464 ## advice 2.607 0.289 ## feduc 5.737 0.669 1.833 ## meduc 4.133 0.470 1.345 1.009 The ICC values show that a proportion 0.156 of the total variance in the GALO achievement test scores as attributable to between-school differences. This could also be calculated manually using the variance estimates at the two levels (26.491 / (26.491 + 143.595) = .156). Educational level of the father has the largest proportion of between-school variance with an ICC of 0.293. 12.5 Fitting a two-level path model In script 13.2, we fit the two-level path model as depicted in Figure 13.1 to the data. model2 &lt;- &#39; level: 1 advice ~ galo galo ~ feduc + meduc # covariance and variances feduc ~~ meduc advice ~~ advice galo ~~ galo feduc ~~ feduc meduc ~~ meduc level: 2 advice ~ galo galo ~ feduc + meduc # covariance and variances feduc ~~ meduc advice ~~ advice galo ~~ galo feduc ~~ feduc meduc ~~ meduc galo ~ 1 advice ~ 1 feduc ~ 1 meduc ~ 1 &#39; fitmod2 &lt;- lavaan(model = model2, data = data, cluster = &quot;school&quot;) Note that this two-level path model has \\(df=4\\). In two-level models, the number of observed statistics are twice as much as in single level models because we count the information at both levels. At the within level there are 4*(4+1)/2 = 10 elements in the within-level covariance matrix, and at the between level there are also 10 elements in the between-level covariance matrix (and there are four observed means). In this example, the number of parameters to be estimated is also doubled in comparison with a single level model, because all \\(\\beta\\) and \\(\\psi\\) parameters are estimated at the within as well as at the between-level. The number of parameters may also be different across levels, when a different model is fitted to the between level and to the within level. But in this model, we estimate three \\(\\beta\\) parameters and five \\(\\psi\\) parameters at each level, leading to \\(df=2\\) per level. At the between-level there are also four intercepts (\\(\\alpha\\)) estimated based on four observed means, so the mean structure is saturated and does not add any \\(df\\) to the model. summary(fitmod2, fit.measures = TRUE) ## lavaan 0.6-11 ended normally after 105 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 1377 ## Number of clusters [school] 58 ## ## Model Test User Model: ## ## Test statistic 38.935 ## Degrees of freedom 4 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 2349.154 ## Degrees of freedom 12 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.985 ## Tucker-Lewis Index (TLI) 0.955 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -12612.714 ## Loglikelihood unrestricted model (H1) -12593.247 ## ## Akaike (AIC) 25265.428 ## Bayesian (BIC) 25369.982 ## Sample-size adjusted Bayesian (BIC) 25306.450 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.080 ## 90 Percent confidence interval - lower 0.058 ## 90 Percent confidence interval - upper 0.103 ## P-value RMSEA &lt;= 0.05 0.013 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.028 ## SRMR (between covariance matrix) 0.046 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## advice ~ ## galo 0.089 0.002 50.305 0.000 ## galo ~ ## feduc 1.092 0.180 6.060 0.000 ## meduc 0.918 0.195 4.706 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## feduc ~~ ## meduc 2.261 0.128 17.622 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .advice 0.000 ## .galo 0.000 ## feduc 0.000 ## meduc 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .advice 0.589 0.023 25.680 0.000 ## .galo 131.155 5.125 25.589 0.000 ## feduc 4.425 0.172 25.687 0.000 ## meduc 3.768 0.146 25.744 0.000 ## ## ## Level 2 [school]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## advice ~ ## galo 0.104 0.007 14.958 0.000 ## galo ~ ## feduc 10.769 16.594 0.649 0.516 ## meduc -10.290 22.314 -0.461 0.645 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## feduc ~~ ## meduc 1.359 0.280 4.853 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .galo 89.913 2.300 39.093 0.000 ## .advice -7.489 0.711 -10.538 0.000 ## feduc 3.860 0.188 20.512 0.000 ## meduc 2.850 0.143 19.899 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .advice 0.028 0.010 2.667 0.008 ## .galo 5.620 5.141 1.093 0.274 ## feduc 1.843 0.380 4.845 0.000 ## meduc 1.015 0.218 4.665 0.000 In the output, lavaan reports an overall test statistic, and several fit measures that are based on the overall model. The SRMR is provided separately for the within and between levels. For the current model exact fit is rejected, the RMSEA is 0.08 with a 90% confidence interval ranging from 0.058 to 0.103, and the CFI (for which the baseline model is the independence model at both levels) is 0.985. The output for the parameter estimates is provided first for the within-level part of the model, and then for the between-level part. Interpretations depend on the level of analysis. For example: The effect of galo on advice is estimated to be .089 at the within-level. This means that when galo scores increase by 1 point among individuals from the same school (i.e., \\(\\eta_{ij} = y_{ij} - \\mu_j\\)), individuals are expected to receive 0.089 points higher advice on average. The effect of galo on advice at the between-level is estimated to be .104. This means that when a schools average galo score (i.e., \\(\\mu_j\\)) increases by 1 point, the school is expected to receive 0.104 points higher advice on average. The difference between a variables between- and within-level effect (\\(\\beta_\\text{B} - \\beta_\\text{W}\\)) is called the contextual effect (see Marsh et al., 2012). In our example, this is the effect on received advice of being in a school with 1-unit higher average galo scores (\\(\\mu_j\\)), given an individuals own galo score (\\(y_{ij}\\)). \\[\\begin{equation} \\begin{split} \\text{advice} &amp;= \\beta_\\text{W} (y_{ij} - \\mu_j) + \\beta_\\text{B} \\mu_j \\\\ &amp;= \\beta_\\text{W} y_{ij} - \\beta_\\text{W} \\mu_j + \\beta_\\text{B} \\mu_j \\\\ &amp;= \\beta_\\text{W} y_{ij} + (\\beta_\\text{B} - \\beta_\\text{W}) \\mu_j \\end{split} \\end{equation}\\] The equivalence of within- and between-level effects can be tested by comparing a model in which the effects are constrained to be equal to a model in which they are estimated freely. Alternatively, one could define the contextual effect as a new parameter by subtracting the two effects of interest, similar to how one defines specific indirect effects. &#39; level: 1 advice ~ b43.w*galo level: 2 advice ~ b43.b*galo b43.contextual := b43.b - b43.w # obtain Wald z test of equivalence v. contextual effect&#39; In standard multilevel regression, one may also include random slopes in the model. This way one could test whether the effect of some variable on another variable varies across clusters. Random slopes are not yet a feature in lavaans multilevel SEM, although the wide-format approach might be feasible in samples with many small clusters (Barendse &amp; Rosseel, 2020). 12.6 References Barendse, M. T., &amp; Rosseel, Y. (2020). Multilevel modeling in the wide format approach with discrete data: A solution for small cluster sizes. Structural Equation Modeling, 27(5), 696721. https://doi.org/10.1080/10705511.2019.1689366 Hox, J. J., Moerbeek, M., &amp; Van de Schoot, R. (2017). Multilevel analysis: Techniques and applications. Routledge. Marsh, H. W., Ludtke, O., Nagengast, B., Trautwein, U., Morin, A. J. S., Abduljabbar, A. S., et al. (2012). Classroom climate and contextual effects: Conceptual and methodological issues in the evaluation of group-level effects. Educational Psychololgy, 47, 106124. https://doi.org/10.1080/00461520.2012.670488 Muthén, B. (1990). Mean and covariance structure analysis of hierarchical data. Los Angeles, CA: UCLA. Muthén, B.O. (1994). Multilevel covariance structure analysis. Sociological Methods &amp; Research, 22(3), 376398. https://doi.org/10.1177%2F0049124194022003006 Williams, R. L. (2000). A note on robust variance estimation for clustercorrelated data. Biometrics, 56(2), 645646. https://doi.org/10.1111/j.0006-341X.2000.00645.x Schijf, H., &amp; Dronkers, J. (1991). De invloed van richting en wijk op de loopbanen in de lagere scholen van de stad Groningen in 1971. IBH Abram, BPM Creemers &amp; A. van derLeij (red.), ORD, 91. Schmidt, W. H. (1969). Covariance structure analysis of the multivariate random effects model [Doctoral dissertation]. University of Chicago, Department of Education. "],["latent-growth-curve-models.html", "13 Latent Growth Curve Models 13.1 Prepare Data and Workspace 13.2 Random Intercept Model 13.3 Linear Growth Model 13.4 Unrestricted Growth Models 13.5 Explaining Growth Factors 13.6 Latent Indicators of Growth Factors 13.7 Multigroup Growth Models 13.8 References", " 13 Latent Growth Curve Models In structural equation modeling (SEM), multiple indicators of a construct can be modeled using exploratory/confirmatory factor analysis (E/CFA). A latent common factor (or common cause) is posited as being the source of shared variance among the observed indicators, and the measurement models for multiple common factors can be embedded within a larger SEM. This chapter introduces another type of common factor, often called a growth factor. Rather than repeatedly measuring a construct with different indicators of that construct (e.g., I feel : happy, glad, cheerful, joyous), growth factors are measured by the same indicator on multiple occasions. Growth factors are interpreted as patterns of change in the indicators expected values, where the type of pattern is defined by the factor loadings. In this chapter we first import and summarize example data, then we use a single-factor model to illustrate differences between growth factors and other common factors (e.g., specification and interpretation). Readers familiar with multilevel modeling (MLM) will benefit from a brief comparison between MLM and SEM for the same data. Next, we 2-factor model for linear growth (again comparing it to a MLM for change) and saturated growth model (comparing it to repeated-measures ANOVA). We conclude by discussing important caveats with some common extensions to the latent growth curve model (LGCM): predictors of growth latent indicators of growth comparing growth across groups 13.1 Prepare Data and Workspace library(lavaan) For illustration, we will use summary statistics reported by Duncan and Duncan (1996, pp. 329330). alcohol, cigarette, and cannabis use (thc) were each measured annually for 4 years using a 5-point scale: never used previous but not current use current use of &lt; 4 times per month) current use of 429 times per month current use of \\(\\ge 30\\) times per month We also imported 2 variables measured at the beginning of the study: age (recruited between 11 and 15 years old) peer encouragement, measured by summing 3 items (alc, cig, thc) using a 5-point Likert scale indicating how much their best friend encouraged use of that substance The following summary statistics were observed for the \\(N=321\\) adolescents who provided complete data in all 4 years. ## variable names vn &lt;- c(paste0(&quot;alc&quot;, 1:4), paste0(&quot;cig&quot;, 1:4), paste0(&quot;thc&quot;, 1:4),&quot;age&quot;,&quot;peer&quot;) ## mean vector duncanM &lt;- setNames(c(2.271, 2.560, 2.694, 2.965, 1.847, 2.043, 2.227, 2.510, 1.510, 1.672, 1.828, 1.947, 13.108, 6.193), nm = vn) ## they reported variances (squared SDs) sqSDs &lt;- c(1.004, .922, .832, .846, 1.305, 1.536, 1.645, 1.888, .807, .896, 1.074, 1.125, 2.216, 8.844) ## correlation matrix (lower triangle) tri &lt;- c(1.000, .640, 1.000, .586, .670, 1.000, .454, .566, .621, 1.000, .568, .451, .390, .357, 1.000, .531, .449, .360, .360, .850, 1.000, .494, .449, .380, .425, .783, .815, 1.000, .387, .393, .344, .473, .617, .702, .779, 1.000, .602, .431, .408, .346, .706, .648, .576, .543, 1.000, .522, .460, .394, .403, .644, .689, .687, .535, .759, 1.000, .499, .461, .440, .472, .542, .592, .676, .566, .671, .790, 1.000, .398, .385, .358, .472, .452, .522, .594, .644, .533, .642, .793, 1.000, .497, .370, .429, .326, .418, .409, .337, .205, .456, .413, .345, .248, 1.000, .336, .398, .300, .276, .397, .433, .386, .318, .388, .522, .421, .342, .273, 1) ## scale tri to make covariance matrix duncanCOV &lt;- getCov(x = tri, sds = sqrt(sqSDs), names = vn) ## sample size N &lt;- 321 13.2 Random Intercept Model Lets focus on a single measurealcohol usemeasured on 4 occasions for each of \\(N=321\\) adolescents. A random-intercept model for Subject \\(i\\)s alcohol use in Year \\(t\\) is frequently depicted in MLM as: \\[\\begin{align} y_{i,t} &amp;= \\beta_{i,0} + \\varepsilon_{i,t} \\label{eq:ri} \\\\ &amp;= (\\gamma_{0} + u_i) + \\varepsilon_{i,t} \\label{eq:ri-decomp} \\end{align}\\] indicating that each Subject \\(i\\) has their own intercept, so the random intercept \\(\\beta_{i,0}\\) is itself a variable. As Eq. \\(\\ref{eq:ri-decomp}\\) shows, the average intercept can be depicted as the sum of 2 components: the average intercept (\\(\\gamma_{0}\\)) This is the grand mean because there are no other predictors. individual deviations around \\(\\gamma_{0}\\) (i.e., \\(u_i\\)) This is how much each subjects mean differs from the grand mean. This can also be interpreted as a Level-2 residual The Level-1 residual (\\(\\varepsilon_{i,t}\\)) is how much each subjects time-specific alcohol use differs from their average alcohol use. Both residuals have distributional assumptions, often depicted as normally distributed with some variance: \\[u_i \\sim \\mathcal{N}(0, \\tau_0) \\ \\ \\ , \\ \\ \\ \\varepsilon_{i,t} \\sim \\mathcal{N}(0, \\sigma)\\] This MLM can also be represented as a SEM. In fact, random effects are latent variables (Mehta &amp; Neale, 2005; Skrondal &amp; Rabe-Hesketh, 2004). The SEM representation of Eqs. \\(\\ref{eq:ri}\\)\\(\\ref{eq:ri-decomp}\\) looks superficially different: there are different symbols, and Subject \\(i\\)s \\(t=1, \\dots, T\\) measurements are stored in a single vector \\(\\mathbf{y}_i\\): \\[\\begin{align*} \\mathbf{y}_i &amp;= \\Lambda \\mathbf{\\eta}_i + \\varepsilon_i \\\\ \\mathbf{\\eta}_i &amp;= \\alpha + \\zeta_i \\end{align*}\\] Note that many parameters from the full SEM are missing (e.g., indicator intercepts \\(\\tau\\) and latent regressions \\(\\mathbf{B}\\)) because they are merely 0 in this model. But the nonzero components of this SEM can all be mapped onto the MLM above (Bauer, 2003; Curran, 2003; Singer &amp; Willett, 2003, ch. 8): \\[\\mathbf{\\eta}_i=\\beta_{0,i} \\ \\ , \\ \\ \\alpha=\\gamma_0 \\ \\ , \\ \\ \\zeta_i=u_i \\ \\ , \\ \\ \\varepsilon_i=\\varepsilon_i \\ \\ , \\ \\ \\Lambda=\\mathbf{1}\\] Note that the factor-loading matrix is merely a vector of ones, filling the role of the constant in a regression model. That is, a vector of ones is treated as a predictor, and its slope is the regression models intercept. The role of factor loadings in a LGCM is counter-intuitive because in most SEMs, we think of them as slopes. Instead, the loadings of a growth factor instead map onto observed variables in a multilevel regression, and random coefficients (\\(\\beta_{0,i}\\)) are represented by the latent variables (growth factors: \\(\\mathbf{\\eta}_i\\)). Figure \\(1\\). Random intercept model for alcohol-use data from Duncan and Duncan (1996). The path diagram above illuminates the differences between a growth factor (here, a random intercept) and a common factor. First, factor loadings in a CFA are freely estimated, and they indicate how strongly an indicator is related to the common factor. In a LGCM, factor loadings are fixed to specify how the growth factor is interpreted. A random intercepts loadings are all 1 because every occasions \\(y_i\\) is weighted equally to calculate the Subject \\(i\\)s overall mean. Second, indicator intercepts in a CFA are freely estimated, and are the indicators expected values when the common factor = 0. Because the common-factor mean is typically constrained to zero for identification, indicator intercepts are often simply the indicator means. In a LGCM, indicator intercepts are fixed to zero, but the growth-factor mean is freely estimated. Thus, the only reason that an indicators model-implied mean differs from zero is if the latent intercepts mean differs from zero. This is a primary feature of an LGCM: the mean structure of the indicators is captured entirely by the mean structure of the growth factors. Thus, this random-intercept model reflects the hypothesis that the mean (\\(\\alpha_1\\): here, average alcohol use) is equal across occasions, although the means can still vary across subjects (\\(\\psi_{1,1}\\)). The mean structure of the latent curve model is a special case of the mean structure of a normal factor model, where the mean structure is given by: \\(\\mu=\\tau+\\mathbf{\\Lambda}\\kappa\\), where \\(\\mu\\) (mu) is a column vector of model-implied means of the observed variables, \\(\\tau\\) (tau) is a column vector of indicator intercepts, \\(\\mathbf{\\Lambda}\\) (lambda) is a matrix of factor loadings, and \\(\\kappa\\) (kappa) is a column vector of latent growth-factor means. Because all growth indicators have intercepts fixed to 0, the equation for the mean structure simplifies to \\(\\mu=\\mathbf{\\Lambda}\\kappa\\). And because this chapter includes examples for growth factors being predicted by other variables (a full SEM), we will refer to both means and intercepts of growth factors as \\(\\alpha\\) (the path-model notation). 13.2.1 Specification and Estimation The following lavaan model syntax specifies the model from Figure 1. mod.ri &lt;- &#39; ## loadings for random intercept INTERCEPT =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 ## residual variances alc1 ~~ alc1 alc2 ~~ alc2 alc3 ~~ alc3 alc4 ~~ alc4 ## latent mean and variance INTERCEPT ~ 1 INTERCEPT ~~ INTERCEPT &#39; fit.ri &lt;- lavaan(mod.ri, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N) The lavaan package has a dedicated growth() package, with default lavOptions() that make sense for a LGCM: int.ov.free = FALSE leaves indicator intercepts fixed to zero int.lv.free = TRUE freely estimates latent intercepts/means auto.var = TRUE freely estimates all variances auto.cov.lv.x = TRUE freely estimates covariances among exogenous latent variables So a shorter syntax could simply specify the factor loadings for the growth factor(s): mod.ri &lt;- &#39; INTERCEPT =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 &#39; fit.ri &lt;- growth(mod.ri, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N) However, as LGCMs become more complex (e.g., latent indicators of growth), this set of lavOptions() becomes problematic. Thus, we advise and demonstrate using the basic lavaan() function, either with fully specified model syntax or by explicitly setting lavOptions() that make sense. For example: mod.ri &lt;- &#39; INTERCEPT =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 &#39; fit.ri &lt;- lavaan(mod.ri, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) 13.2.2 Interpret Results The results in the summary() output indicate that the grand mean alcohol use in this sample is 2.638, which is between previous but not current use and &lt; 4 times per month on the 15 scale. parameterEstimates(fit.ri, output = &quot;pretty&quot;) ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## INTERCEPT =~ ## alc1 1.000 1.000 1.000 ## alc2 1.000 1.000 1.000 ## alc3 1.000 1.000 1.000 ## alc4 1.000 1.000 1.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## INTERCEPT 2.638 0.045 59.223 0.000 2.551 2.725 ## .alc1 0.000 0.000 0.000 ## .alc2 0.000 0.000 0.000 ## .alc3 0.000 0.000 0.000 ## .alc4 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .alc1 0.646 0.059 11.026 0.000 0.531 0.761 ## .alc2 0.321 0.035 9.293 0.000 0.253 0.389 ## .alc3 0.257 0.030 8.467 0.000 0.198 0.317 ## .alc4 0.549 0.051 10.729 0.000 0.449 0.650 ## INTERCEPT 0.541 0.051 10.697 0.000 0.442 0.640 The INTERCEPT variance indicates that individual subjects means vary \\(SD = \\sqrt{0.541} = 0.736\\) around the grand mean, frequently referred to as the Level-2 variance in a MLM. Whereas a MLM only estimates a single Level-1 residual variance, the SEM approach is more flexible in allowing Level-1 residuals to be heteroskedastic across time (i.e., the degree to which time-specific observations deviate from subject means). The \\(H_0\\) of homoskedasticity could be tested by comparing this fitted model to one in which the residual variances are constrained to equality. 13.3 Linear Growth Model A linear LGCM has two growth factors: a random intercept (the subjects initial status) a random slope (the subjects rate of change) In order to prevent confusion of the terms intercept and slope with other model parameters, we will refer to the growth factors as initial status and rate of change (Singer &amp; Willett, 2003). Each growth factor has a mean (the average initial status and rate of change across subjects) and a variance (individual differences in initial status and rate of change). Just as in MLM, the random intercept and random slope can covary. Recall that the factor loadings in a LGCM are specified to according to how the growth factors should be interpretedspecifically, the loadings in \\(\\mathbf{\\Lambda}\\) correspond to the matrix of predictors (\\(\\mathbf{X}\\)) in a regression model. Thus, it is more appropriate to think of \\(\\mathbf{\\Lambda}\\) as observed data and of the growth factors as slopes (which vary across subjects). In addition to a column of ones (to estimate the latent intercept), the matrix of factor loadings for linear growth includes a column of values indicating how time should be coded. Figure 2 depicts a common choice for coding time: the number of years (or other units) elapsed since the beginning of the study. \\[\\mathbf{X}_i \\Longleftrightarrow \\mathbf{\\Lambda} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3\\end{bmatrix}\\] The first row of loadings implies that the expected value of the first indicator is only a function of the first growth factors mean, because the second column contains a zero. For each subsequent row, that indicators expected value is a sum of the mean initial-status plus increasing amounts of the mean rate of change. Specifically, the mean rate of change is multiplied by the second columns loading, similar to a regular regression, in which the slope (rate of change: \\(\\beta_1\\)) is multiplied by the value of time. Note that these specified loadings are arbitrary. We could code time using a different occasion as the reference, such as the last occasion: \\[\\mathbf{X}_i \\Longleftrightarrow \\mathbf{\\Lambda} = \\begin{bmatrix} 1 &amp; -3 \\\\ 1 &amp; -2 \\\\ 1 &amp; -1 \\\\ 1 &amp; 0\\end{bmatrix}\\] Using this alternative coding scheme, the mean rate-of-change would remain the same, but the mean of the latent intercept would instead be interpreted as the average status at the end of the study (final status, rather than initial status). However, we will proceed by coding 0 for the first occasion so that the latent intercept represents initial status. Figure \\(2\\). Linear growth model for alcohol-use data from Duncan and Duncan (1996). mod.lin &lt;- &#39; ## loadings Init.Status =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 Rate.Change =~ 1*alc2 + 2*alc3 + 3*alc4 ## factor covariance Init.Status ~~ Rate.Change &#39; fit.lin &lt;- lavaan(mod.lin, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) summary(fit.lin, std = TRUE) ## lavaan 0.6-11 ended normally after 27 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 321 ## ## Model Test User Model: ## ## Test statistic 8.174 ## Degrees of freedom 5 ## P-value (Chi-square) 0.147 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status =~ ## alc1 1.000 0.836 0.820 ## alc2 1.000 0.836 0.889 ## alc3 1.000 0.836 0.931 ## alc4 1.000 0.836 0.895 ## Rate.Change =~ ## alc2 1.000 0.195 0.208 ## alc3 2.000 0.391 0.435 ## alc4 3.000 0.586 0.628 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status ~~ ## Rate.Change -0.080 0.023 -3.504 0.000 -0.489 -0.489 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .alc1 0.000 0.000 0.000 ## .alc2 0.000 0.000 0.000 ## .alc3 0.000 0.000 0.000 ## .alc4 0.000 0.000 0.000 ## Init.Status 2.291 0.054 42.558 0.000 2.739 2.739 ## Rate.Change 0.220 0.018 12.342 0.000 1.128 1.128 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .alc1 0.341 0.051 6.660 0.000 0.341 0.328 ## .alc2 0.306 0.033 9.337 0.000 0.306 0.346 ## .alc3 0.274 0.030 9.224 0.000 0.274 0.339 ## .alc4 0.309 0.046 6.726 0.000 0.309 0.354 ## Init.Status 0.700 0.077 9.064 0.000 1.000 1.000 ## Rate.Change 0.038 0.010 3.665 0.000 1.000 1.000 The initial-status mean indicates average alcohol use began as 2.291, and the rate-of-change mean indicates average alcohol use increased by 0.220 per year. The standardized solution is not particularly meaningful for some LGCM parameters (e.g., loadings, latent means), but it is helpful for interpreting the size of the (significant) covariance between initial status and rate of change. The substantial factor correlation (\\(-0.489\\)) indicates that adolescents who drank less at the beginning of the study tended to increase the drinking at a higher annual rate. 13.3.1 Model Fit and Comparison The model-fit test statistic in the summary() output does not provide any evidence against the \\(H_0\\) that linear change is sufficiently complex to describe the data, \\(\\chi^2(5)=8.174, p=.147\\). Linear change is the simplest functional form of change, and significant latent mean, \\(z=12.342, p&lt;.001\\), indicates that the model cannot be made simpler by omitting linear change. We can verify this by testing whether the random-intercept model statistically differs from the linear growth model in which it is nested. Because the linear-growth loadings are all fixed values, the random-intercept model can be seen as a linear-growth model in which the latent rate of change has a mean and variance of zero (in which case it cannot covary with another factor). lavTestLRT(fit.ri, fit.lin) # or anova(fit.ri, fit.lin) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.lin 5 2963.8 2997.7 8.174 ## fit.ri 8 3128.8 3151.4 179.175 171 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The linear growth model captures the observed data patterns significantly better than the random-intercept model, \\(\\chi^2(3)=171.001, p&lt;.001\\). The standard baseline model for calculating incremental-fit indices (e.g., CFI, TLI) is merely the statistical-independence model: all covariances fixed to zero means and variances are freely estimated But this model is not nested within all competing LGCMs Even the linear-growth model places more constraints on the mean structure than the standard independence model, and it also constrains the model-implied variances. Thus, Widamin and Thompson (2003) recommended a more constrained null/baseline model in which all means (and all variances) of repeatedly measured variables are constrained to equality (representing no change in those values). mod.null &lt;- c(paste(paste0(&quot;alc&quot;, 1:4, collapse = &quot; + &quot;), &quot;~ m*1&quot;), paste0(&quot;alc&quot;, 1:4, &quot; ~~ v*alc&quot;, 1:4)) cat(mod.null, sep = &quot;\\n&quot;) # print model syntax to see what the code above does ## alc1 + alc2 + alc3 + alc4 ~ m*1 ## alc1 ~~ v*alc1 ## alc2 ~~ v*alc2 ## alc3 ~~ v*alc3 ## alc4 ~~ v*alc4 fit.null &lt;- lavaan(mod.null, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N) This model is nested within the LGCMs. semTools::net(fit.null, fit.ri, fit.lin) ## ## If cell [R, C] is TRUE, the model in row R is nested within column C. ## ## If the models also have the same degrees of freedom, they are equivalent. ## ## NA indicates the model in column C did not converge when fit to the ## implied means and covariance matrix from the model in row R. ## ## The hidden diagonal is TRUE because any model is equivalent to itself. ## The upper triangle is hidden because for models with the same degrees ## of freedom, cell [C, R] == cell [R, C]. For all models with different ## degrees of freedom, the upper diagonal is all FALSE because models with ## fewer degrees of freedom (i.e., more parameters) cannot be nested ## within models with more degrees of freedom (i.e., fewer parameters). ## ## fit.lin fit.ri fit.null ## fit.lin (df = 5) ## fit.ri (df = 8) TRUE ## fit.null (df = 12) TRUE TRUE Thus, it can be used to calculate incremental fit indices, which requires using the fitMeasures() function rather than merely summary(..., fit = TRUE). fitMeasures(fit.lin, c(&quot;cfi&quot;,&quot;tli&quot;), baseline.model = fit.null) ## cfi tli ## 0.995 0.988 13.4 Unrestricted Growth Models mod.thc &lt;- &#39; ## loadings Init.Status =~ 1*thc1 + 1*thc2 + 1*thc3 + 1*thc4 Rate.Change =~ 1*thc2 + 2*thc3 + 3*thc4 ## factor covariance Init.Status ~~ Rate.Change &#39; fit.thc &lt;- lavaan(mod.thc, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) summary(fit.thc, std = TRUE) mod.cig &lt;- &#39; ## loadings Init.Status =~ 1*cig1 + 1*cig2 + 1*cig3 + 1*cig4 Rate.Change =~ 1*cig2 + 2*cig3 + 3*cig4 ## factor covariance Init.Status ~~ Rate.Change &#39; fit.cig &lt;- lavaan(mod.cig, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) summary(fit.cig, std = TRUE) Although linear growth appears sufficient to explain these data, this will not always be the case. When a linear-growth model does not fit well, standardized residuals from lavResiduals() can reveal which summary statistics the model fails to reproduce closely. Large residuals for indicator means imply the mean structure is too restrictive, in which case an exploratory growth model can help to reveal how much more complex the latent trajectory is. Note that this would be a switch from confirmatory to exploratory methods, so you would no longer be testing or (dis)confirming a hypothesis; instead, you would be generating a hypothesis or revising your theory. Transparency is vitally important to the scientific process, so clearly communicate in your report when you are conducting confirmatory vs. exploratory research, preferably in separate sections. That is, first test your theory using a priori hypotheses; if your data disconfirm your expectations, then begin looking for evidence in the data of how your theory should be revised. Note also that specific hypotheses of nonlinear growth might be expected based on a study design. For example, trajectories might be expected to differ before and after a particular occasion (a change-point, e.g., an intervention or an important life event). In such cases, a piecewise-growth model can be specified, in which there is are separate latent rate-of-change factors whose loadings are specified to capture changes in means among indicators before and after the change-point. There are a couple of options for exploratory LGCMs, described below. 13.4.1 Latent Basis Curve The most popular is the latent basis curve, which resembles the linear model but only fixes 2 loadings for the rate-of-change factor, freeing the others to capture non-specific change patterns. For example, fixing the first 2 loadings to 0 and 1, respectively, sets the (average) amount of change between the first 2 occasions as a baseline. This is depicted in the path diagram below (left panel). Figure \\(3\\). Latent basis models for alcohol-use data from Duncan and Duncan (1996). The script below specifies the model depicted in the left panel of Figure 3. fit.basis12 &lt;- lavaan(mod.basis12, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) coef(fit.basis12)[paste0(&quot;Rate.Change=~alc&quot;, 3:4)] ## Rate.Change=~alc3 Rate.Change=~alc4 ## 1.502240 2.466024 The two fixed loadings determine the interpretation of the latent mean \\(\\alpha_2\\), which is no longer a constant rate of change but rather the amount of change that occurred between the reference occasions. Each estimated loading tells us how much (average) change occurred at that occasion since the first (or whichever occasions loading is fixed to 0), relative to the amount of change between the first 2 occasions. That is, there is a 1-unit increase in time, represented between the loadings of alc1 and alc2. If change were perfectly linear, we would expect the estimated loadings to continue increasing by 1 unit at a time (assuming the occasions are evenly spread out, as they are in these annual measurements). However, we observe something different: There is only a 0.5-unit increase in the loadings from alc2 to alc3, indicating that only half as much (average) change occurred between those 2 occasions than between the first 2 occasions. There is a (nearly) 1-unit increase in the loadings from alc3 to alc4, indicating that about as much change occurred between those occasions as between the first 2 occasions. This model estimates the average rate of change to be (\\(0.281\\)), the unrestricted change pattern during this 4-year span appears to be an increase of \\(0.281\\) units, then \\(0.1405\\) units, then \\(0.281\\) units again. To test whether this deviation from linearity can be accounted for by mere sampling error, we can conduct a nested model comparison: lavTestLRT(fit.lin, fit.basis12) # or anova(fit.lin, fit.basis12) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.basis12 3 2962.7 3004.2 3.114 ## fit.lin 5 2963.8 2997.7 8.174 5.0601 2 0.07966 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We cannot reject the null hypothesis that linearity is sufficiently complex. Another example of a latent basis curve fixes the first and last loadings to 0 and 1, respectively, with similar implications for interpretation. The script below specifies the model depicted in the right panel of Figure 3. fit.basis14 &lt;- lavaan(mod.basis14, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) coef(fit.basis14)[paste0(&quot;Rate.Change=~alc&quot;, 2:3)] ## Rate.Change=~alc2 Rate.Change=~alc3 ## 0.4055110 0.6091749 Here, the baseline amount of change is set by comparing the first and last occasions. Again, if change were perfectly linear, we would expect the estimated loadings to increase by a constant amount, but we see a similar pattern as the first basis model above. There is a 0.4-unit increase in the loadings from alc1 to alc2 The increase in loadings from alc2 to alc3 is about half that (0.2 units) Then the loadings increase from alc3 to alc4 by about as much as the increase between the first 2 occasions This basis model is statistically equivalent to the first basis model, so they yield identical model fit with proportionally equivalent estimated loadings. Likewise, the estimated latent mean is proportionally higher because it reflects the average amount of change between Years 1 and 4. A latent basis model is flexible in terms of the factor loadings, which are somewhat easy to interpret. However, there is a hidden proportionality assumption represented by estimating only a single latent variance for rate of change (\\(\\psi_{2,2}\\); Wu &amp; Lang, 2016). Although some software (e.g., Mplus and OpenMx) can be tricked into relaxing this assumption (McNeish, 2020), an alternative model (described next) can be even more useful for exploratory purposes. 13.4.2 Saturated Growth Model A saturated model fits perfectly by definition, although the perfect fit is arbitrary because all information from summary statistics was used to estimate parameters to reproduce those summary statistics (i.e., \\(df=0\\)). However, because we are in hypothesis-generating exploratory mode, testing a hypothesis of fit is not necessarily a priority. In this section, we show how to fit a saturated LGCM by using polynomial contrast codes to specify the loadings (Voelkle &amp; McKnight, 2012). In the GLM or MLM, polynomial contrast codes can be used to represent a predictor that is an ordinal grouping variable, which is often a continuous variable that is merely measured in discrete units. In our example, time is measured only in whole years, discounting differences in days or months. Note: In fact, a limitation of wide-format approaches for longitudinal data (LGCM and repeated-measures ANOVA) is that time must be treated discretely. In long-format approaches (e.g., MLM), predictors like time can be represented continuously, capturing more detail about individual differences in measurement schedules, thus providing more information about true developmental trajectories. A polynomial is the power to which a variable is raised. For example, a linear effect is merely the first power (\\(x^1=x\\)), but a quadratic effect is the second power (\\(x^2\\)) and a cubic effect is the third power (\\(x^3\\)), etc. Any number raised to the zero power is 1 (\\(x^0=1\\)), which is the constant used to estimate the intercept. Thus, even linear models are a polynomial model, but only up to the first power. When we measure a variable on \\(t=1, \\dots, T\\) occasions, we can estimate up to the \\(T-1\\) power, for the same reason that differences between \\(g=1, \\dots, G\\) groups can be represented by \\(G-1\\) dummy codes. Our example has \\(T=4\\) occasions, so we can estimate up to power 3; another way to think of it is that we do estimate \\(T=4\\) polynomial effects, but we begin with 0 (the intercept) rather than 1 (the linear slope). Note that a saturated LGCM could simply use the same loadings for the latent intercept and linear slope, then a quadratic (and cubic) factor can have loadings that are the squared (and cubed) values of the linear-slopes loadings; however, we can avoid possible (multi)collinearity of growth factors by specifying orthogonal contrast codes instead (Voelkle &amp; McKnight, 2012). Polynomial contrast codes can be obtained in R using the contr.poly() function, whose first argument is the number of occasions (\\(T=4\\)): contr.poly(4) # .L = linear, .Q = quadratic, .C = cubic ## .L .Q .C ## [1,] -0.6708204 0.5 -0.2236068 ## [2,] -0.2236068 -0.5 0.6708204 ## [3,] 0.2236068 -0.5 -0.6708204 ## [4,] 0.6708204 0.5 0.2236068 If we have unequally spaced measurements (e.g., measured at ages 7, 9, 12, and 13), we can indicate this using the scores= argument: contr.poly(4, scores = c(7, 9, 12, 13)). Rather than fixing loadings of the latent intercept to 1, we can specify a centercept (Wainer, 2000) as \\(\\frac{1}{\\sqrt{T}}\\) (\\(\\frac{1}{2}\\)). The advantage of specifying the latent level (rather than initial status) in this way is that its variance (\\(\\psi_{1,1}\\)) captures individual differences in grand means, not merely individual differences at a particular occasion or in the middle of all occasions (Voelkle &amp; McKnight, 2012, p. 26). Finally, because there is a growth factor for each occasion, the entire covariance structure can be reproduced in the latent space (factor (co)variances). Thus, we must fix indicators residual variances to zero, as depicted in Figure 4. Figure \\(4\\). Saturated growth model for alcohol-use data from Duncan and Duncan (1996). The script below specifies the model in Figure 4, fits the model, and prints the estimated latent means for interpretation. mod.sat &lt;- &#39; ## loadings Level =~ .5*alc1 + .5*alc2 + .5*alc3 + .5*alc4 Linear =~ -.671*alc1 + -.224*alc2 + .224*alc3 + .671*alc4 Quad =~ .5*alc1 + -.5*alc2 + -.5*alc3 + .5*alc4 Cubic =~ -.224*alc1 + .671*alc2 + -.671*alc3 + .224*alc4 ## residual variances alc1 ~~ 0*alc1 alc2 ~~ 0*alc2 alc3 ~~ 0*alc3 alc4 ~~ 0*alc4 &#39; fit.sat &lt;- lavaan(mod.sat, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE, auto.cov.lv.x = TRUE) PE.sat &lt;- parameterEstimates(fit.sat, output = &quot;pretty&quot;) PE.sat[PE.sat$op == &quot;~1&quot;, ] ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## alc1 0.000 0.000 0.000 ## alc2 0.000 0.000 0.000 ## alc3 0.000 0.000 0.000 ## alc4 0.000 0.000 0.000 ## Level 5.245 0.088 59.649 0.000 5.073 5.417 ## Linear 0.495 0.040 12.386 0.000 0.417 0.574 ## Quad -0.009 0.031 -0.287 0.774 -0.071 0.053 ## Cubic 0.065 0.030 2.216 0.027 0.008 0.123 The latent quadratic trend is not significant, but the cubic one is. A quadratic curve (i.e., a parabola) can only change directions once (i.e., from increasing to decreasing, or from decreasing to increasing). A cubic curve can change directions twice (and a 4th-order curve can change direction 3 times, etc.). This result is consistent with what we observed with the latent basis model, when the increase was steeper, then less steep, then steeper again. Unfortunately, we have no way to test whether this developmental trajectory truly follows a cubic (3rd-order) trend using only \\(3+1=4\\) occasionsrecall that \\(df=0\\). If we were to generate a hypothesis about nonlinear change, it would need to be confirmed with new data gather from a design with more measurement occasions. But in this case, also recall that we could not reject the \\(H_0\\) of linear change because the linear model does not fit significantly worse than a saturated model (such as the one above). fitMeasures(fit.lin, c(&quot;chisq&quot;,&quot;df&quot;,&quot;pvalue&quot;), output = &quot;pretty&quot;) ## ## Model Test User Model: ## ## Test statistic 8.174 ## Degrees of freedom 5 ## P-value 0.147 Fixing both the quadratic and cubic factor means to zero would not lead to rejecting the \\(H_0\\) (\\(p=.08\\)), so this is a lesson in guarding against using multiple tests to conduct exploratory analyses. If you employ a saturated or latent-basis model, bear in mind that they are exploratory, and it is safer to interpret the informative parameters descriptively in order to generate (not test) new hypotheses derived from the data. 13.4.2.1 Alternative Contrast Codes When estimating growth factors from intervention studies, linear growth might not be reasonable because we hypothesize means will change immediately after the intervention but not after. For example, in a pretest, posttest, follow-up design, our hypotheses might be: \\(H_1\\): Parenting intervention will reduce parentchild conflicts \\(H_2\\): The effect will be stable after intervention The piecewise growth model we mentioned at the beginning of Section 4 would be appropriate in principle, but it would not be practical because there are only 3 occasions with a changepoint between the first 2 occasions (pretest and posttest). Instead, we can simply apply a different set of contrast codes than the polynomials discussed above. Helmert contrast codes would be appropriate to test whether pretest scores differ from both posttest and follow-up scores (\\(H_1\\)) and whether follow-up scores differ from initial posttest scores (\\(H_2\\)). The factor loadings below would allow these hypotheses to be tested: Intercept Pre.v.After Post.v.Follow Pretest \\(\\frac{1}{\\sqrt{3}}\\) -2 0 Posttest \\(\\frac{1}{\\sqrt{3}}\\) 1 -1 Follow-up \\(\\frac{1}{\\sqrt{3}}\\) 1 1 This growth trajectory would be saturated, but 3 occasions already does not allow for sufficient df to test strong hypotheses about fit (e.g., a quadratic factor would already make it saturated). But relevant and sensible hypotheses that follow from such a 3-occasion intervention study can be answered using these contrast codes. 13.5 Explaining Growth Factors Growth-factor variances represent individual differences in developmental trajectories. The literature often distinguishes two related concepts: Individual differences occur between subjects Individual change occurs within a subject Thus, growth-curve models (in MLM or SEM) simultaneously model inter-individual differences in intra-individual change. It is possible to estimate latent trajectories while statistically controlling for Level-1 covariates that only vary within subjects. These are often called time-varying predictors. It is also possible to explain inter-individual differences in latent trajectories, using Level-2 predictors that only vary between subjects. These are often called time-invariate predictors. Either type of predictor introduces moderation of (or by) time, which is not as familiar as moderation represented by interactions in regression models. Here, too, a comparison with MLM will facilitate interpretation of moderated effects in LGCMs using SEM. The fundamental principle to bear in mind is the basic meaning of moderation: the relationship between two variables is a function of a third variable. To this end, we briefly review statistical interaction in a regression model, wherein moderation is represented by a product between predictors. \\[\\begin{equation} \\begin{split} \\label{eq:int} Y_i &amp;= \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\varepsilon_i \\\\ &amp;= (\\beta_0 + \\beta_1 X_{i1}) + (\\beta_2 + \\beta_3 X_{i1}) \\times X_{i2} + \\varepsilon_i \\\\ &amp;= \\vartheta_0 + \\vartheta_1 X_{i2} + \\varepsilon_i \\end{split} \\tag{13.1} \\end{equation}\\] By grouping the terms in the second row of Eq. @ref{eq:int}, we can reinterpret it as a simple regression of \\(Y\\) on \\(X_2\\) (the third row of Eq. @ref{eq:int}). But the intercept and slope are themselves variables because they depend on (are functions of) \\(X_1\\). Simple intercepts: \\(\\vartheta_0 = \\beta_0 + \\beta_1 X_{i1}\\) Simple slopes: \\(\\vartheta_1 = \\beta_2 + \\beta_3 X_{i1}\\) Recall the moderation is symmetric, so the terms in Eq. @ref{eq:int} could also be grouped such that the effect of \\(X_1\\) depends on \\(X_2\\). Moderation can also be interpreted in both directions in a LGCM, but it is not as obvious how because time is not an explicit variable in the model, as other predictors are. Before proceeding, consider that growth factors are variables in a SEM. They can therefore be predictors as well as outcomes, even in the same model. Growth factors have already been treated as predictors of occasion-specific indicators in the sections above, but they could also predict subject-level outcome variables (e.g., to investigate whether the rate of increased alcohol consumption predictors a students academic performance). The following sections only elaborate on predictors of growth factors and correlations among latent trajectories, but bear in mind that more complex models are possible. 13.5.1 Time-Invariant (Level-2) Predictors Level-2 predictors vary between subjects, so they can explain between-subject differences in trajectories, which are represented by growth factors. Although Duncan and Duncan (1996) had several such covariates, our example data include the following two subject-level predictors: age at the beginning of the study and peer encouragement to consume illicit substances. Consider the MLM for growth, which extends Eq. \\(\\ref{eq:ri}\\) by adding time as a predictor. Below, we intersperse some equivalent SEM symbols as reminders of how the MLM and SEM frameworks map onto each other (Bauer, 2003; Curran, 2003; Singer &amp; Willett, 2003, ch. 8). \\[\\begin{equation} \\text{Level 1: } y_{i,t} &amp;= \\beta_{i,0} + \\beta_{i,1} \\text{Time}_{i,t} + \\varepsilon_{i,t} = \\textcolor{blue}{\\Lambda \\mathbf{\\eta}_i + \\varepsilon_i} \\tag{13.2} \\end{equation}\\] \\[\\begin{equation} \\text{Level 2: } \\beta_{i,0} &amp;= \\textcolor{blue}{(\\eta_0=)} \\gamma_{0,0} + \\gamma_{0,1} \\text{Age}_i + \\gamma_{0,2} \\text{Peer-Influence}_i + u_{0,i} \\label{eq:L2int+pred} \\\\ \\beta_{i,1} &amp;= \\textcolor{blue}{(\\eta_1=)} \\gamma_{1,0} + \\gamma_{1,1} \\text{Age}_i + \\gamma_{1,2} \\text{Peer-Influence}_i + u_{1,i} \\tag{13.3} \\end{equation}\\] \\[\\begin{equation} \\text{Combined: } y_{i,t} &amp;= \\gamma_{0,0} + \\gamma_{0,1} \\text{Age}_i + \\gamma_{0,2} \\text{Peer-Influence}_i + \\gamma_{1,0} \\text{Time}_{i,t} \\label{eq:L2main} \\\\ &amp;+ \\gamma_{1,1} (\\text{Age}_i \\times \\text{Time}_{i,t}) + \\gamma_{1,2} (\\text{Peer-Influence}_i \\times \\text{Time}_{i,t}) \\label{eq:L2ints} \\\\ &amp;+ u_{0,i} + u_{1,i} \\text{Time}_{i,t} + \\varepsilon_{i,t} \\ \\ \\textcolor{gray}{\\text{(joint residual)}} \\tag{13.4} \\end{equation}\\] The path diagram below depicts age and peer influence as observed variables that predict growth factors, conforming to how ?lavaan::model.syntax works. In the background, lavaan internally represents observed predictors as single-indicator factors. Thus, age and peer influence are assigned \\(\\eta_3\\) and \\(\\eta_4\\), respectively, in the path diagram below. Figure \\(5\\). Linear growth model for alcohol-use data from Duncan and Duncan (1996) with subject-level predictors of growth factors. The LGCM script below specifies a measurement model for the growth factors (Eq. \\(\\ref{eq:L1}\\)) and a structural model that includes both predictors of initial status (Eq. \\(\\ref{eq:L2int+pred}\\)) and rate of change (Eq. \\(\\ref{eq:L2slope+pred}\\)). We will consider the combined model in Eqs. \\(\\ref{eq:L2main}\\)\\(\\ref{eq:L2res}\\) further when we interpret the structural model. mod.pred2 &lt;- &#39; ##### MEASUREMENT MODEL Init.Status =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 Rate.Change =~ 1*alc2 + 2*alc3 + 3*alc4 ##### STRUCTURAL MODEL ## Level-2 predictors of growth trajectories Init.Status + Rate.Change ~ age + peer ## factor (residual) covariance Init.Status ~~ Rate.Change &#39; fit.pred2 &lt;- lavaan(mod.pred2, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) summary(fit.pred2, std = TRUE, rsq = TRUE) ## lavaan 0.6-11 ended normally after 43 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 321 ## ## Model Test User Model: ## ## Test statistic 23.607 ## Degrees of freedom 9 ## P-value (Chi-square) 0.005 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status =~ ## alc1 1.000 0.841 0.832 ## alc2 1.000 0.841 0.887 ## alc3 1.000 0.841 0.936 ## alc4 1.000 0.841 0.902 ## Rate.Change =~ ## alc2 1.000 0.203 0.214 ## alc3 2.000 0.406 0.452 ## alc4 3.000 0.609 0.653 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status ~ ## age 0.269 0.031 8.579 0.000 0.320 0.476 ## peer 0.086 0.016 5.455 0.000 0.102 0.303 ## Rate.Change ~ ## age -0.033 0.012 -2.700 0.007 -0.163 -0.243 ## peer -0.008 0.006 -1.227 0.220 -0.037 -0.110 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Init.Status ~~ ## .Rate.Change -0.054 0.020 -2.741 0.006 -0.424 -0.424 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .alc1 0.000 0.000 0.000 ## .alc2 0.000 0.000 0.000 ## .alc3 0.000 0.000 0.000 ## .alc4 0.000 0.000 0.000 ## .Init.Status -1.774 0.399 -4.447 0.000 -2.108 -2.108 ## .Rate.Change 0.703 0.156 4.505 0.000 3.462 3.462 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .alc1 0.315 0.047 6.645 0.000 0.315 0.308 ## .alc2 0.320 0.033 9.834 0.000 0.320 0.356 ## .alc3 0.275 0.029 9.373 0.000 0.275 0.341 ## .alc4 0.301 0.045 6.634 0.000 0.301 0.346 ## .Init.Status 0.427 0.056 7.587 0.000 0.603 0.603 ## .Rate.Change 0.038 0.010 3.740 0.000 0.914 0.914 ## ## R-Square: ## Estimate ## alc1 0.692 ## alc2 0.644 ## alc3 0.659 ## alc4 0.654 ## Init.Status 0.397 ## Rate.Change 0.086 Although we can reject the omnibus \\(H_0\\) of perfect datamodel correspondence, \\(\\chi^2(9)=23.607, p=.005\\), inspection of standardized residuals (lavResiduals(fit.pred2) output not shown) suggests the misfit is merely the accumulation of several minor discrepancies between observed and expected summary statistics. So we will proceed to interpret the model parameters. Note: If one were interested in interpreting an incremental fit index (e.g., CFI) for this model, the custom null model in Section 3.1 must be appended by including the Level-2 predictors, whose means and variances are also estimated. But because they are exogenous covariates, the null model should also estimate the age~~peer covariance. The estimated intercept for the initial-status factor is \\(\\alpha_0=-1.774\\), which is the expected alcohol use when all predictors (age, peer, and Time) are 0. In this case, it is an extrapolation beyond the data because no subjects began the study at birth, and peer-influence was measured on a 15 scale. 13.5.1.1 Simple Effects There are 3 predictors with simple effects in Eq. \\(\\ref{eq:L2main}\\): age, peer, and Time. But their effects are not represented in the same way in a LGCM. First, we focus on the effects of time-invariant predictors. Both effects of Level-2 predictors on initial status were significant (\\(p&lt;.001, R^2=39.7\\%\\)), indicating that when the study began, adolescents who were older and whose best friends more strongly encouraged illicit-substance use exhibited more frequent alcohol consumption. Standardized slopes indicator both effects were medium, but unstandardized slopes are interpretable in the given units. Specifically, each year older increases their expected initial alcohol consumption by \\(\\gamma_{0,1}=0.269\\) units, for a given level of peer influence. And at a given age, one (Likert-scale) unit higher peer influence increases their expected initial alcohol consumption by \\(\\gamma_{0,2}=0.086\\) units. The simple effects of age and peer are conditional on Time = 0 (the beginning of the study), but we might be interested in probing their effects at other occasions. To obtain simple effects of time-invariant predictors conditional on other occasions, simply change the reference occasion in \\(\\mathbf{\\Lambda}\\) and fit the (statistically equivalent) model. For example, we could set the rate-of-change factor loadings to \\(\\lambda_{2,.}=-1, 0, 1, 2\\), which would make the latent intercept interpreted as the level at the second occasion. Thus, the simple effects of age and peer would be conditional on Time = 1 year after the study began. The simple effect of Time, on the other hand, is not an explicit regression slope in a LGCM. Instead, it is a latent variable: the rate-of-change factor. The simple effect of Time, therefore, is the estimated intercept (\\(\\alpha_1=0.703\\)) of the rate-of-change factor (i.e., the average rate of change, conditional on age = 0 and peer = 0). As with the estimated initial-status intercept, this is an extrapolation beyond the data. If one were interested in interpreting simple slopes of Time for particular (observed) values of age and peer, one could center age and peer at those values and fit the statistically equivalent model. 13.5.1.2 Moderation Effects The interactions in Eq. \\(\\ref{eq:L2ints}\\) are represented by the predictors effects on the rate-of-change factor, which explained \\(R^2=8.6\\%\\) of between-subjects variance in growth trajectories. That is, the effect of Time depends on both age and peer. The form of the model facilitates describing the nature of this interaction when Time is the focal predictor. At a given level of peer influence, one year older decreases the effect of Time on alcohol consumption by \\(\\gamma_{1,1}=-0.033\\) units/year. The effect of peer influence (given age) was not significant, implying no evidence moderation of Times effect by peer influence. We did not estimate an age \\(\\times\\) peer interaction, so the LGCM also did not estimate a 3-way interaction. This is not possible in our example because doing so would require raw data to calculate the product term and include it as an additional predictor of growth factors. The 3-way interaction would be represented by the age \\(\\times\\) peer product terms effect on the rate-of-change factor, while the products (simple) 2-way interaction would be represented by its effect on the initial-status factor. 13.5.2 Time-Varying (Level-1) Predictors Level-1 predictors vary within subjects, as does Time. Thus, we can estimate the effect of Time while statistically controlling for time-varying predictors. Our example data includes annual cigarette use, which would be included in the Level-1 component of a MLM for growth. \\[\\begin{equation} \\label{eq:L1pred} \\text{AlcUse}_{i,t} = \\beta_{i,0} + \\beta_{i,1} \\text{Time}_{i,t} + \\beta_2 \\text{CigUse}_{i,t} + \\beta_3 (\\text{Time}_{i,t} \\times \\text{CigUse}_{i,t}) + \\varepsilon_{i,t} \\end{equation}\\] Notice that Eq. \\(\\ref{eq:L1pred}\\) only includes a fixed (not random) effect of cigarette use. The possibility of including a random effect for both Time and cigarette use is one of the ways in which MLM is more flexible than SEM. Notice also that Eq. \\(\\ref{eq:L1pred}\\) includes a Time \\(\\times\\) cigarette-use interaction effect (\\(\\beta_3\\)). This is how moderation is represented in a regression model, but moderation of Time by a Level-1 variable in LGCM is quite different, involving more than one parameter. The path diagram below depicts both alcohol and cigarette use as observed variables, with the latter predicting the former. However, lavaans internal LISREL representation does not have a parameter matrix for regressions among observed variables (Mplus does, and the RAM representation used by the OpenMx package only has a single matrix with all slopes among observed and latent variables). So internally, lavaan promotes alcohol and cigarette use variables to latent variables corresponding to their single indicators. However, to keep the path diagram readable, this is overlooked and the slopes are labeled simply as \\(\\beta\\)s. Figure \\(6\\). Linear growth model for alcohol-use data from Duncan and Duncan (1996), controlling for cigarette-use data. The LGCM script below specifies a measurement model for the growth factors, but also regresses each indicator on the level-1 covariate at the same occasion. The latter parameters are labeled so that we can apply equality constraints= in the lavaan() call, to exclude moderation between Time and cigarette use. mod.pred1 &lt;- &#39; ## loadings Init.Status =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 Rate.Change =~ 1*alc2 + 2*alc3 + 3*alc4 ## factor covariance Init.Status ~~ Rate.Change ## Level-1 covariates alc1 ~ b1*cig1 alc2 ~ b2*cig2 alc3 ~ b3*cig3 alc4 ~ b4*cig4 &#39; fit.pred1 &lt;- lavaan(mod.pred1, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE, ## constrain cigarette-use&#39;s effect to equality over time constraints = c(&quot;b1 == b2&quot;, &quot;b1 == b3&quot;, &quot;b1 == b4&quot;)) summary(fit.pred1, std = TRUE) ## lavaan 0.6-11 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## Number of equality constraints 3 ## ## Number of observations 321 ## ## Model Test User Model: ## ## Test statistic 86.677 ## Degrees of freedom 20 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status =~ ## alc1 1.000 0.679 0.725 ## alc2 1.000 0.679 0.753 ## alc3 1.000 0.679 0.764 ## alc4 1.000 0.679 0.752 ## Rate.Change =~ ## alc2 1.000 0.185 0.205 ## alc3 2.000 0.370 0.416 ## alc4 3.000 0.555 0.615 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## alc1 ~ ## cig1 (b1) 0.260 0.023 11.490 0.000 0.260 0.316 ## alc2 ~ ## cig2 (b2) 0.260 0.023 11.490 0.000 0.260 0.356 ## alc3 ~ ## cig3 (b3) 0.260 0.023 11.490 0.000 0.260 0.374 ## alc4 ~ ## cig4 (b4) 0.260 0.023 11.490 0.000 0.260 0.394 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status ~~ ## Rate.Change -0.058 0.020 -2.927 0.003 -0.460 -0.460 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .alc1 0.000 0.000 0.000 ## .alc2 0.000 0.000 0.000 ## .alc3 0.000 0.000 0.000 ## .alc4 0.000 0.000 0.000 ## Init.Status 1.814 0.062 29.228 0.000 2.671 2.671 ## Rate.Change 0.165 0.018 9.286 0.000 0.893 0.893 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .alc1 0.328 0.049 6.687 0.000 0.328 0.374 ## .alc2 0.330 0.034 9.789 0.000 0.330 0.406 ## .alc3 0.312 0.032 9.800 0.000 0.312 0.396 ## .alc4 0.266 0.044 6.107 0.000 0.266 0.326 ## Init.Status 0.462 0.059 7.769 0.000 1.000 1.000 ## Rate.Change 0.034 0.010 3.462 0.001 1.000 1.000 ## ## Constraints: ## |Slack| ## b1 - (b2) 0.000 ## b1 - (b3) 0.000 ## b1 - (b4) 0.000 We reject the \\(H_0\\) of perfect model fit, \\(\\chi^2(20)=86.677, p&lt;.001\\), and there are many large standardized residuals (lavResiduals(fit.pred1) output not shown). This is not surprising given how constrained this model is. Cigarette-use is not only constrained to have a constant effect across time, but the effects only occur within an occasion. There are many theoretically motivated reasons to improve the modelfor example, one might expect that smoking leads to later alcohol consumption. We could add lagged effects (e.g., alc2 ~ cig1) to improve model fit, which combines features of cross-lagged panel models and LGCMs. In fact, there have also been attempts to merge autoregressive and latent-growth models (e.g., the autoregressive latent-trajectory (ALT) model; Bollen &amp; Curran, 2004). We will not explore these complexities here, but we will briefly discuss the importance of the equality-constrained slopes on the interpretation of Times average effect (i.e., the rate-of-change mean). The script below releases the constraints, which significantly improves model fit. fit.tc &lt;- lavaan(mod.pred1, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE) lavTestLRT(fit.pred1, fit.tc) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.tc 17 2844.7 2893.7 67.036 ## fit.pred1 20 2858.3 2896.1 86.677 19.64 3 0.0002015 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The effect of cigarette use now decreases as time goes on, and the decrease is not linear as it is in Eq. \\(\\ref{eq:L1pred}\\) because the LGCM can add 3 parameters to account for moderation more flexibly than only \\(\\beta_3\\). Because the effect of cigarette use now depends on Time, the estimated rate-of-change mean is only a simple slope for Time conditional on CigUse = 0 (again, an extrapolation beyond the bounds of this 15 scale). Probing the effect of time at different levels of cigarette use is simple enough: simply center the moderator (all 4 cig variables) at the same theoretically interesting value (e.g., 1, 3, and 5), which changes the interpretation of each AlcUse intercept to the expected value at that level of CigUse. ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## alc1 ~ ## cig1 (b1) 0.364 0.035 10.281 0.000 0.364 0.434 ## alc2 ~ ## cig2 (b2) 0.308 0.027 11.415 0.000 0.308 0.415 ## alc3 ~ ## cig3 (b3) 0.221 0.025 8.738 0.000 0.221 0.326 ## alc4 ~ ## cig4 (b4) 0.213 0.028 7.687 0.000 0.213 0.327 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status 1.626 0.078 20.852 0.000 2.500 2.500 ## Rate.Change 0.276 0.034 8.171 0.000 1.573 1.573 How does one recenter variables without raw data? Well, the first step of calculating a (co)variance is to subtract the mean, so the covariance matrix does not change. And we know exactly how recentering the variables would affect the mean vector: simply subtract the same value from each mean that would have been subtracted from the variable itself. For example, lets probe the simple effect of Time when CigUse is in the middle of the scale (3). duncM3 &lt;- duncanM # copy, then center cig1-cig4 duncM3[paste0(&quot;cig&quot;, 1:4)] &lt;- duncM3[paste0(&quot;cig&quot;, 1:4)] - 3 fit.tc3 &lt;- update(fit.tc, sample.mean = duncM3) # refit model with centered means ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## alc1 ~ ## cig1 (b1) 0.382 0.035 10.911 0.000 0.382 0.453 ## alc2 ~ ## cig2 (b2) 0.266 0.030 8.766 0.000 0.266 0.367 ## alc3 ~ ## cig3 (b3) 0.220 0.029 7.660 0.000 0.220 0.324 ## alc4 ~ ## cig4 (b4) 0.242 0.030 8.133 0.000 0.242 0.368 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Status 2.693 0.058 46.438 0.000 4.096 4.096 ## Rate.Change 0.118 0.021 5.551 0.000 0.650 0.650 Note that this model is not statistically equivalent to the model fitted to uncentered data. The rate-of-change factor loadings constrain the trajectory to be linear, and fit.tc imposes that linearity constraint on different intercepts than fit.tc3 does. The average rate of change when CigUse = 3 is \\(\\alpha_1=0.118\\). 13.5.3 Parallel Growth Curves In LGCM, time-varying covariates must be measured in parallel with the outcome itself (i.e., at all the same occasions). An alternative to controlling for them is to simultaneously model their own developmental trajectory. In such models, we can explore how the development of different variables relate to each other. The script below specifies linear growth factors for both alcohol and cigarette use. mod.par &lt;- &#39; ## loadings for alcohol use Init.Alc =~ 1*alc1 + 1*alc2 + 1*alc3 + 1*alc4 Lin.Alc =~ 1*alc2 + 2*alc3 + 3*alc4 ## loadings for cigarette use Init.Cig =~ 1*cig1 + 1*cig2 + 1*cig3 + 1*cig4 Lin.Cig =~ 1*cig2 + 2*cig3 + 3*cig4 &#39; fit.par &lt;- lavaan(mod.par, sample.cov = duncanCOV, sample.mean = duncanM, sample.nobs = N, int.lv.free = TRUE, auto.var = TRUE, auto.cov.lv.x = TRUE) ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Alc ~~ ## Lin.Alc -0.095 0.023 -4.191 0.000 -0.511 -0.511 ## Init.Cig 0.627 0.071 8.850 0.000 0.662 0.662 ## Lin.Cig -0.026 0.019 -1.375 0.169 -0.106 -0.106 ## Lin.Alc ~~ ## Init.Cig -0.096 0.021 -4.508 0.000 -0.394 -0.394 ## Lin.Cig 0.029 0.006 4.477 0.000 0.463 0.463 ## Init.Cig ~~ ## Lin.Cig -0.068 0.026 -2.584 0.010 -0.213 -0.213 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Init.Alc 2.288 0.054 42.560 0.000 2.690 2.690 ## Lin.Alc 0.223 0.018 12.419 0.000 1.021 1.021 ## Init.Cig 1.842 0.064 28.773 0.000 1.651 1.651 ## Lin.Cig 0.210 0.019 10.793 0.000 0.733 0.733 Results indicate increases in average use of both substances during adolescence. Both intercepts are negatively correlated with both slopes, indicating that those who used either illicit substance at the beginning of the study tended to increase the use of either substance at a higher rate (although initial cigarette use was not significantly correlated with increasing alcohol consumption). Latent intercepts were positively correlated, indicating that adolescents who used one substance more frequently at the beginning of the study tended to use the other more frequently as well. Latent slopes were also positively correlated, indicating that adolescents who increasingly used one substance tended to increasingly use the other. 13.6 Latent Indicators of Growth Factors All the examples so far have used observed indicators of growth factors. When we want to estimate an LGCM for a latent variable, additional identification constraints are required, depending on the type of latent variable. Recall that latent variables have no intrinsic location (mean) or scale (variance) because they have not been directly measured. We give them arbitrary locations/scales simply to identify the model. Growth-factor means are identified because we fix all the indicator intercepts to zero; thus, indicator means are reproduced as functions of growth-factor means. Growth-factor variances are identified because we fix all the factor loadings; thus, (co)variances among indicators are reproduced as functions of growth-factor (co)variances and loadings. We will discuss 2 commonly used types of latent growth indicator: a common factor (itself measured with multiple indicators) and a latent response variable (LRV) underlying a discrete (binary, ordinal) observation. 13.6.1 Common Factors Recall that a common factor can be identified by making it z scoresthat is, we fix its \\(\\alpha_i=0\\) and \\(\\psi_{i,i}=1\\). Other identification methods place constraints on measurement parameters (loadings and intercepts), but we will ignore those methods because they hide the identification issues that we discuss below. After discussing LGCM with common-factor indicators (where LGCM is the structural model imposed on a CFA), we also briefly discuss another type of model in which the nested components are switched (a CFA to capture common varience in several parallel growth models). 13.6.1.1 Curve of Factors At first glance, common factors may seem like poor indicators of growth because all the means are fixed to zero; thus, the identification method makes it appear that no change can occur. However, recall that when a common factor is measured repeatedly with the same indicators, we can use a longitudinal CFA to evaluate equivalence/invariance of the measurement parameters across time. If scalar invariance constraints (equal loadings and intercepts) can be validly applied to at least a subset of repeatedly measured indicators, then the identification constraints (\\(\\alpha_i=0\\), \\(\\psi_{i,i}=1\\)) would only be required on the first occasion. On subsequent occasions, latent means (and variances) would be estimable. Thus, growth factors could be specified to reproduce the mean (and covariance) structure among the common factors. However, because the common-factor mean remains fixed to zero at the first occasion, it follows that the Initial-Status growth factor must have a mean of zero. In a LGCM, all growth-factor indicators have their intercepts fixed to zero, but with common-factor indicators, the first indicators latent intercept is already fixed to zero for identification, so there is no exchange of df to estimate the latent mean of Initial Status. Therefore, in a LGCM with common-factor indicators, the mean of Initial Status must be fixed to zero for identification. The path diagram in Figure 7 illustrates a curve of factors model, in which growth factors explain growth among repeated measures of a common factor substance use. This common factor represents adolescents propensity for engaging in risk substance-use behaviors, indicated by their use of alcohol, cigarettes, and cannabis (thc). The left panel of Figure 7 illustrates only one occasions measurement model, with longitudinal scalar-invariance constraints in place (i.e., the same \\(\\lambda\\)s and \\(\\nu\\)s are constant across occasions). The left panel also shows that each common-factor intercept is fixed to \\(\\alpha=0\\) because those means are structured by the (higher-order) LGCM. The gray arrow directed from the left to right panels indicates that this measurement model applies to each latent indicator of growth. The right panel depicts the (higher-order) LGCM itself. Because only one measurement model for substance use is depicted, Figure 7 is missing the residual covariances that should be estimated for each substance-use indicator across time (i.e., alc1s residuals are correlated with alc2s, alc3s, and alc4s residuals; same for cigarettes and cannabis). In practice, measurement invariance should be tested first, as shown in Chapter 25. Although some authors have advocated testing longitudinal invariance with the LGCM in place (), we recommend using a CFA to test invariance because it places no restrictions on the latent mean or covariance structures. Any misfit due to invalid structural constraints (e.g., linear growth when true growth is nonlinear) would inflate Type I error rates for tests of invariance. Note also that if one identifies common factors by fixing a reference (or average) indicators loading to 1 and intercept to 0, common-factor means and variances would be freely estimated even without invariance constraints in place. However, the LGCM would not truly model change in a common factor, but rather change in the reference indicator. Measurement invariance is a prerequisite to drawing valid inferences about structural differences/changes in a construct. Figure \\(7\\). Curve-of-factors model for substance-use data from Duncan and Duncan (1996). The script below specifies the curve-of-factors model depicted in Figure 7. mod.cof &lt;- &#39; ##### MEASUREMENT MODEL (3 indicators) ## equal loadings across 4 occasions SubUse1 =~ L1*alc1 + L2*cig1 + L3*thc1 SubUse2 =~ L1*alc2 + L2*cig2 + L3*thc2 SubUse3 =~ L1*alc3 + L2*cig3 + L3*thc3 SubUse4 =~ L1*alc4 + L2*cig4 + L3*thc4 ## equal intercepts across 4 occasions alc1 + alc2 + alc3 + alc4 ~ nu1*1 cig1 + cig2 + cig3 + cig4 ~ nu2*1 thc1 + thc2 + thc3 + thc4 ~ nu3*1 ## residual (co)variances across 4 occasions alc1 ~~ alc1 + alc2 + alc3 + alc4 alc2 ~~ alc2 + alc3 + alc4 alc3 ~~ alc3 + alc4 alc4 ~~ alc4 cig1 ~~ cig1 + cig2 + cig3 + cig4 cig2 ~~ cig2 + cig3 + cig4 cig3 ~~ cig3 + cig4 cig4 ~~ cig4 thc1 ~~ thc1 + thc2 + thc3 + thc4 thc2 ~~ thc2 + thc3 + thc4 thc3 ~~ thc3 + thc4 thc4 ~~ thc4 ## STRUCTURAL MODEL (higher-order LGCM) ## loadings for growth factors Init.Status =~ 1*SubUse1 + 1*SubUse2 + 1*SubUse3 + 1*SubUse4 Rate.Change =~ 1*SubUse2 + 2*SubUse3 + 3*SubUse4 ## growth-factor means and (co)variances Init.Status ~ 0*1 # fixed to 0 for identification Rate.Change ~ 1 ## growth-factor means and (co)variances Init.Status ~~ Init.Status + Rate.Change Rate.Change ~~ Rate.Change ## residual variances of growth indicators SubUse1 ~~ psi1*SubUse1 SubUse2 ~~ NA*SubUse2 SubUse3 ~~ NA*SubUse3 SubUse4 ~~ NA*SubUse4 &#39; fit.cof &lt;- lavaan(mod.cof, sample.cov = duncanCOV, sample.mean = duncanM, ## identify the model by constraining the first loading sample.nobs = N, constraints = &quot;L1 == 1&quot;) ## Alternatively, use effects-coding: constraints = &quot;L1 == 3 - L2 - L3&quot; ## or fix first factor&#39;s variance using: std.lv = TRUE or constraints = &quot;psi1 == 1&quot; We do not present output because the conclusions are quite similar to the linear model for alcohol consumption in Section 3. The added benefit of this model is that we can see cigarette and cannabis use are stronger indicators (std. factor loadings approximately 0.8) of propensity for substance use than alcohol consumption (std. loadings 0.60.7). The benefit of modeling change in an error-free construct is only as valid as the CFA itself. The disadvantage of latent indicators is that each method of identification in the script above provides a different (arbitrary) scale on which to interpret the average rate of change. The standardized solution provides a scale-free estimate, but the units of \\(SD\\) with which to interpret it are between-subject differences in rates of change, which is not necessarily a meaningful quantity to judge the size of the average rate of change. 13.6.1.2 Factor of Curves Although (individual differences in) growth trajectories might differ across substances, an individuals trajectories might be correlated (e.g., an adolescent whose alcohol-consumption trajectory is higher than average might also increase their cigarette and cannabis use more than average). One might posit a common factor that explains why each substances initial statuses are correlated, and another common factor explaining when each substances rates of change are correlated. Duncan and Duncan (1996, pp. 335338) discussing fitting a factor-of-curves model to their substance-use data, which involves several parameter constraints that are less than intuitive and open to criticism. The factor-of-curves model is rarely used in practice, and its plausibility is often tenuous, with limited benefits over interpreting the higher-order growth factors in a curve-of-factors model. Thus, we do not discuss or demonstrate the factor-of-curves model here. 13.6.2 Discrete Indicators Latent growth can be modeled for LRVs underlying discrete data, which means that the interpretations of growth factors will apply to the LRV locations and scales. As with common factors, the locations and scales are arbitrary, so regarding interpretation of the size of rate-of-change (for example), LGCMs for discrete data share the same drawbacks as curve-of-factors models. However, hypotheses about the functional form of growth (if any) can still be tested using latent indicators. 13.6.2.1 Binary Indicators As discussed in Chapter 12, binary outcomes have a single threshold (\\(\\tau\\)) to discretize a continuous/normal LRV (\\(y^*\\)) into 2 categories: \\[y = I(y^* &gt; \\tau),\\] where the indicator function \\(I()\\) assigns 1 when its argument is TRUE and 0 when it is FALSE. The default behavior in lavaan is to identify the model by fixing each LRVs intercept and variance to 0 and 1, respectively, enabling all thresholds to be estimated. However, it is statistically equivalent to fix a binary items threshold to zero and estimate its intercept. In fact, they will have the same absolute value but opposite sign. Because indicator means are fixed to zero anyway (so they can be reproduced by growth factors via factor loadings), one could hypothetically fix thresholds for binary indicators to zero in lavaan syntax in order to fit a a LGCM to it (Newsom, 2015, p. 185). For example, if we had raw example data whose substance-use indicators were binary, we could update any of the scripts above by adding something like this to the model syntax: &#39; alc1 + alc2 + alc3 + alc4 | 0*t1&#39; However, there is no way to link the LRV scales across time because there is only one threshold, whose df is traded for the intercept (all of which are in turn traded for growth-factor means). Recall also from Chapter 12 that there are 2 common methods of identifying the LRV scale: parameterization = \"delta\" (default) fixes the marginal/total variance of an LRV to 1 parameterization = \"theta\" fixes an LRVs residual variance to 1 Either parameterization implies equality of variance across occasions, neither of which is likely to hold in practice. So even if the mean LRV remains the same over time, a change in variance will make it appear as though the mean does change because its distance from zero in units of \\(SD\\) changes. Thus, any true change in the mean is confounded with changes in variance, making it highly problematic to fit a LGCM to binary growth indicators. 13.6.2.2 Ordinal Indicators With more than 2 categories, we can trade 2 df (e.g., by fixing the first threshold to 0 and second threshold to 1) to estimate both the LRV intercept and variance. Recall from Chapter 12 that any ordinal variable with categories \\(c=0, \\ldots, C\\) can be interpreted as a crude discretization of an underlying LRV, whose \\(C\\) thresholds divide the latent distribution into \\(C+1\\) categories. \\[y=c \\ \\ \\ \\text{ if } \\ \\ \\ \\tau_c &lt; y^* \\le \\tau_{c+1}\\] Because the normal distribution is unbounded, the first category starts at \\(\\tau_0 = -\\infty\\), and the last category ends at \\(\\tau_{C+1} = +\\infty\\). With only 3 categories, there is only enough information to trade the \\(C=2\\) thresholds for the LRV intercept and variance, which is sufficient to link the response scales across occasions. So we can still only assume (not test) the assumption that thresholds do not change over time, but threshold invariance can be tested when items have \\(\\ge3\\) thresholds (Mehta et al., 2004). In our example data, substance use was measured with 5 categories, so each indicator would have \\(C=4\\) thresholds, enabling us to test the assumption of threshold invariance (if we had access to the raw data). Chapter 27 will discuss threshold invariance in greater detail in the context of CFA with categorical indicators, but here it is sufficient to say that we can constrain thresholds across occasions in LGCM model syntax by using the same labels (below, a, b, c, and d) for equality-constrained parameters. &#39; alc1 | a*t1 + b*t2 + c*t3 + d*t4 alc2 | a*t1 + b*t2 + c*t3 + d*t4 alc3 | a*t1 + b*t2 + c*t3 + d*t4 alc4 | a*t1 + b*t2 + c*t3 + d*t4 &#39; Intercepts remain constrained to zero (the default setting) so that the LRV means can be modeled by the growth-factor means and loadings. However, we must free the variances in the model syntax as well. Similar to invariance with common factors, the first occasions variance remains fixed to 1. Using the default parameterization = \"delta\", we must free the marginal variances on subsequent occasions, which is controlled by the latent scale parameters, whose operator resembles how we specify a (co)variance that includes an asterisk to indicate it refers to the LRV \\(y^*\\). &#39; alc2 ~*~ NA*alc2 alc3 ~*~ NA*alc3 alc4 ~*~ NA*alc4 &#39; Using parameterization = \"theta\", we must instead free the residual variances on subsequent occasions. &#39; alc2 ~~ NA*alc2 alc3 ~~ NA*alc3 alc4 ~~ NA*alc4 &#39; The estimated growth trajectories will still differ across parameterizations because the latent scales will differ. However, the latent scales are now linked across occasions, so changes in the LRV intercepts are no longer confounded with changes in the LRV scale. So the estimated growth trajectories will be proportionally equivalent across parameterizations (i.e., the standardized solutions will be equivalent). 13.7 Multigroup Growth Models Any of the models already discussed can be fitted simultaneously to multiple groups. Differences in parameters across groups represent moderation by the grouping variable. For example, if the latent mean of the Initial-Status factor differs across groups, that is the simple effect of Group at Time 0. Other simple effects of Time can be probed by choosing a different reference occasion, just as discussed with explicit predictors of growth factors in Section 5.1. Likewise, if the latent mean of the Rate-of-Change factor differs across groups, that is the moderation of Times effect by Group. And if there is a predictor of growth factors, then the same principles from Section 5.1 can be extended to the grouping variable. Extending the example from Section 5.1, if the effect of Age on Initial Status (i.e., the simple effect of Age) differs across groups, that is an Age \\(\\times\\) Group interaction. And if the effect of Age on Rate of Change (i.e., the Age \\(\\times\\) Time interaction) differs across groups, that is an Age \\(\\times\\) Time \\(\\times\\) Group interaction. When the growth-factor indicators are latent variables (see Section 6), comparisons across groups are only valid in the presence of invariance constraints. For common factors, scalar invariance must therefore be tested across both groups and occasions. For discrete indicators, thresholds must be invariant across both groups and occasions. For both types of latent indicator, identification constraints for the latent scale only need to be set on one GroupTime combination (typically the first group on the first occasion), while latent scales in all other groups/occasions can be freely estimated. 13.8 References Bauer, D. J. (2003). Estimating multilevel linear models as structural equation models. Journal of Educational and Behavioral Statistics, 28(2), 135167. https://doi.org/10.3102/10769986028002135 Bollen, K. A., &amp; Curran, P. J. (2004). Autoregressive latent trajectory (ALT) models: A synthesis of two traditions. Sociological Methods &amp; Research, 32(3), 336383. https://doi.org/10.1177/0049124103260222 Curran, P. J. (2003). Have multilevel models been structural equation models all along? Multivariate Behavioral Research, 38(4), 529569. https://doi.org/10.1207/s15327906mbr3804_5 Duncan, S. C., &amp; Duncan, T. E. (1996). A multivariate latent growth curve analysis of adolescent substance use. Structural Equation Modeling, 3(4), 323347. https://doi.org/10.1080/10705519609540050 McNeish, D. (2020). Relaxing the proportionality assumption in latent basis models for nonlinear growth. (Structural Equation Modeling, 27)(5), 817824. https://doi.org/10.1080/10705511.2019.1696201 Mehta, P. D., &amp; Neale, M. C. (2005). People are variables too: Multilevel structural equations modeling. Psychological Methods, 10(3), 259284. https://doi.org/10.1037/1082-989X.10.3.259 Mehta, P. D., Neale, M. C., &amp; Flay, B. R. (2004). Squeezing interval change from ordinal panel data: Latent growth curves with ordinal outcomes. Psychological Methods, 9(3), 301333. https://doi.org/10.1037/1082-989X.9.3.301 Newsom, J. T. (2015). Longitudinal structural equation modeling: A comprehensive introduction. Routledge. Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780195152968.001.0001 Skrondal, A., &amp; Rabe-Hesketh, S. (2004). Generalized latent variable modeling: Multilevel, longitudinal, and structural equation models. Chapman and Hall/CRC. https://doi.org/10.1201/9780203489437 Voelkle, M. C., &amp; McKnight, P. E. (2012). One size fits all? A Monte-Carlo simulation on the relationship between repeated measures (M)ANOVA and latent curve modeling. Methodology, 8(1), 2338. https://doi.org/10.1027/1614-2241/a000044 Wainer, H. (2000). The centercept: An estimable and meaningful regression parameter. Psychological Science, 11(5), 434436. https://doi.org/10.1111/1467-9280.00284 Widaman, K. F., &amp; Thompson, J. S. (2003). On specifying the null model for incremental fit indices in structural equation modeling. Psychological Methods, 8(1), 1637. https://doi.org/10.1037/1082-989X.8.1.16 Wu, W., &amp; Lang, K. M. (2016). Proportionality assumption in latent basis curve models: A cautionary note. Structural Equation Modeling, 23(1), 140154. https://doi.org/10.1080/10705511.2014.938578 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
