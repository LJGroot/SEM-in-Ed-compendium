# Using `lavaan` to Run Regression and ANOVA as a SEM {#ch2}

This chapter prepares the reader to learn about structural equation modeling (SEM) by reviewing the fundamentals of ordinary least-squares (OLS) regression in the general(ized) linear modeling (GLM) framework.  Some key take-home messages will  reinforce what was discussed about *covariance structure analysis* in the previous chapter:

- Analyzing different data formats (raw vs. summary data) yields equivalent results (under conditions of normality and complete data).
    - **Recall** from the previous chapter that maximum-likelihood estimation (MLE) in SEM chooses parameters that minimize discrepancies between observed and expected *summary statistics* rather than casewise observations.
- Grouping variables can be represented as dummy, effects, or contrast codes, but there are some advantages of stratifying results by group.   This will be demonstrated using multigroup SEM and comparing results to the single-group approach.  
- Point estimates from SEM will match GLM because OLS estimates are ML estimates when the distributional assumptions are met.
    - independent, identically distributed (normal with homogeneous variance)
- However, the *SE*s are typically smaller under ML with SEM than under OLS with GLM.  
- Furthermore, different (but analogous) test statistics will be compared between GLM (*t* and *F* statistics) and SEM (*z* and $\chi^2$ statistics).
- Model comparisons require fitting nested models to the same data, which in SEM means fitting them to the same summary statistics.

<!-- TODO: 
Frame the need to talk about MG-SEM because it allows for thinking more about CSA v. reproducing raw data 
-->


## Prepare Data and Workspace

### Import Example Data

We will use data from a tutorial published on the [Social Change Lab's web site](http://www.socialchangelab.net/uploads/9/8/7/7/98771854/moderated_mediation_example_write-up.pdf).

```{r import raw data, message=FALSE}
dat <- foreign::read.spss("demoData/MODMED.sav", to.data.frame = TRUE)[c(2, 3, 5, 7)]
## print first and last 3 rows
dat[c(1:3, 98:100), ]
```

Recall from [Chapter 1](#ch1) that SEM can equivalently be fitted to summary statistics rather than raw data.  This chapter will demonstrate this both approaches.
```{r import summary data ch02}
## Save summary statistics
M <- colMeans(dat)
S <- cov(dat)
N <- nrow(dat)
```

Also recall that it is possible to fit a "multigroup SEM" to obtain parameter estimates separately per group.  This chapter will also demonstrate how to fit a multigroup SEM to raw data as well as to summary statistics.  For the latter, we require group-specific summary statistics, which [Chapter 1](#ch1) discussed how to obtain. Here, we use slightly more efficient syntax to obtain lists of group-specific summary statistics, although the more sophisticated `sapply()` function makes the syntax less immediately intuitive to read.

```{r import summary data}
## Create factor from effects-coded conditions
dat$mood.f <- factor(dat$MOOD, levels = c(-1, 1),
                     labels = c("neutral","positive"))
## Save lists of group-specific summary statistics
CC <- c("ATT","NFC","POS")   # when group = "mood.f"
gM <- sapply(c("neutral","positive"), simplify = FALSE, 
             FUN = function(g) colMeans(dat[dat$mood.f == g, CC]) )
gS <- sapply(c("neutral","positive"), simplify = FALSE,
             FUN = function(g) cov(     dat[dat$mood.f == g, CC]) )
gN <- table(dat$mood.f)
```


### `lavaan` Syntax

Notice from the section-header that `lavaan` should always be lower-case. It is a portmanteau of \textbf{la}tent \textbf{va}riable \textbf{an}alysis, similar to \textbf{AN}alysis \textbf{O}f \textbf{VA}riance (ANOVA).

Load `lavaan` into your workspace.
```{r load lavaan-02, message=FALSE}
library(lavaan)
```

The standard syntax for regression in R is a `formula` object.  To regress `ATT`itude on `MOOD` condition, `N`eed `F`or `C`ognition (`NFC`), and `POS`itive thoughts:

```{r formula, eval=FALSE}
ATT ~ MOOD + NFC + POS
```

SEM is a multivariate modeling framework, so a `formula` object does not allow for sufficient complexity.

- There can be many outcome variables, each with its own formula (i.e., different predictors)
    - Multivariate GLMs (e.g., MANOVA) require the same predictors for all outcomes
    - e.g., `cbind(ATT, POS) ~ MOOD + NFC`
- Outcomes can even predict other outcomes, which GLM does not allow
    - Another name for an SEM is a *simultaneous equations model*.

Later chapters will introduce additional complexity that `formula` objects do not allow for:

- Residual variances can be specified explicitly using a "double tilde" operator  (`~~`), which allows specifying equality constraints on variance estimates.
- There can be unobserved (latent) variables, which require a special operator (`=~`) to define in `lavaan` syntax.
- Details about these and other operators can be found in the **Details** section of the `?model.syntax` help page, and the website also has a [syntax tutorial](https://lavaan.ugent.be/tutorial/index.html) for various short topics.  Details about syntax features we will use in this chapter are summarized in the table below.


| **Operator** | **Purpose** | **Example** |
|:--|:------------|:--------------|
| `~`  | Estimate regression slope(s) | `'outcome ~ predictor'` |
| `~1` | Estimate intercept(s) | `'outcome ~ 1'` |
| `value*`  | Fix (number) or free (missing value) parameters | `'posttest ~ 1*pretest + NA*x1'` |
| `string*`  | Label parameters | `'y ~ myLabel*x'` |
| `+`  | Additional parameters or operators | `'y ~ NA*x1 + beta2*x2'` |
| `c()`  | Operators for multiple groups | `'y ~ c(NA, 1)*x1 + c(beta1, beta2)*x2'` |
| `:=`  | Define a function of parameters | `'slopeRatio := beta1 / beta2'` |


`lavaan` requires collecting all of these model parameters in a `character` vector, even when there is only one outcome.

```{r character, eval=FALSE}
' ATT ~ MOOD + NFC + POS '
```


Note how the syntax above looks exactly like the `formula` we would pass to `lm()`, except that it is within quotation marks.  `lavaan`'s model syntax is flexible enough that we can equivalently specify each parameter on a different line, which can be useful in larger models or to make `#comments` in syntax.

```{r character2, eval=FALSE}
' ATT ~ 1     # intercept
  ATT ~ MOOD  # slopes
  ATT ~ NFC
  ATT ~ POS '
```













#### OLS Regression

Using the familiar `lm()` function to obtain OLS estimates of a regression model:

```{r OLS}
ols <- lm(ATT ~ MOOD + NFC + POS, data = dat)
summary(ols)
```


#### MLE in `lavaan` with Raw or Summary Data

When data are complete, maximum likelihood estimation (MLE) provides equivalent results using either raw or summary data.  There are multiple model-fitting functions in `lavaan`:

- `lavaan()` is the main "engine", but expects that models are fully specified in complete detail
- `sem()` is a "wrapper" that calls `lavaan()` with some sensible defaults that apply to most SEMs (e.g., automatically estimating residual variances, automatically estimating covariances among predictors)
- `cfa()` is meant for fitting confirmatory factor analysis (CFA) models, discussed in a later chapter.  `cfa()` and `sem()` actually behave identically (i.e., they call `lavaan()` with the same defaults)
- `growth()` is meant for very simple latent growth curve models, also discussed in a later [chapter](#ch26)

For now, we will use the `sem()` function similar to the way we use the `lm()` function, including how we obtain results from `summary()`.

```{r SEM}
## Raw Data
mle <- sem('ATT ~ MOOD + NFC + POS', data = dat,
           meanstructure = TRUE) # explicitly request intercept
## Summary Data
mle <- sem('ATT ~ MOOD + NFC + POS', sample.nobs = N,
           sample.cov = S, sample.mean = M)
```

Now inspect the output.  Identify each parameter estimate, and compare it to the corresponding OLS estimate in the `lm()` results.
```{r SEM summary}
summary(mle, header = FALSE, nd = 4) # rsquare = TRUE also available
sigma(ols)^2 # compare to residual variance from OLS
```


#### Compare/Contrast Estimates

`lavaan`'s point estimates match OLS, except residual variance ($\sigma^2_\varepsilon$)

- OLS estimates $\sigma^2_\varepsilon$ by dividing the residual $SS$ by the residual $df$
- SEM estimators choose estimates that reproduce $\bar{\mathrm{y}}$ and $S$ 

If we calculate variance of the residuals ($\mathrm{y}_i - \hat{\mathrm{y}}$) using the "population formula" (dividing by $N$), we obtain `lavaan`'s result

```{r compare res var}
sigma(ols)^2 # residual variance from OLS
sum(resid(ols)^2) / N # manually calculated using POPULATION formula
coef(mle)[["ATT~~ATT"]] # matches lavaan
```

- *Note*: The double-tilde (`~~`) operator signifies a two-headed arrow, in contrast to the one-headed arrow (`~`) of a directed effect


#### Compare/Contrast $SE$s

The estimated $SE$s are smaller from `lavaan` because SEM is an asymptotic method

- estimator assumes $N$ is sufficiently large that $S \approx \Sigma$
- Wald $z$ statistics replace OLS's $t$ statistics
- Model comparison results in a $\chi^2$ rather than $F$ statistic

The asymptotic assumption yields more power, but inflated Type I error rates when sample size is low (relative to number of estimated parameters).

Note that `lavaan` simply calculates `sample.cov=` and `sample.mean=` from `data=`, but they can be passed directly.

Also note that raw `data=` are required to 

- analyze incomplete data (using full information MLE rather than listwise deletion),
- obtain robust statistics (e.g., to account for nonnormality of continuous outcomes), or 
- model discrete (binary or ordinal) outcomes (which require a *threshold model*, explained later in a [chapter about categorical data](#ch12))

## Caveats with SEM

### Model Comparison with OLS

To test the $H_0$ that the experimental manipulation (`MOOD`) explains $R^2=0$\% of variance in `POS`itive thoughts, compare models that represent the $H_0$ and $H_A$

```{r OLS compare}
ols0 <-  lm(POS ~ 1, data = dat)
ols1 <-  lm(POS ~ MOOD, data = dat)
anova(ols0, ols1)
```

This result matches the $F$ test at the bottom of `summary(ols1)`


### Model Comparison with `lavaan`

Omitting variables from an SEM means they are removed from $\bar{\mathrm{y}}$ and $S$, returning a warning 

```{r syntax compare, eval=FALSE}
mle0 <- sem('POS ~ 1', data = dat)
mle1 <- sem('POS ~ 1 + MOOD', data = dat)
anova(mle0, mle1)
```

```{r printWarn, echo=FALSE}
mle1 <- sem('POS ~ 1 + MOOD', data = dat)
cat('... models are based on a different set of observed variables')
```

Instead, **keep predictor(s) in the model**, but fix the slope(s) to zero

```{r SEM compare}
mle0 <- sem('POS ~ 1 + 0*MOOD', data = dat)
anova(mle0, mle1)
```


### Moderation Caveats

When evaluating moderation using a product term, that product term is a new variable in $\bar{\mathrm{y}}$ and $S$

- e.g., we want to test `MOOD`'s effect, controlling for `NFC`
- before comparing adjusted group means, we should evaluate the homogeneity-of-slopes assumption

As in `formula` objects, `lavaan` syntax recognizes the colon (`:`) operator

- but the asterisk (`*`) is reserved for assigning labels or values to parameters

```{r modform, eval=FALSE}
POS ~ 1 + MOOD + NFC + MOOD:NFC
```

`MOOD:NFC` is now an additional variable in $\bar{\mathrm{y}}$ and $S$, so comparison to any model without that interaction term requires including the product term but fixing its effect to zero


#### Estimate and Test an Interaction with OLS

```{r OLS int}
ols.hom <- lm(POS ~ MOOD + NFC, data = dat)
ols.het <- lm(POS ~ MOOD + NFC + MOOD:NFC, data = dat)
anova(ols.hom, ols.het)
```

Using ML estimation, model comparison would analogously yield an analysis of *deviance*

- a.k.a. likelihood ratio test (LRT)


#### Estimate and Test an Interaction with `lavaan`

```{r SEM int}
mle.hom <- sem('POS ~ 1 + MOOD + NFC + 0*MOOD:NFC', data = dat)
mle.het <- sem('POS ~ 1 + MOOD + NFC +   MOOD:NFC', data = dat)
anova(mle.hom, mle.het)
```

We can reject $H_0$ of homogeneous `NFC` slopes across `MOOD` groups


#### Compare Adjusted Means

Suppose we had failed to reject the $H_0$ of homogeneity of slopes, and we wanted a single comparison of (adjusted) group means, controlling for NFC. 
`MOOD` is effect coded, so its slope (which is identical using OLS regression or SEM) is interpreted as

- the difference between a group's mean and the grand mean
- half the group mean difference

The $t$ test (OLS) or Wald $z$ test (SEM) is sufficient to test the $H_0: \hat{\mathrm{y}}_\text{Treatment}|x = \hat{\mathrm{y}}_\text{Control}|x$

```{r print M_adj}
summary(mle.hom)
```

But the `MOOD` slope could also be fixed to zero (or dropped) to obtain a LRT from SEM (or $F$ test from ANOVA).


### Summary of Caveats

SEMs are only comparable when they are fitted to the same data

- same observations (rows) **and** same variables (columns)
- Instead of removing predictors, fix slopes to zero

Interaction terms are additional variables, added to $\bar{\mathrm{y}}$ and $S$

- only compare models with same variables
- products of variables can be saved in the `data.frame` or specified using the colon (`:`) operator (e.g., `x1:x2`), **not the asterisk** (`*`)
- *standardized solution is incorrect* because it treats product terms as separate variables (e.g., if they were independently transformed to $z$ scores rather than *products* of $z$ scores)



## Multigroup SEM

### Group-Specific Intercepts in OLS

Recall that omitting the intercepts allows each group to have a dummy code in the model

```{r slope w/o int-b}
slope0 <- lm(POS ~ -1 + mood.f + NFC, data = dat)
summary(slope0)
```

This still assumes homoskedasticity of residuals, and comparing intercepts (or adjusted means) requires the [delta method](https://en.wikipedia.org/wiki/Delta_method), which is a way to estimate the *SE* of a function of parameters (e.g., difference between 2 slopes) from the *SE*s of the original parameter estimates.

```{r deltaMethod}
car::deltaMethod(slope0, "b2 - b1", rhs = 0,
                 parameterNames = paste0("b", 1:3))
```


### Group-Specific Intercepts in SEM?

`lavaan` automatically uses the delta method when 

- parameters are labeled in the model syntax (before `*` operator)
- model syntax includes user-defined parameters (`:=` operator)

However, SEM cannot fit group-specific intercepts because they are linearly dependent (recall (multi)collinearity among predictors from a previously read regression text). That is, the positive-mood dummy code is simply 1 minus the neutral-mood dummy code, so they are perfectly (negatively) correlated.  Therefore, $S$ would not be [positive definite](https://en.wikipedia.org/wiki/Definite_matrix), which is a term that indicates some redundancy among variables.  This means that fitting a single-group SEM with group-specific intercepts is not possible:

```{r NPD, eval = FALSE}
dat$treat   <- dat$mood.f == "positive"
dat$control <- 1 - dat$treat # linear dependency

## label slopes "b1" for neutral group, "b2" for positive group
mod1 <- ' POS ~ b1*mood.fneutral + b2*mood.fpositive + NFC
  ## user-defined parameter
    adj_mean_diff := b2 - b1
'
fit1 <- sem(mod1, data = dat) # lavaan ERROR: 
## sample covariance matrix is not positive-definite
```


### Multiple Groups in SEM

Rather than including `MOOD` as variable(s) in the model, we can fit the remainder of the model (`POS ~ 1 + NFC`) separately in each group

- each group gets unique parameter estimates
- constraints across groups can be added
    - homoskedasticity of residuals
    - homogeneous slopes

`MOOD`'s effect is how group-specific intercepts differ

- each group gets unique parameter labels, in a vector
    - below: `POS ~ c(b1, b2)*1`
- use the same label for homogeneous slopes
    - below: `POS ~ c(b3, b3)*NFC`

```{r MG-SEM}
mod2 <- ' POS ~ c(b1, b2)*1 + c(b3, b3)*NFC
  ## user-defined parameter
    adj_M_diff := b2 - b1
'
fit2 <- sem(mod2, data = dat, group = "mood.f")
summary(fit2)
## compare to OLS, which assumes homoskedasticity
summary(slope0)
## delta method to test adj_M_diff
car::deltaMethod(slope0, "b2 - b1", rhs = 0,
                 parameterNames = paste0("b", 1:3))
```

Note that the point **and** *SE* estimates from this delta-method result (using OLS) can differ from the corresponding results using SEM.  Because SEM does not assume residual homoskedasticity, it may be more robust, but there is a trade-off because OLS is not biased by small samples the way that SEM is.



#### Summary Statistics for Multigroup SEM

`lavaan` calculates summary statistics from the raw `data=` separately in each group, but lists can be passed directly

```{r MG-SEM summary stats}
gN # vector of sample sizes
gM # mean vectors
gS # list of covariance matrices
fit2 <- sem(mod2, sample.nobs = gN, # "group=" is implied by lists
            sample.mean = gM, sample.cov = gS)
```


#### Advantages of Multigroup SEM

- Less restrictive assumptions
    - parameters differ by default
- More interpretable parameters
    - each group has their own intercept and slopes
- Intuitive to specify $H_0$ as user-defined parameter
    - e.g., the difference between the intercepts in the treatment group (`b2`) and control group (`b1`) is zero
- Intuitive to represent assumptions about equality constraints by using the same labels across groups
    - e.g., the same `NFC` effect (`b3`)
    - testable by comparing models with(out) constraints 
- Can easily test $H_0$ about parameters other than means
    - Are variances or correlations equal across groups?

#### Disadvantages:

- asymptotic assumption (large $N$) applies to each group
- single-group regression better in small samples




