# Regression as Mean- and Covariance-Structure Analysis

This chapter prepares the reader to learn about structural equation modeling (SEM) by reviewing the fundamentals of ordinary least-squares (OLS) regression. Different types of data are discussed, and some review of matrix algebra will be incorporated into the regression review.

## Installing `lavaan`

The SEM software that we will use is an open-source R package called [\textcolor{blue}{lavaan}](https://lavaan.ugent.be/) [\textcolor{blue}{(Rosseel, 2012)}](http://dx.doi.org/10.18637/jss.v048.i02).  To install the `lavaan` package you can type the following command in the R console:

```{r install, eval=FALSE}
install.packages("lavaan", dependencies = TRUE)
```

Depending on your [\textcolor{blue}{R}](https://www.r-project.org/)([\textcolor{blue}{Studio}](https://www.rstudio.com/)) settings, this might open a window where you have to select a [\textcolor{blue}{CRAN}](https://cran.r-project.org/) mirror (select "[your country], [closest city]") prior to installing the package, including all additional packages that it depends on.  You only need to install the package once (on a specific computer), but you must also "load" the library to access its functionality (similar to first installing an app, then opening the app to use it):

```{r load, message=FALSE}
library(lavaan)
```

Every time you start R you need to use this command to activate the lavaan package into the current R workspace. Therefore, it is advisable to start every script with this command.

Sometimes, it will be necessary to install the *development version* of `lavaan` before the next version is made available on CRAN.  For example, if the developer has fixed a bug or introduced a new feature, the development version can be installed using this syntax:
```{r install dev, eval=FALSE}
install.packages("lavaan", repos = "https://www.da.ugent.be", type = "source")
```



## Types of Data Used in SEM

SEM can be seen as a generalization of the general(ized) linear model (GLM) for multivariate data. But rather than merely allowing multiple predictors and multiple outcomes, any variable can operate **both** as a predictor and an outcome in the same model.  This allows us to model complex relationships among variables, including chains of causation (i.e., mediation). 

Estimation of a GLM involves minimizing discrepancies (differences) between observed ($y$) and expected ($\hat{y}$) *casewise* values (i.e., an individual subject's score on an outcome variable $y$).  In contrast, estimation of a SEM involves minimizing discrepancies between observed and expected *summary statistics* (i.e., the modeled variables' means and covariance matrix).  This is why another (older) name for an SEM is *covariance structure analysis* (when only trying to explain why variables are related to each other), or mean and covariance structure (MACS) analysis when means are also modeled.

Summary statistics can be calculated from observed raw data, so raw data can be provided directly to SEM software for analysis.  But if data are complete and come from a multivariate normal distribution, summary statistics can also be passed directly to the SEM software, which has advantages.  Unlike raw data, summary statistics do not compromise the privacy of the research participants, so summary statistics can be reported in published empirical research. That allows readers (and students like you!) to have access to the same summary statistics to replicate the results or to fit a different model (e.g., that represents a competing theory). Thus, this chapter will show how to import both raw and summary data, as well as how to calculate summary statistics from raw data so that they can be reported along with results.


### Raw Data

Typically, data are already stored in an external file with a predefined format, such as SPSS (\*.sav) or Excel (\*.xlsx), and there are some pre-specified functions in R that can read in data from such type of formats, although they require the installation of specific packages.  As an alternative, you can store the data in a plain text (with .txt or .csv extensions), where each line represents the observed responses of one individual. Spaces, commas, semicolons or tabs can be used to separate the individual responses. Some programs (like SPSS) can also export data to these types of file formats. 

```{r eval=FALSE}
read.table("data.txt", header = TRUE)
read.spss("data.sav", to.data.frame = TRUE)	# requires the package foreign
read.xls("data.xls")	# requires the package gdata
as.data.frame(read_excel("data.xlsx"))	# requires the package readxl
```


This chapter uses data from a tutorial published on the [\textcolor{blue}{Social Change Lab's web site}](http://www.socialchangelab.net/uploads/9/8/7/7/98771854/moderated_mediation_example_write-up.pdf).

```{r import data-01, message=FALSE}
dat <- foreign::read.spss("MODMED.sav", to.data.frame = TRUE)[c(2, 3, 5, 7)]
head(dat)
```

`MOOD` indicates experimental conditions

- $+1$ = positive mood induction
- $-1$ = neutral mood (control condition)

`NFC` = need for cognition, `POS` = positive thoughts, and `ATT` = attitude


### Summary Statistics from Raw Data

To report summary statistics of your modeled variables, they can be calculated using the `colMeans()` function for means, the `cov()` function for a covariance matrix, and the `nrow()` function for the sample size

- *Note*: Counting the number of rows **assumes you have complete data**

```{r summaries}
(M <- colMeans(dat)) # mean vector
(S <- cov(dat)) # covariance matrix
(N <- nrow(dat)) # complete-data sample size
```

Covariances are explained in more detail in a later section, but it is sufficient here to understand that the covariance between variables $x$ and $y$ (i.e., $\sigma_{xy}$) is their unstandardized correlation coefficient ($r_{xy}$).  That is, a correlation is a covariance between $z$ scores.  Because correlations are easier to interpret than covariances, it is common to report the means, *SD*s, and correlations.  Using the *SD*s, readers can rescale correlations to covariances in the orignal units of measurement (i.e., $\sigma_{xy} = \sigma_x \times \sigma_y \times r_{xy}$).  The `cov2cor()` function can standardize a covariance matrix, or you can use the `cor()` function directly.
```{r SD+cor}
(SD <- sapply(dat, sd))
sqrt(diag(S)) # SDs == square-roots of variances 
cov2cor(S) # standardize the covariance matrix, or use cor()
(R <- cor(dat))
```

Because covariance and correlation matrices are symmetric (and diagonal elements of a correlation matrix are 1 by definition), both could be reported in a single table.  For example, the upper triangle could be correlations, and the lower triangle (including the diagonal) could be (co)variances.
```{r cov+cor}
printS <- S
printS[upper.tri(R)] <- R[upper.tri(R)]
printS # not symmetric, correlations in upper triangle
```


#### Summary Statistics for Multiple Groups

In SEM, it is common to estimate model parameters simultaneously in multiple groups (i.e., multigroup SEM or MG-SEM).  In our example data, the `MOOD` variable is a 2-group variable, so we could create a `factor` from the numeric codes:
```{r factor}
dat$mood.f <- factor(dat$MOOD, levels = c(-1, 1),
                     labels = c("neutral","positive"))
```

Then we can calculate the summary statistics separately within each group.
```{r group summaries}
modVars <- c("ATT","NFC","POS")   # modeled variables
gM <- sapply(c("neutral","positive"), simplify = FALSE, 
             FUN = function(g) colMeans(dat[dat$mood.f == g, modVars]) )
gS <- sapply(c("neutral","positive"), simplify = FALSE,
             FUN = function(g) cov(dat[dat$mood.f == g, modVars]) )
gN <- table(dat$mood.f)
```

To fit a MG-SEM in `lavaan`, the summary statistics must be a list with one (mean vector or covariance matrix) per group, as well as a vector of group sample sizes.
```{r print summaries}
gM
gS
gN
```


### Importing Summary Data

#### Full Covariance Matrix

If we are importing summary data (e.g., from an article that reported them), we can type our data directly to our R script.  Suppose the values above from `S` were rounded to 3 decimal places:
```{r type S values}
covVals <- c( 1.010, -0.032,  4.355,   6.841, 
             -0.032,  1.973,  0.795,   2.599, 
              4.355,  0.795, 69.263,  87.914,
              6.841,  2.599, 87.914, 282.060)
covVals
```

If these values were saved as plain text, we could also read the data from a file using the `scan()` function, which returns a vector that contains the stored values in the file "values.txt". 
```{r read S values, eval=FALSE}
covVals <- scan("values.txt")
```

Either way, we can place these values from the full covariance matrix into a matrix using the `matrix()` function, specifying how many rows and columns there are:

```{r store S values}
(newS <- matrix(data = covVals, nrow = 4, ncol = 4))
```

We can give names to the variables in the covariance matrix by using the `dimnames=` argument of the `matrix()` function, or by using the `dimnames()` function after creating the matrix.  Let's save the variable names in an object `obsnames`, then use them to assign names. 

```{r dimnames}
obsnames <- c("MOOD", "NFC", "POS", "ATT")
## providing names when creating matrix
newS <- matrix(covVals, nrow = 4, ncol = 4,
               dimnames = list(obsnames, obsnames))
## or assign afterward, all at once
dimnames(newS) <- list(obsnames, obsnames)
## or one dimension at a time (useful for asymmetric matrix)
rownames(newS) <- obsnames
colnames(newS) <- obsnames
```


##### Rescaling a Correlation Matrix

If the imported matrix is instead a full correlation matrix (i.e., assuming all variables have $SD=1$), then it can be transformed back into a covariance matrix using the $SD$s, so variables will be in their original units.  This is important because an SEM fitted to a correlation matrix will have biased $SE$s and inflated Type I error rates unless complex constraints are imposed.

```{r type R values}
## store values from correlation matrix
corVals <- c(  1,    -0.023, 0.521, 0.405,
              -0.023, 1,     0.068, 0.110,
               0.521, 0.068, 1,     0.629, 
               0.405, 0.110, 0.629, 1)
newR <- matrix(data = corVals, nrow = 4, ncol = 4,
               dimnames = list(obsnames, obsnames))
newR
## store SDs as a diagonal matrix
(SDs <- diag(SD))
## transform correlations to covariances
scaled.S <- SDs %*% newR %*% SDs
round(scaled.S, 3)
## matches covariance matrix (within rounding error)
newS
```


Note that rescaling the correlation matrix to be a covariance matrix required pre- and postmultipication of the (diagnal matrix of) $SD$s.  The operator for matrix multiplication is `%*%` (the normal scalar multiplication operator `*` would simply multiply each cell of one matrix with the corresponding cell of another matrix).  The table below gives an overview of commonly used functions in matrix algebra in R. 


| **Symbol** | **Function**                       | **Example in R**  |
|------------|------------------------------------|-------------------|
|   $A+B$    | Addition                           |     `A + B`       |
|   $A-B$    | Subtraction                        |     `A - B`       |
|  (none)    | Elementwise Multiplication (in R)  |     `A * B`       |
|  (none)    | Elementwise Division (in R)        |     `A / B `      |
|  $AB$      | Matrix Multiplication              |     `A %*% B`     |
|  $A^{-1}$  | Inverse (enables "division")       |     `solve(A)`    |
|  $A^{'}$   | Transpose (rows become columns)    |     `t(A)`        |
|  $|A|$     | Determinant (generalized variance) |     `det(A)`      |

To review basic matrix algebra, consult a linear algebra text or an appendix of some SEM textbooks (e.g., Bollen, 1989; Schumacker & Lomax, 2016).


#### Upper/Lower Triangle of a Covariance Matrix

As a covariance matrix is a symmetric matrix, we do not need to provide the complete matrix. Instead, we can import only the nonredundant information from only the lower (or upper) triangle of the covariance matrix, then use the `lavaan` function `getCov()` to create the full matrix.  By default, `getCov()` expects the values from the lower triangle, and it will read values row-by-row (i.e., left-to-right, top-to-bottom), including the diagonal elements (`diagonal = TRUE` is the default argument).

$$
\begin{bmatrix} 
 1.010 & & & \\ 
-0.032 & 1.973 & & \\
 4.355 & 0.795 & 69.263 & \\
 6.841 & 2.599 & 87.914 & 282.060
\end{bmatrix}
$$

This is equivalent to reading the upper triangle column-by-column (i.e., top-to-bottom, left-to-right).  One can also add a vector with the `names=` of the observed variables. It returns the complete covariance matrix, including row and column names. The following code can be used to create a full matrix that is equal to `S` and `newS` above, using only the values from the lower triangle of the matrix.

```{r lower S}
lowerS <- c( 1.010,
            -0.032, 1.973, 
             4.355, 0.795, 69.263, 
             6.841, 2.599, 87.914, 282.060)
getCov(x = lowerS, names = obsnames)
``` 

Sometimes you will find the upper triangle reported in a published paper instead of the lower triangle. Because `getCov()` **only reads values row-by-row** (i.e., left-to-right, top-to-bottom), reading an upper-triangle requires changing the argument to `lower = FALSE`. 

$$
\begin{bmatrix} 
1.010 & -0.032 &  4.355 &   6.841  \\
      &  1.973 &  0.795 &   2.599  \\
      &        & 69.263 &  87.914  \\
      &        &        & 282.060  
\end{bmatrix}
$$
```{r upper S}
upperS <- c(1.010, -0.032, 4.355,  6.841, 
                    1.973, 0.795,  2.599, 
                          69.263, 87.914,
                                 282.060)
getCov(x = upperS, names = obsnames, lower = FALSE)
```


##### Upper/Lower Triangle of a Correlation Matrix

Quite frequently, in published papers the covariance matrix is not provided, but instead a correlation matrix and $SD$s are given separately. The `getCov()` argument `sds=` can be used  to automatically rescale the correlation matrix. Because the diagonal of a correlation matrix is always 1, it is not necessary to include the diagonal values.

$$
\begin{bmatrix} 
 1 & & & \\ 
-0.023 & 1 & & \\
 0.521 & 0.068 & 1 & \\
 0.405 & 0.110 & 0.629 & 1
\end{bmatrix}
$$

In this case, tell `lavaan` that the diagonal entries are omitted using the `diagonal=FALSE` argument. The following syntax creates the complete covariance matrix that was used in the previous examples from the correlations and $SD$s.

```{r lower R}
getCov(x = c(-0.023, 0.521, 0.068, 0.405, 0.110, 0.629),
       diagonal = FALSE, sds = SD, names = obsnames)
```

If you are wondering whether the correlations appear in the correct order in the matrix, you can first leave the $SD$s out and check that the cells of the correlation matrix are in the correct order.  If everything looks correct, then you can add the $SD$s.  Covariance/correlation matrices are also symmetric, so it is also important to check that the lower triangle is a reflection of the upper triangle (i.e., Row-$x$ Column-$y$ of the matrix should contain the same value as Row-$y$ Column-$x$).  The R function `isSymmetric()` can perform this check for you:
```{r isSymmetric}
isSymmetric(S)
```







## Regression Review


### Linear Regression Models

An outcome $y$ is a linear combination (weighted sum) of predictors

- Like a recipe for the outcome: How much of $x_1$ and $x_2$ do you need to recreate $y$?

\begin{align*}
y_i &= \textcolor{blue}{\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}} &+& \textcolor{darkred}{\varepsilon_i} & \\
    &= \textcolor{blue}{\beta_0 + \sum_{j=1}^p \beta_j x_{ij}} &+& \textcolor{darkred}{\varepsilon_i} &\sim \textcolor{darkred}{\mathcal{N}(0, \sigma)}
\end{align*}

- The \textcolor{blue}{deterministic component} of $y$ can be predicted or explained by $X$
- The \textcolor{darkred}{stochastic component} of $y$ includes all other unmodeled effects (omitted variables) and sources of error



### Matrix Notation

For subjects $i = 1, \dots, N$ in rows and predictors $j = 1, \dots, p$ in columns:

\begin{equation*}
  \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_N
  \end{bmatrix} =
  \begin{bmatrix}
      1 & x_{1,1} & x_{1,2} & \dots  & x_{1,p} \\
      1 & x_{2,1} & x_{2,2} & \dots  & x_{2,p} \\
 \vdots & \vdots  & \vdots  & \ddots & \vdots  \\
      1 & x_{N,1} & x_{N,2} & \dots  & x_{N,p}
  \end{bmatrix}
  \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2 \\
    \vdots \\
    \beta_p
  \end{bmatrix} +
  \begin{bmatrix}
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
    \varepsilon_N
  \end{bmatrix} 
\end{equation*}

- Shorthand:  $\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\varepsilon}$

The **generalized linear model** (GLM) extends basic linear models by incorporating different 

- functions that link the outcome to its linear predictor
- distributional assumptions about the outcome('s errors)


### Interpreting and Estimating Coefficients

Regression models provide predictions of the outcome for any level(s) of the predictor(s)

- **Only the intercept** ($\beta_0$) is interpreted as a mean:
    - The **expected value** ($\hat{y}$) when each predictor $x=0$
- Each slope ($\beta_j$) is interpreted as the **change** in $\hat{y}$ as $x_j$ increases 
    - i.e., How does $y$ **covary** with $x_j$, given other $x$s?

Ordinary Least Squares (OLS) estimator of coefficients minimizes overall discrepancies between observations and predictions ($y - \hat{y}$):

$$\beta = (\mathbf{X}^{'} \mathbf{X})^{-1} \mathbf{X}^{'} \mathbf{y}$$

- Minimizing residual variance = maximizing $R^2$ 
- $R$ = Cor($y, \hat{y}$) 


### Syntax Examples with Raw Data

Using matrix algebra to estimate the coefficients:

```{r XtXiXtY}
y <- dat$ATT
X <- cbind(`(Intercept)` = 1, # first column is constant
           as.matrix(dat[c("MOOD","NFC","POS")]))
solve(t(X) %*% X) %*% t(X) %*% y # beta
```


Matches results from linear-modeling function:
```{r multiple reg}
mod <- lm(ATT ~ MOOD + NFC + POS, data = dat)
coef(mod)
```


### Categorical Predictors

Groups $g = 1, \ldots, G$ can be represented using numeric codes

- dummy code for Group $g$ indicates whether someone belongs to Group $g$ (1) or not (0)
- effects codes and orthogonal contrast codes are alternatives

`MOOD` is an effects-coded to indicate experimental conditions

- $+1$ = positive mood induction
- $-1$ = neutral mood (control condition)

```{r MOOD table}
table(dat$MOOD)
```


#### Regression with a Baseline Group

When we are interested in comparing scores on a variable across groups, we are typically interested in the differences between the group means.  Recall that an intercept-only model estimates a single mean for everyone
```{r intercept only}
mod0 <- lm(POS ~ 1, data = dat)
```

When we add a predictor to the model, the intercept becomes a conditional mean (i.e., the expected value when each predictor = 0), and the slope represents how the means differ between groups.

By default, `lm()` will create dummy codes for all levels of a `factor` variable, except for the first group (the baseline for comparison)

```{r MOOD factor}
dat$mood.f <- factor(dat$MOOD, levels = c(-1, 1),
                     labels = c("neutral","positive"))
coef(mod1 <- lm(POS ~ mood.f, data = dat))
```

**Practice interpreting** the intercept and slope:

```{r group means+diff}
aggregate(POS ~ mood.f, data = dat, FUN = mean)
diff(aggregate(POS ~ mood.f, dat, mean)$POS)
```


#### Regression with Group-Specific Intercepts

Rather than choosing a baseline group, we can omit the intercept from the model, in which case `lm()` will include all $G$ dummy codes (not $G-1$)

```{r no intercept}
coef(mod2 <- lm(POS ~ -1 + mood.f, data = dat))
```

Essentially, this model gives each group its own intercept, which is also the group-mean when there are no other predictors in the model
```{r group means}
aggregate(POS ~ mood.f, data = dat, FUN = mean)
```

Later in this course, you will learn about *multigroup SEM*, which is when the same SEM is fitted separately (but simultaneously) to 2 or more groups.  Multigroup SEM is analogously advantageous, but (unlike in the GLM) variances can also differ across groups, so we do not need to assume homoskedasticity when using multigroup SEM.


#### Baseline Comparison vs. Group-Specific Intercepts

To test the $H_0$ that the group means are equivalent, we can compare the model that represents $H_0$ (i.e., the intercept-only model, which estimates a single mean for all groups) to the model that represents the alternative hypothesis ($H_A$) that group means may differ.  

Because the models `mod1` (intercept + 1 slope) and `mod2` (2 slopes that are group-specific intercepts) are statistically equivalent (i.e., they make identical predictions), the ANOVA results are identical:

```{r ANOVAs}
anova(mod0, mod1)
anova(mod0, mod2)
```

That is, both `mod1` and `mod2` say that we need 2 parameters to make good predictors (i.e., separate means for 2 groups), whereas the intercept-only model says that estimating 1 parameter provides similar predictions.


#### $t$ vs. $F$ Tests

The group mean difference can also be tested if it is an estimated parameter, as it is in the baseline-group approach
```{r print t tests, echo=FALSE}
summary(mod1)
```

- Same $p$ value, $t^2 = F$ from ANOVA

But group-specific intercepts (and slopes) can also be advantageous

- Each group's intercept and slopes are model parameters, rather than functions of parameters when using a baseline group



### Constant vs. Varying Slopes

Suppose we are investigating the possibility that this grouping variable *moderates* the effect of another (focal) predictor: *Need for cognition* (NFC).  

- See [this primer](http://quantpsy.org/interact/interactions.htm) for a review.

To represent the $H_0$ that NFC has a constant effect on positive thoughts across groups, we can add it as a predictor to the model without an intercept. In this case, rather than group-specific means, the dummy codes are *group-specific intercepts*.
```{r slope w/o int-a}
slope0 <- lm(POS ~ -1 + mood.f + NFC, data = dat)
summary(slope0)
```

An interaction implies that slopes differ across groups

- With a *baseline group*, the reference-group's slope is estimated, along with how much each of $G-1$ group's slopes differ from it

```{r slope baseline}
## or        POS ~ mood.f * NFC
slope1 <- lm(POS ~ mood.f + NFC + mood.f:NFC, data = dat)
summary(slope1)
```

- With *group-specific parameters*, each group's own intercept **and** slope are estimated, but multiplying each group's dummy code against the moderator

```{r group slopes}
## no "main effect" of NFC without a reference group
slope2 <- lm(POS ~ -1 + mood.f + mood.f:NFC, data = dat)
summary(slope2)
```



## Summary Statistics

### Reimport Data

The next section is easier if all the variables in the `data.frame` are `numeric`, so let's just use the given effects-coded `MOOD`

```{r import again, message=FALSE}
dat <- foreign::read.spss("MODMED.sav",
                          to.data.frame = TRUE)[c(2, 3, 5, 7)]
```



### Raw Data vs. Summary Statistics

Regression models include both mean-structure parameters (intercepts) and covariance-structure parameters (slopes, residual variance).

Much of SEM involves the analysis of mean and covariance structures (MACS).

- OLS regression minimizes discrepancies between observed and predicted **casewise observations** of $y$
    - $y - \hat{y}$
- LS and ML estimators in SEM minimize discrepancies between observed and predicted **summary statistics**
    - mean vector: $\bar{y} - \widehat{\mu}$
        - $\bar{y}$ = observed sample means
        - $\widehat{\mu}$ = expected/predicted/model-implied population means
    - covariance matrix: $\mathbf{S} - \widehat{\Sigma}$
        - $\mathbf{S}$ = observed sample (co)variances
        - $\widehat{\Sigma}$ = expected/predicted/model-implied population (co)variances

Rather than raw data in $\mathbf{X}$ and $\mathbf{y}$, we can obtain the same estimates using summary statistics alone


### What Is Covariance?

Variance ($\sigma^2_y$) quantifies how individual scores vary

- the mean of squared deviations from $\bar{y}$

$$\text{Var}(y) = \frac{\sum_{i=1}^N (y_i - \bar{y})^2}{N-1} = \frac{\sum_{i=1}^N (y_i - \bar{y})(y_i - \bar{y})}{N-1}$$

Covariance ($\sigma_{xy}$) quantifies how $x$ and $y$ are linearly related 

- *co*vary = vary together

$$\text{Cov}(x,y) = \frac{\sum_{i=1}^N (x_i - \bar{x})(y_i - \bar{y})}{N-1}$$

Difficult to interpret its absolute value

- bound by $\pm \sigma_x \sigma_y$
- so, dividing $\sigma_{xy}$ by $\sigma_x \sigma_y$ limits the range to $\pm 1$

For $>2$ variables in a data set $\mathbf{Y}$, we can simultaneously calculate all (co)variances using matrix algebra:

1. Mean-center the data-matrix
    - $\mathbf{Y} - \bar{\mathbf{Y}}$
2. "Square" it by (pre)multiplying it by its transpose

$$\Sigma = (\mathbf{Y} - \bar{\mathbf{Y}})^{'} (\mathbf{Y} - \bar{\mathbf{Y}})$$

Variances are on the diagonal of $\Sigma$, and each off-diagonal cell contains the covariance between the row-variable and the column-variable.


### Interpreting Covariance

We can scale the covariance to provide familiar interpretations.

**Slopes** are change in $y$ (or $x$) per unit $x$ (or $y$)

- divide by variance: $\beta_{yx} = \frac{\sigma_{xy}}{\sigma^2_x} = \frac{\sigma_{xy}}{\sigma_x \sigma_x}$ (or $\frac{\sigma_{xy}}{\sigma^2_y} = \frac{\sigma_{xy}}{\sigma_y \sigma_y}$)

**Correlations** are standardized covariances (bound by $\pm 1$)

- divide by both $SD$s: $\rho_{xy} = \frac{\sigma_{xy}}{\sigma_x \sigma_y}$

This is how slopes can be standardized ($\beta^*$), which is a correlation when there is only 1 predictor

\begin{align*}
\beta^*_{yx} &= \beta_{yx} \times \frac{\sigma_x}{\sigma_y} \\
&= \frac{\sigma_{xy}}{\mathcolorbox{orange}{\sigma_x} \sigma_x} \times \frac{\mathcolorbox{orange}{\sigma_x}}{\sigma_y} \text{ (\colorbox{orange}{these} cancel out)} \\
&= \frac{\sigma_{xy}}{\sigma_x \sigma_y} (= \rho_{xy})
\end{align*}


## Covariance Matrix

With mean-centered predictors, $(\mathbf{X}^{'} \mathbf{X})^{-1}$ is their covariance matrix and $\mathbf{X}^{'} \mathbf{y}$ captures their covariances with the outcome

```{r cov matrix}
n <- nrow(dat) - 1L
Xc <- scale(dat[c("MOOD","NFC","POS")],
            center = TRUE, scale = FALSE)
cbind(t(Xc) %*% Xc   ,   t(Xc) %*% y)  / n
cov(dat)
```


### OLS with Summary Statistics: $\beta = (\mathbf{X}^{'} \mathbf{X})^{-1} \mathbf{X}^{'} \mathbf{y}$

In simple regression (1 predictor), the slope is simply the covariance scaled by  the predictor's variance: $\beta_1 = \frac{\text{Cov}(x,y)}{\text{Var}(x)}$

- $\mathbf{X}^{'} \mathbf{X} = \text{Var}(x)$, so $(\mathbf{X}^{'} \mathbf{X})^{-1} = \frac{1}{\text{Var}(x)}$
    - matrix-multiplication by inverse $\approx$ division
- $\mathbf{X}^{'} \mathbf{y} = \text{Cov}(x,y)$

In multiple regression, the same formulas additionally account for covariances of $x$ and $y$ with covariates

```{r slope from cov}
cov(dat)["POS","ATT"] / var(dat$POS)
coef(lm(ATT ~ POS, data = dat))[["POS"]]
```


### Intercepts and Means

With uncentered data, $X$ can include a constant (all 1s) in the first column, which effectively "partials out" the mean. 

With no other predictors (an intercept-only model):

\begin{align*}
y_i &= \beta_0 (1) + \varepsilon_i \\
    &= \bar{y} + (y_i - \bar{y})
\end{align*}

- the intercept is the mean
- residuals are mean-centered $y$ (same variance)

OLS simplifies to the arithmetic mean formula: $\bar{y} = \frac{\Sigma y}{N}$

- $(\mathbf{X}^{'} \mathbf{X})^{-1} = \frac{1}{N}$
- $\mathbf{X}^{'} \mathbf{y} = \sum y$


### Intercepts and Means

```{r mean from OLS}
## using matrix algebra
ONE <- X[ , "(Intercept)"]
solve(t(ONE) %*% ONE) %*% t(ONE) %*% y
## fitted linear model with only an intercept
coef(lm(y ~ 1))
## calculate the mean
mean(y)
```


### What Are Mean and Covariance *Structures*?

Regression models already illustrate how means are structured

- A mean ($\bar{y}$) is an expected value under a particular condition ($\beta_0$ when $X=0$) plus predictors' means times their slopes: $\bar{y} = \beta_0 + \sum \beta_j \bar{x}_j$

Recall how often statistical procedures involve partitioning variance into components

- Regression: explained ($R^2$) vs. unexplained (residual) variance
- ANOVA: variance between vs. within groups
- Multilevel modeling: Level-1 v. -2 variance
    - and (un)explained at each level
- Between vs. within multiple imputations of missing data 

Covariance can also be partitioned into components

- Variance is merely a variable covarying with itself
- We can account for multiple reasons why 2 variables are related

The decomposition of covariances will be explained after introducing path analysis.



## References

Bollen, K. A. (1989). *Structural equations with latent variables*. Wiley. <http://dx.doi.org/10.1002/9781118619179>

Rosseel, Y. (2012). `lavaan`: An R package for structural equation modeling. *Journal of Statistical Software, 48*(2), 1--36. <http://dx.doi.org/10.18637/jss.v048.i02>

Schumacker, R. E., & Lomax, R. G. (2016). *A beginner's guide to structural equation modeling* (4th ed.). Lawrence Erlbaum. 



